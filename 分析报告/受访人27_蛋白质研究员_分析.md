# 受访人27分析报告：蛋白质研究员（医学化学领域）

## 一、基本画像

| 维度 | 信息 |
|------|------|
| 身份 | 蛋白质/医学化学研究员 |
| 研究方向 | 蛋白质降解（把protein和另一个分子拉在一起，让好的"吃掉"不好的） |
| 主要工具 | ChatGPT Plus（GPT5）、Google AI mode |
| 语言 | 英文为主（研究背景为英语环境） |
| 使用历史 | 2023年开始接触，2024年7-8月开始高频使用 |
| 特点 | 低信任度、高验证意识、领域严谨性要求高 |

## 二、使用场景分析

### 2.1 核心使用场景

**场景一：初步方向探索**
- 对不了解的课题或方向，用AI缩小范围
- AI提供initial framework
- 强调：general问题AI信任度高，specific问题信任度低

**场景二：写作润色**
- 收集所有资料（AI+Google）后，让AI improve内容
- "在写作方面，我觉得他帮到我是挺多的"
- AI帮助improve整个flow和storytelling

**场景三：资料验证与补充**
- 不依赖AI给的文献（"很多都不太靠谱"）
- 用Google核对AI答案
- 找有经验的人验证AI建议

### 2.2 明确不使用的场景

**化学实验troubleshooting**：
- 问AI为什么实验出错、为什么yield低
- 结果："他就会开始给我乱说一通"
- 这类specific问题AI"答案还是跟还有相当大的差距"

## 三、信任轨迹：低信任+高验证型

### 3.1 信任水平

**低于50%信任度**：
- "直接应用的话，我觉得少于50。50%吧"
- 这是所有受访者中最低的信任度之一
- 与医学领域对准确性的高要求有关

### 3.2 验证行为

**多重验证机制**：
- Google核对AI答案
- 询问"比我有经验的人"
- 发现错误后不再问AI相关问题，转向其他来源

### 3.3 信任边界

**高信任场景**：
- General问题（"没那么高的specificity的时候，ai的那个信任度还是蛮高的"）
- 写作润色和storytelling

**低信任场景**：
- Specific化学实验问题
- 需要reference支持的学术内容
- 文献推荐（"ai给我的文章很多都不太靠谱"）

## 四、核心专家策略

### 4.1 "前后包夹"策略

**AI角色定位**：
- "它就是充当一个前面跟一个后面这样的角色，然后，中间的那一段就是由我自己来补充"
- 前面：AI提供框架和思路
- 中间：自己填充核心内容
- 后面：AI润色和优化

### 4.2 对话隔离策略

**每问题新对话**：
- "通常我就会针对一个问题，然后我就开一个新的对话框"
- 原因：AI会"一直延续下去"，"硬硬会跟你扯关系"
- 避免AI"记忆混乱"和"思路混乱"

### 4.3 错误后放弃策略

**发现错误即转向**：
- "通常我发现他错了，我就不会再问他"
- 直接去其他地方寻找答案
- 不花时间纠正AI

### 4.4 输入精细化策略

**缩小范围提高准确性**：
- 提前告知：task内容、字数限制、观众群体、professional程度
- "把那个方向缩小了之后，他给的答案会更加的准确一些"

## 五、深层认知分析

### 5.1 思考变浅的自我觉察

**坦诚承认依赖性**：
- "我觉得可能我的思考会比以前浅了挺多"
- "我会对他有一定的那个依赖性"
- "遇到了什么不知道的，我就会想要去问他，而不是先自己去思考一遍"

### 5.2 领域严谨性决定低信任

**医学化学的特殊要求**：
- "我们需要非常精准的一些答案跟观点"
- "如果有稍微一些不严谨的都会被问"
- 因此参考内容"会比较少用ai"

### 5.3 AI定位的演进期待

**现阶段：Assistant**
- "Assistant，现阶段还是assistant"

**期待未来：Collaborator**
- "能进步，能当个collaborator，其实也还是一个不错的方向"
- 前提是AI"给出的答案就是有一定的那个思考性跟一定的那个参考程度"

### 5.4 对AI改进的建议

**Reference集成**：
- "把那个ai跟google做更好的integration会很好"
- "所有ai说过的话，如果都有reference去提升他说的那句话的意义"

**问题敏感度**：
- "能很精准的抓到你想要问的到底是什么东西"
- "他的sensitivity，如果能再进步一些"

## 六、对论文的核心价值

### 6.1 独特贡献："前后包夹"策略

**AI作为框架+润色工具**：
- 这与"起点+终点"的定位一致
- 中间核心内容由人类掌控
- 形成明确的人机协作边界

### 6.2 理论贡献

**领域严谨性与信任度的负相关**：
- 医学化学要求精准 → 低AI信任度
- 这支持了"领域特性影响信任模式"的假设
- 与营销等"无标准答案"领域形成对比

### 6.3 思考退化的自我报告

**罕见的坦诚反思**：
- "思考会比以前浅了挺多"
- 主动承认依赖性影响
- 但认为"还是主要是看个人的那个使用方式"

### 6.4 假文献风险的强调

**对学术诚信的担忧**：
- "假文献的风险最大"
- "如果有些人他们不去verify之后再拿来用的话，那那就一团糟了"
- 与学术不端的关联

## 七、关键引用

### 关于信任水平
> "直接应用的话，我觉得少于50。50%吧，就是现在来看的话。"

### 关于AI角色定位
> "我觉得它就是充当一个前面跟一个后面这样的角色，然后，中间的那一段就是由我自己来补充。"

### 关于思考变浅
> "我觉得他确实是帮我省了很多的时间，但是同时我也觉得可能我的思考会比以前浅了挺多，因为就是我会对他有一定的那个依赖性。"

### 关于AI定义
> "Assistant，现阶段还是assistant。"

### 关于对话隔离
> "通常我就会针对一个问题，然后我就开一个新的对话框...因为有时候那个ai它会一直延续下去...它硬硬会跟你扯关系。"

### 关于假文献风险
> "假假文献的风险最大吧...如果有些人他们不去verify之后再拿来用的话，那那就一团糟了。"

### 关于领域严谨性
> "我们需要非常精准的一些答案跟观点，就是如果有稍微一些不严谨的都会被问。"

## 八、分析验证

待核对的关键引用：
- [ ] "少于50%"信任度表述
- [ ] "充当一个前面跟一个后面这样的角色"
- [ ] "思考会比以前浅了挺多"
- [ ] "Assistant，现阶段还是assistant"
- [ ] "假文献的风险最大"

---

**信任类型标签**：低信任型 / 高验证型 / 领域严谨导向

**核心发现**：医学化学领域的"<50%"低信任度，"前后包夹"的AI角色定位，以及对思考变浅的坦诚反思
