# 受访人25分析报告：AI+教育科研从业者

## 一、基本画像

| 维度 | 信息 |
|------|------|
| 身份 | AI+教育领域科研从业者 |
| 背景 | NLP/计算语言学，曾研究非裔美国人英语 |
| 主要工具 | Claude Max（$200/月）、GPT Team版 |
| 开发工具 | GPT API、DeepSeek、千问、Dify框架 |
| 使用历史 | 2022年11月（GPT出来即开始） |
| 特点 | 深度技术理解 + 教育视角的双重身份 |

## 二、使用场景分析

### 2.1 核心使用场景

**场景一：论文写作**
- 不先写完整草稿再让AI改
- 而是列出段落大纲 → AI生成自然段 → 人工修改
- 关键指令："be natural, clarify the details"
- 告诉AI哪块扩写、哪块缩写

**场景二：完整代码生成**
- 让Claude生成1000+行完整代码
- 包括提示词设计、Agent间通信方式
- 关键策略：让AI输出每一步的input/output（print出来）
- 容易发现问题："跑不起来就让他修改"

**场景三：AI驱动的信息检索**
- 取代传统搜索引擎
- 搜索文献："这个领域引用量高的文献有哪些？"
- 现在有链接可验证，幻觉问题减少

**场景四：邮件撰写**
- "基本上他给我写"
- 告诉需求，让它生成，再修改

### 2.2 明确不使用的场景

**头脑风暴/Idea生成**：
- "个位数"次使用
- "idea可能并不来源于文献综述，而是我发现有这么一个需求"
- AI无法判断"什么值得做"

**模型创新**：
- 调整神经网络结构时必须自己非常明确
- AI只能执行，不能创新

## 三、信任轨迹：技术洞察型

### 3.1 对AI本质的深刻认识

**"AI没有元认知"**：
- "它不会说是去先去想我说这句话到底我相信不相信"
- "人在说话的时候，大家都知道自己说的哪些是真的，哪些是假的"
- AI没有这个功能

**"AI没有价值观"**：
- "人是有价值...知道好坏"
- "他不知道什么东西值得做，哪些东西不值得做"

### 3.2 流畅性造成的虚假权威

**过度信任的根源**：
- "GPT表述得特别的官方、特别的正式，造成了一种它非常权威的假象"
- "前后文相关性特别高，人没办法去鉴定他到底知道不知道"
- 类比传销："我也不知道他到底是不是说的真的，我就觉得他可信"

### 3.3 领域熟悉度与信任

**不熟悉领域 → 完全信任**：
- "不熟悉，所以完全信任"
- 破局方法："对AI非常熟悉，知道他在某种情况下容易产生不正确的信息"

## 四、核心专家策略

### 4.1 段落化写作控制

**精细化的扩缩控制**：
- "我觉得哪一块需要扩写，哪块不需要扩写，然后告诉他"
- 人掌握重点判断，AI执行文字生成
- 最后过渡和拼接由人完成

### 4.2 代码调试的Print策略

**每步输出追踪**：
- "让他把所有的input或output，每一步骤全都print out出来"
- "你只需要看这个print out就可以了"
- 比传统调试更高效

### 4.3 付费策略

**精准匹配需求的付费**：
- Claude开Max（$200/月）：因为能写长代码
- GPT开Team版：基础功能够用
- 原因："Claude还算是比较听话"，GPT会"偷懒"

## 五、深层认知分析

### 5.1 语境语言的洞察

**日语/中文 vs 英语**：
- "日文是语境语言，必须靠语境理解"
- "人有对隐藏信息的理解能力"
- AI"如果没有足够的训练语料，推导不出来"

### 5.2 复杂任务的AI局限

**造楼比喻**：
- 问AI怎么造一栋楼，AI会生成方案
- 但建筑师会反问"要造什么样的楼？"
- "模型不会意识到...不会去验证他输出的内容到底和你说的是不是相符"

### 5.3 对学生使用AI的策略

**刻意限制creative thinking任务**：
- "我不知道他creative thinking出来的东西到底是啥"
- 只让学生做监控性任务
- "只需要摁两个按键就可以执行的"

**布置AI无法完成的作业**：
- "AI提的研究问题会非常vague"
- "范围太大了，所以研究不了"
- "一下就能看出来"是AI生成的

## 六、对论文的核心价值

### 6.1 独特贡献：AI能力边界的专业剖析

**"没有元认知"和"没有价值观"**是极具理论价值的发现：
- 解释了为什么AI无法判断idea价值
- 解释了为什么research question需要人类确定
- 提供了区分人机能力边界的理论框架

### 6.2 理论贡献

**流畅性→虚假权威→过度信任**的因果链：
- 不是因为内容正确才信任
- 而是因为表述流畅、前后一致
- 与传销机制类比，深刻揭示信任本质

### 6.3 语境语言视角

**高语境语言（日语/中文）对AI的挑战**：
- 需要prior knowledge
- 隐性知识无法直接输入
- 这是独特的NLP专业视角

### 6.4 教育实践洞见

**对学生使用AI的务实态度**：
- 学生需要AI做"知识原始积累"
- Coding无法禁止AI使用
- 重点是培养"什么是好，什么是坏"的判断力

## 七、关键引用

### 关于AI的元认知缺失
> "人在说话的时候，我觉得大家都知道自己说的哪些是真的，哪些是假的...但是大模型它没有，就是它不会说是去先去想我说这句话到底我相信不相信它不会去想这个问题。"

### 关于价值判断缺失
> "人是有价值，人是知道好坏的...但他是没有价值观，他不知道什么东西值得做，哪些东西不值得做。"

### 关于流畅性造成的虚假权威
> "GPT在表述的过程中，它表述得特别的官方，特别的正式，然后就造成了一种它非常权威的假象，然后人就信了。"

### 关于领域熟悉度与信任
> "不熟悉，所以完全信任。但是还有一种破局的方法，就是你对AI非常熟悉，你知道他在某种情况下容易去产生不正确的信息。"

### 关于写作策略
> "writing的话...我会先把，比如说这个段落的大纲列出来，我要先说什么再说什么...然后告诉他，让他帮我写出这么一个自然段，然后再去看他写出来这个自然段，我再修改。"

### 关于代码调试
> "你可以让他写代码，然后让他把所有的这个input或output，每一步骤的input和output全都是print out出来，然后你就能看到，然后你只需要看这个print out就可以了。"

### 关于对学生的管理
> "我不会，我尤其是本科生的话，我会让他执行那种非常简单，只需要摁两个按键就可以执行的...我刻意的不让他去做creative thinking，因为我不知道他creative thinking出来的东西到底是啥。"

### 关于评估AI在教育中的影响
> "如果是GPT会干的，我就不要求学生一定会自己干...我觉得更多的是怎么样培养学生跟AI去协作完成一件事情，他只需要知道什么是好，什么是坏。"

## 八、分析验证

- [x] AI+教育双重身份、NLP背景 ✓
- [x] Claude Max + GPT Team的付费策略 ✓
- [x] "没有元认知"和"没有价值观"的洞察 ✓
- [x] 段落化写作 + Print调试策略 ✓
- [x] 流畅性→虚假权威的因果分析 ✓
- [x] 语境语言（日语/中文）的独特视角 ✓
- [x] 对学生刻意限制creative thinking任务 ✓

---

**信任类型标签**：技术洞察型 / 元认知意识 / 价值判断保留

**核心发现**："AI没有元认知和价值观"的深刻洞察，以及"流畅性造成虚假权威"的信任机制分析
