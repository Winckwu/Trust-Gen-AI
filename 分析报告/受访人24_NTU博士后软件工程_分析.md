# 受访人24分析报告：NTU博士后（Software Engineering for AI）

## 一、基本画像

| 维度 | 信息 |
|------|------|
| 身份 | NTU博士后研究员 |
| 专业领域 | Software Engineering for AI |
| 主要工具 | ChatGPT（免费版为主）、DeepSeek（批判性使用） |
| 使用历史 | GPT出来后即开始使用 |
| 角色定位 | 自视为"教育者"，AI为"学生" |
| 特点 | 高度自主、批判性思维强 |

## 二、使用场景分析

### 2.1 核心使用场景

**场景一：邮件撰写**
- 最高频使用场景
- 优于传统机器翻译
- 能写出"得体、不啰嗦、适合国外沟通"的内容
- 会控制GPT不要过于客套

**场景二：论文写作（Writing）**
- 不让AI凭空生成creative内容
- 自己提供完整思路，AI只做润色
- 分component处理：背景、knowledge gap、insight、实现、evaluation
- 关键策略：**要求AI自我反思是否满足所有要求**

**场景三：Response Letter撰写**
- 审稿意见回复
- GPT擅长写得体的回应
- 但偏冗长、过于礼貌，需要简化
- 能辩证指出回应的逻辑漏洞

**场景四：伦理把关**
- 检查论文中可能引起不适的表述
- GPT对种族、性别等敏感议题有防范意识
- 帮助检查是否有stereotype等问题
- 甚至知道"Black和White的大小写规则"

**场景五：PPT规划与脚本**
- 规划PPT结构、每页要点
- 润色演讲脚本
- 避免"中国人讲英语太formal"的问题

### 2.2 明确不使用的场景

**代码生成**：
- 早期使用后放弃
- 原因："给我生成的好像很有道理，但其实有一些代码又是什么包，又是什么包的依赖...debug这些小错的时候非常非常费劲"
- 现在："我自己还不如对着那个example自己写"
- 但AI可以在high level给建议（推荐哪些包、优劣性）

## 三、信任轨迹：指导者型

### 3.1 信任特征

**"我是教育者，AI是学生"**：
- "我觉得我属于一个指导者，从认知上各种方面我都是一个指导者"
- "它只是起到一个辅助的作用，没有它我也能做这个任务，只是花的时间会更长"

### 3.2 信任边界

**高信任场景**：
- 语言润色和格式转换
- 伦理合规性检查
- 非专业领域的general knowledge问答

**低信任场景**：
- 代码生成（有过负面经验）
- Literature review（担心假引用）
- Creative内容生成

## 四、核心专家策略

### 4.1 需求反思验证策略

**最独特的策略**：让AI自我检查是否满足所有要求
- "我把我的一个要求再说一遍，你对照我的要求你去反思一下你给我的这个答案需不需要进一步优化"
- GPT会回应："你的要求我总结起来有1234点，第一点怎么样...这个已打勾、已达到，需要提升是什么"
- 用于长文本生成，怀疑AI会遗漏requirements

### 4.2 语义等价转换定位

**核心认知框架**：
- 对于实事求是的任务（论文、rebuttal）："我给他的应该就是一个语义的等价体"
- AI的任务是"帮我做语义变换，变成一个更好的形式"
- 而非创造新内容

### 4.3 付费版与免费版的理性选择

- 曾用过付费版，但发现"对我这种用户使用情况好像差别不大"
- 回归免费版
- 原因："主动权在我...对性能要求不是太高"

## 五、深层认知分析

### 5.1 专家自信与AI定位

作为软件工程for AI的研究者，对AI能力有清醒认识：
- 知道benchmark与real-world的gap
- 了解代码生成的真实局限
- 对学生过度依赖AI深感忧虑

### 5.2 对学生使用AI的批判

**强烈批评学生过度依赖**：
- "一眼GPT"：能立刻看出AI写的文章
- 特征："逻辑很差，全部都是废话、冗余的话，不凝练不达意"
- Survey尤其明显："写出来的文章明显能看出高度依赖LLM"
- 问题本质："没有自己的方法论、逻辑在支撑"

**对逻辑弱的学生**：
- "GPT对他们来说简直就是灾难"
- "思维根本就是在降级"
- "writing也没训练上，逻辑也没训练上"

### 5.3 理想的人机分工

- GPT负责：writing层面的工作
- 人类负责：logic层面的把关
- "比较好的方法是你以GPT来帮你负责writing这部分减轻一些工作，但更多地把自己的精力花在logic上"

## 六、对论文的核心价值

### 6.1 独特贡献：需求反思验证策略

**"让AI自我反思是否满足要求"**是极具价值的策略：
- 利用AI的self-reflection能力
- 形成闭环验证
- 特别适用于长文本、复杂要求的任务

### 6.2 理论贡献

**"教育者vs学生"的角色定位**：
- 与某些受访者的"学生vs教育者"形成对照
- 取决于用户的expertise水平
- 专家用户更倾向于"指导者"角色

### 6.3 对教育影响的专业洞见

作为AI+教育从业者的双重视角：
- 技术侧：了解AI的真实能力边界
- 教育侧：观察到学生能力退化
- 结论：教育环节变得更重要，而非更不重要

### 6.4 DeepSeek的批评性评价

**对DeepSeek的深刻批判**：
- "幻觉情况很严重"
- "喜欢说很多很冗余很废话的话"
- "假大空...经不起深推敲"
- 可能原因："用了很多中文的一些奇怪的语料去训练"

## 七、关键引用

### 关于指导者角色
> "我觉得我整个在跟GPT的LLM交互的过程中，我觉得我属于一个指导者...它只是起到一个辅助的作用，就是没有它我也能做这个任务，只是我要花的时间会更长而已。"

### 关于需求反思策略
> "我不管满不满意，反正我经常会习惯性地加一句，把我的一个要求再说一遍，你对照我的要求你去反思一下你给我的这个答案需不需要进一步优化。"

### 关于代码生成的放弃
> "我让它写一些数据分析的代码的时候，它给我生成的好像很有道理，但其实有一些代码又是什么包，又是什么包的依赖什么的...debug这些小错的时候非常非常费劲。"

### 关于学生依赖AI
> "那个学生写出来的东西就很夸张，就是写得废话连篇...一眼就能看出来他高度依赖于LLM写的文字。"

> "像这种人的话，GPT对他们来说简直就是灾难，就是感觉把他们思维根本就是在降级...writing也没训练上，逻辑也没训练上。"

### 关于伦理把关
> "那个文章我几乎是全文都让GPT帮我把关过的，就哪一些话你觉得有可能会有一些伦理风险，有可能会引起有些人不适...我觉得他在这方面比我们有sense。"

### 关于理想分工
> "比较好的方法是你以GPT来帮你负责writing这部分减轻一些工作，但是我们更多地把自己的精力花在这个logic...这样是我觉得比较好的一个交互方式。"

## 八、分析验证

- [x] 博士后身份、Software Engineering for AI ✓
- [x] 代码生成的负面经验及放弃 ✓
- [x] "教育者vs学生"角色定位 ✓
- [x] 需求反思验证策略 ✓
- [x] 对学生过度依赖AI的批评 ✓
- [x] 伦理把关的独特应用 ✓
- [x] DeepSeek"假大空"批评 ✓

---

**信任类型标签**：指导者型 / 专家自信 / 写作-逻辑分离

**核心发现**："让AI自我反思是否满足要求"的需求验证策略，以及"GPT负责writing，人负责logic"的分工理念
