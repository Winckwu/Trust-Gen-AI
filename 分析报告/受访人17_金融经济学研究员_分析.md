# 受访人17：金融经济学研究员 - 深度分析报告

## 基本画像

| 项目 | 内容 |
|------|------|
| 身份 | 研究人员（Researcher） |
| 研究领域 | 金融经济学 |
| 本科背景 | 计算机（C++、算法、ML、DL） |
| 使用起始 | 23年底或24年开始使用 |
| 使用频率 | 每天使用 |
| 主要工具 | ChatGPT（开会员）、豆包 |

## 使用场景

### 场景1：文献总结 ⭐⭐⭐⭐

**使用方式演变**：

早期模式（复杂prompt）：
> "你要包括research question，对吧？然后methodology，conclusion，finding。然后还要加上那些什么你的巴拉巴拉"

现在模式（简单prompt）：
> "Summarize this paper in details. 它出来的结构会比我用prompt的要更好"

**关键发现**：
> "我发现我直接就跟GPT说summarize this paper in details，它出来的结构会比我用prompt的要更好"
> "它好像会被我的prompt束缚住，然后它输出的内容就会...按照你的prompt"

**论文类型对总结效果的影响** ⭐⭐⭐⭐⭐：
> "时政问题很少...它的结论都是呈现出来的，就是x影响y这样"
> "论述的假如说它有很多个影响因素...这时候他就混乱了"
> "理论性很强的文章，他就没办法去很好地总结"

### 场景2：代码编写 ⭐⭐⭐

**使用方式**：
> "我觉得这个Python太简单了...我就让他帮我把它拼起来"
> "我一般会是后者多一点，就是说不会整个都交给他"

**信任与验证**：
> "coding我信任。coding我信任，但我会扫它代码...你知道他在干嘛"

**效率问题**：
> "他有时候给的代码，他的方法一看就是运行得特别慢嘛。嗯，他这个算法的复杂度是那个n方，你能给他整一个这种根号n的复杂度的"

### 场景3：信息检索 ⭐⭐⭐

**使用方式**：
> "把它当成是一个那种谷歌的替代品，就你搜到搜索栏的东西丢给它"

**对准确性的态度**：
> "我会做，就是会check一下，因为我总体对这种LM其实本质不是很信任"
> "它其实就类似于一个，就在搜索这方面，其实就类似于一个普通的搜索引擎"

### 场景4：邮件写作 ⭐⭐

**使用方式**：
> "都有。" （直接生成或修改）

**效果评价**：
> "我觉得是OK的，因为很短"

### 场景5：Brainstorm ⭐⭐

**使用方式**：
> "你抛个问题给他，让他给你几个可能方向的那种解决方案"

**信任程度**：
> "我会更不信任一些，因为你学术上的东西你是有成本的"

### 有趣场景：选水果 ⭐⭐⭐

> "选榴莲，真的这个巨有用"
> "他就会直接告诉你1234他标好，然后他觉得哪个好吃"

## 信任轨迹：理性怀疑型

### 信任的底层逻辑 ⭐⭐⭐⭐⭐

> "我总体对这种LM其实本质不是很信任，因为当你学过它的原理之后，你就会发现它很依赖于它训练的时候的那些材料，所以就是说它这种东西输出的时候，它其实是一个概率模型，他很难说他是百分百是对的"

### 零容错的信任边界 ⭐⭐⭐⭐⭐

> "你10行给我输出10个文章的观点，然后有两个假的，我就会质疑你剩下的8个"
> "我可以接受他不回我...你我你问我十篇文献，我两个是铁错的，你你，你怀不怀疑我剩下8个"

### 信任层级

| 任务类型 | 信任水平 | 原因 |
|----------|----------|------|
| 文献总结（实证） | ⭐⭐⭐⭐ 较高 | 结论明确，可验证 |
| 文献总结（理论） | ⭐⭐ 较低 | 逻辑复杂，容易混乱 |
| 代码编写 | ⭐⭐⭐ 中等 | 可扫一眼验证 |
| 信息检索 | ⭐⭐⭐ 中等 | 当搜索引擎用 |
| 学术Brainstorm | ⭐⭐ 较低 | 成本太高 |
| 生活建议 | ⭐⭐⭐⭐ 较高 | 成本低 |

## 核心专家策略

### 策略1：简化Prompt策略 ⭐⭐⭐⭐⭐

> "Summarize this paper in details. 它出来的结构会比我用prompt的要更好"
> "它好像会被我的prompt束缚住"

**理论价值**：过度约束的prompt反而限制AI输出质量

### 策略2：任务分解自主策略

> "我不会拆...我自己拆"
> "不会整个都交给他"
> "我很少给他描述一个场景，一般就是指定性给他一个任务"

**工作方式**：自己拆解任务，只让AI执行特定步骤

### 策略3：成本导向信任策略 ⭐⭐⭐⭐

> "你学术上的东西你是有成本的...你生活的成本、隐性成本，就显性成本没有那么高"
> "你信了出来GPT给你一个idea，结果那idea但some work，你第二天就被老板骂了...你生活上的，你让他选个榴莲，那个榴莲顶多就是甜跟不甜的区别"

**信任逻辑**：根据错误成本高低决定信任程度

### 策略4：代码扫描验证策略

> "coding我信任，但我会扫它代码，就这东西写代码你扫一眼，所以你的肯定要少啊"

## 深层认知

### 对AI本质的理解 ⭐⭐⭐⭐⭐

> "它其实是一个概率模型，他很难说他是百分百是对的"
> "即使是我们去Google search，Google上搜一些东西，我们也会自己判断"

将AI视为"升级版搜索引擎"，而非智能助手

### 对Prompt的反思 ⭐⭐⭐⭐

> "Prompt的话就众说纷纭。我有人说这个好用，说那个好用，我很难去判"
> "发现对，他好像会被我的prompt束缚住"

观察到过度prompt可能适得其反

### 对AI依赖的态度

> "我觉得是因为我对他始终是怀疑的态度"
> "我有时候写代码真的是全靠他，这不是说我不会写，而是说我懒得写"

怀疑态度与实际依赖并存

### 对文献造假的敏感 ⭐⭐⭐⭐⭐

> "他例如说这篇我要一篇顶刊JFE的，他直接跟你讲这是JFE，你点进去之后是一篇不知名刊物"
> "我搞了两三次，我不想跟他玩"

经历过文献造假后产生强烈不信任

### 对隐私安全的担忧

> "我听他们说就是如果你给AI喂那语料，它其实是会拿你的语料去训练"
> "会不会有一种可能就是我们写了一篇paper，然后别人已经其实他的平台是可以读到你的paper"

## 独特发现

### 1. Prompt束缚效应 ⭐⭐⭐⭐⭐

> "它好像会被我的prompt束缚住，然后它输出的内容就会...按照你的prompt"
> "直接就跟GPT说summarize this paper in details，它出来的结构会比我用prompt的要更好"

**理论价值**：过度详细的prompt可能限制AI的输出质量和结构组织能力

### 2. 论文类型与AI总结能力的关系 ⭐⭐⭐⭐⭐

> "时政问题很少...它的结论都是呈现出来的，就是x影响y这样"
> "理论性很强的文章，他就没办法去很好地总结"

**理论价值**：
- 实证类论文（结论明确）：AI总结效果好
- 理论类论文（逻辑复杂）：AI总结效果差

### 3. 概率模型认知视角 ⭐⭐⭐⭐⭐

> "它其实是一个概率模型，他很难说他是百分百是对的"

**理论价值**：了解AI原理的用户形成更理性的信任预期

### 4. 零容错信任逻辑 ⭐⭐⭐⭐

> "你10行给我输出10个文章的观点，然后有两个假的，我就会质疑你剩下的8个"

**理论价值**：部分用户对AI错误采取零容忍态度，一次错误影响整体信任

### 5. 成本导向的信任分配

根据任务错误成本高低分配信任程度

## 对论文的核心价值

### 1. Prompt设计的"少即是多"原则

简单直接的指令可能比复杂prompt效果更好

### 2. 任务类型与AI适配性

不同类型的学术任务对AI的适配程度不同，实证类优于理论类

### 3. 技术背景对信任的影响

了解AI原理（概率模型）的用户形成更理性但也更怀疑的信任态度

### 关键引用（可用于论文）

1. **概率模型认知**：
   > "我总体对这种LM其实本质不是很信任，因为当你学过它的原理之后，你就会发现它很依赖于它训练的时候的那些材料，所以就是说它这种东西输出的时候，它其实是一个概率模型"

2. **Prompt束缚效应**：
   > "它好像会被我的prompt束缚住，然后它输出的内容就会...按照你的prompt"
   > "Summarize this paper in details. 它出来的结构会比我用prompt的要更好"

3. **零容错信任**：
   > "你10行给我输出10个文章的观点，然后有两个假的，我就会质疑你剩下的8个"
   > "我可以接受他不回我"

4. **成本导向信任**：
   > "你学术上的东西你是有成本的...你生活的成本、隐性成本，就显性成本没有那么高"

5. **论文类型效果差异**：
   > "时政问题很少...它的结论都是呈现出来的，就是x影响y这样"
   > "理论性很强的文章，他就没办法去很好地总结"

6. **搜索引擎定位**：
   > "把它当成是一个那种谷歌的替代品"
   > "它其实就类似于一个，就在搜索这方面，其实就类似于一个普通的搜索引擎"

7. **懒得写vs不会写**：
   > "我有时候写代码真的是全靠他，这不是说我不会写，而是说我懒得写"

---
*分析完成时间：2025年1月*
