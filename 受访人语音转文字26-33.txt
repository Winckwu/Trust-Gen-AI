
受访人 26:
Unknown Speaker
Like store then also, I also just started on my own,

Unknown Speaker
sorry, store,

Unknown Speaker
FB as well. Yeah,

Speaker 1
I think I lost you for a little bit. It's that means store for E commerce or something,

Unknown Speaker
yeah.

Speaker 1
So which kind of E commerce? Which region are you trying to reach out?

Unknown Speaker
Oh, currently, I'm I'm in Malaysia, yeah.

Speaker 1
And you are doing the business between Malaysia and Singapore, yes, okay, that is cool, actually. So what when you start using AI in your work or your entrepreneur projects,

Speaker 2
I think it's more of the marketing side when I need to do journey post, some Instagram posts and stuff. So use AI, or sometimes you generate some ideas, I'll use it as well. Yeah,

Speaker 1
so do you have a team, or you just do all the marketing stuff yourself? Yeah, myself. So what, which kind of, for example, the if you want to launch a campaign like, how much of the workflow you're gonna generate by AI,

Unknown Speaker
I will say, I hate.

Speaker 2
I always started my workflow by AI, yeah. Then from there, I Google right here and there, a bit, but mostly all my, you know, Instagram posts and stuff, or by AI.

Speaker 1
So do you think AI gave you a guideline or something in this portion, in this process?

Speaker 2
Yes, a guideline, and yeah, a guideline and a very detailed workflow

Speaker 1
after it gives you the guideline, would you to optimize something, or you just directly, using that

Unknown Speaker
depends I was, depends on what

Speaker 2
it gave me. Because sometimes I don't really like the idea that the AI, K, so I will try to optimize a bit, but sometimes, if I'm happy with the result, I will just use it directly.

Speaker 1
Also, which kind of model you are frequently using,

Unknown Speaker
mostly,

Speaker 2
I would say, optimize, optimize.

Speaker 1
Okay, so is that the framework or yes, okay, okay. So, for example, because I used to doing marketing, if you are trying to make a a copyright, okay, or you're just just trying to make a copy for the post. So how much you gonna rely on AI and how much you're gonna to rely on your own creativity?

Speaker 2
Mainly, if I'm reading an Instagram post, normally on photos, I'll shoot by myself, like I'll be by myself, then I'll feed the AI what the photo shows and what kind of like emotions I want to evoke in that post, or what kind of information I want to give in that particular book, yeah, then AI will generate the captions. So,

Speaker 1
okay, so do you feel like the AI generated really help you to get the flow, or help you to get the data that you are expected.

Unknown Speaker
Yes, I think it helped me a lot.

Speaker 2
Sorry, you gave me two options, right? But I couldn't really understand

Speaker 1
well, it means if you have a goal for each task. Do you think? Do you think AI can help you to to reach that? And how much you feel like it's gonna help you?

Speaker 2
I think it can actually easily help around 70, 80%

Speaker 1
so, so how did you like? Break it down the work.

Unknown Speaker
Break it down the work.

Speaker 2
Okay, maybe, for example, I want to start make a marketing campaign. So I'll give them, ask them to help me generate some guideline ideas first, so I optimize it. So, you know, they will, sometimes they'll give me a very detailed every, every single step. But even under the every single step, I will still ask the AI to help me generate even more detailed steps with that particular step. Yeah,

Speaker 1
so how can you just feel the tutorial that the air has already give to you is the accurate one.

Speaker 2
I don't know if it's giving me the I think sometimes what I do is I will go and press check online, but most of the time it mostly is like, what the internet okay, what the internet give me? Yeah? Well, for me, it's more of the trial and error. This is a very marketing base. So there's not really a right and wrong answer for you,

Speaker 1
but have you ever tried to verify it? Maybe by the other different kind of AI models or something?

Speaker 2
Not really, because so far, I think it's all been quite okay.

Unknown Speaker
Okay, okay, so, like,

Speaker 1
how much of the parts you feel like aI doing the best,

Unknown Speaker
the pose, the

Unknown Speaker
parts, the parts, the part. Oh, the parts, okay. I think the

Unknown Speaker
best is still giving me a

Unknown Speaker
general guideline, yeah,

Speaker 1
still light. Like to make AI to be your consultant or your Yes, yes. It kind of tutor to help you to have a better understanding in each of the task, if you start going out and it's got just give you a inspiration to get started, right? If I understand, yes. So okay, thank you. So if you trying to do this, you alone with AI and you have no team together. So which do you think is the best way to deal this kind of collaboration, like, how do you identify the boundaries of AI's capabilities?

Speaker 2
Probably, I, personally, I don't see there's any boundaries between this cooperation method. Yeah.

Speaker 1
So do you feel like maybe the thing is generated is too, like, AI style or something?

Speaker 2
I think the caption, yes, sometimes I think for AI, like, for instance, chatgpt, normally, when I asked to write, you know, the post itself, normally it's too weak, sometimes a bit too rigid and too false. You know, they have the really long and dash, which normally, most people don't use and dash. So, yeah, I think maybe yeah, that is one of the limitation I would say.

Unknown Speaker
How could you deal with this kind of problem?

Speaker 2
I will replace everything like that particular statement, for instance.

Speaker 1
So how can you just, like, figure out which kind of is is to AI, and which kind of the things that is generated is you feel like it's okay. Therapy can be used,

Speaker 2
I think, is from reading the like, if you read many articles and posts and stuff like, you'll kind of just get it ahead of it, like it's genuine the way you fight it, yeah, and stuff like that. Make it more personal, I guess.

Speaker 1
So it's mostly both on your own experience, right? So, yeah, okay, so I just have a question, is about if you trying to get connection with AI, just you and AI itself. So do you ever ask AI to act as a role, or something like, if we as a reviewer, a a marketer, a audience of of your target, anything this kind of

Unknown Speaker
Yes. I

Speaker 2
sometimes do ask them to be another role, yes.

Speaker 1
So do you think is, can directly identify which kind of role to present different kind of the content, right?

Unknown Speaker
I think

Unknown Speaker
for this, I maybe

Speaker 2
I when I use role, right? It's not only for my marketing, but mostly for, like, my interview process. Yeah. So I will give the AIP AI a road the background of the particular interviewer, and they will kind of generate what kind of potential question they will ask or like stuff around it. And I think it's pretty accurate so far.

Speaker 1
But, like, however, if the interview like you between me that I actually have a blur background of yourself, so can kind of just accurate to output that well, and then you should how, like, how to instruct it, um,

Speaker 2
I think all my interviews are face to face, so that is not applicable to me.

Speaker 1
So for example, we if you have an interview right now, how can you to make AI to better understand your audience, like which kind of the information you're gonna give it to

Speaker 3
him. I think it's mainly based on the kind

Speaker 2
of the background, the particular career background of the interviewer, and the particular job role, job description of the rule they have applied.

Unknown Speaker
Yeah, that kind of information,

Speaker 1
and it's gonna give you the progress that is you It thinks you're gonna be better to getting on the progress of the interview. So, yeah, so if you started to to to review the content is generated you cannot directly use, or you just review and optimize yourself.

Unknown Speaker
Review and optimize.

Speaker 1
So based on the content that AI generated, you think which part is the thing need to optimize?

Unknown Speaker
I think it's the way

Unknown Speaker
they phrase particular like introduction.

Speaker 2
Maybe it's sometimes too generalized or sometimes too rigid, yeah. So I will try to optimize by, you know, rephrasing the whole introduction, but, you know, keeping the main point that AI asked me to point it out and stuff like that.

Speaker 1
So were you to require AI to provide the references these sources or something

Unknown Speaker
not really

Speaker 1
the How can you verify the correctness of AI?

Speaker 2
Because this is like my experience. Do you know that I need to fit AI my own resume? I don't understand the question.

Speaker 1
It's about if you just rely on AI on the output of your work. How can you verify the correctness of AI's output if you don't require the relevant sources just recording on the correct the corresponding outputs that directly give it to you?

Speaker 2
Just to clarify that you are referring to that, do I need? Do I give AI certain information for them to, you

Unknown Speaker
know, come with the output,

Unknown Speaker
okay, yeah. I mean, I do give

Unknown Speaker
AI, you know,

Unknown Speaker
my resume and my background

Speaker 2
to ask them, help me generate an introduction, like, based on my strengths and weaknesses, experience

Unknown Speaker
to highlight based on

Unknown Speaker
that particular job description. Yeah.

Speaker 1
So like you are, like, how much you you trust in AI, after you give you the information and the things that is already up to you,

Speaker 2
I will say, I'll give it like 70% I'll trust it 70% like whatever I mentioned that 30% is really sometimes it's the way it phrase is. It's just not. I just think that's not personal right now. Yeah,

Unknown Speaker
so how can you deal with the 30%

Unknown Speaker
I will

Unknown Speaker
rephrase on my own,

Unknown Speaker
without the eye.

Speaker 1
Could you please just give me a scenario like one of your your working campaign, or one of your content output?

Unknown Speaker
Okay, for, for instance, when

Speaker 2
I want to write a particular Instagram post itself, it will, it will kind of give a very rigid bones. For example, a big day ahead. Today is a big day. We are pushing out a certain product and stuff like that, with a lot of on dash, with little explanation of that particular product. But for me, this is very rigid in a way. So I will we raise the whole thing and make it into,

Unknown Speaker
like a more personal story, like, you know,

Speaker 2
this, our company, how we started, and we are pushing out this brand, instead of making it into a very point, point form, like and make it your story.

Speaker 1
Okay, so if you optimize the content, will you give it back to AI to let him to check or something? Normally No, normally no, you just directly to Yes, yep. Okay. So in the entrepreneur scenario, you only use these kind of tools in the content output. Or do you ever use it to any like business development or something?

Unknown Speaker
I will use it to find particular

Unknown Speaker
product source as well.

Speaker 2
Yeah. For instance, last night, I was trying to find different brands

Unknown Speaker
in

Speaker 2
particular footprints that is being produced by a certain brand, or I'm looking for a certain product that's only produced by a few brands. Then I need more information about that. Then I think AI can pretty generate all the information I need instead of me, you know, going down to the Support Market, check it on my own.

Speaker 1
Okay, so have you ever feel the AI's uncertain effect your trust?

Speaker 2
I uh, it affect my trust in what

Speaker 1
in the process of your no matter, looking for the other partners, for the other like the you have imagined about the supply chain, for the products or anything, do you feel like the uncertain things maybe to affect your trust to the this kind of target audience.

Speaker 2
I I think I'm I'm not. I still quite, I don't really have this trust issue. Yeah. Sorry. Okay,

Speaker 1
so have you ever to face the financing kind of deal with this tools by using AI.

Unknown Speaker
No, well, finance, I don't use AI,

Speaker 1
okay, so you think all the marketing issues that you have already have in your business can be deal with AI, just you with it. There's no need for the team, right for for now?

Unknown Speaker
Yes,

Speaker 1
so since you adopt these tools, like, how has you decision making, quality and the kind of efficiency or thinking style be changed.

Speaker 2
Clarify. You're talking about how AI can affect my efficiency in work.

Unknown Speaker
Okay, okay,

Speaker 2
I think AI really increased my efficiency of work, yeah, by a lot. Yeah, and it kind of save our manpower course as well.

Speaker 1
So how is it's gonna to influence your decision making,

Speaker 2
in decision making in terms of like, what decision making, or what kind of decision

Unknown Speaker
making, no matter for the

Speaker 1
content decision, or for the the target audience decision, the creativity decision, the branding decision, this kind of decision, if you rely on AI will, how has it the serial quality gonna be be influenced.

Speaker 2
I think currently, my decision making is really, I would say, heavily dependent on AI.

Speaker 1
And yes, just, could you play a little bit they probably like before you, before AI generated. This Euro comes, what is kind of your decision making, like, your logic and after AI, what is your decision making? Quality changed?

Speaker 2
Okay? Maybe, for instance, back then, when I want to create a marketing campaign and stuff like that, right? It took me, like, hours to look online to see what's the viral post like, and stuff like this. But I think currently, AI can give me a very, very good like, you know, the bone, the structure of the post itself, and all I need to do is, like, pick it here and there. And I can, you know, that particular pose can be done within like, like, three minutes, max.

Unknown Speaker
Yeah, so this is the efficiency part.

Speaker 2
And for the rest back then, I think I hate, I rely heavily on my friends and family, I'll ask them, you know, what, what they feel about a particular pose, how, what kind of emotion you will evolve this kind of thing. But, yeah, now it's just me and the eye. So,

Speaker 1
okay, so how do you avoid over like, rely on it.

Unknown Speaker
I I,

Unknown Speaker
actually, I, I guess I

Speaker 2
don't have a clear boundary of what is over relying. Because I know I now rely heavily on AI, but what I can do is I will not, you know, 100% trust what the response AI give me instead of No, sometimes I'll still ask around to, you know, contract the particular information, yeah, and stuff like that.

Speaker 1
So you just started to using AI when you started your business, right?

Unknown Speaker
Yes.

Speaker 1
Okay, so do you feel maybe some improvements would you like to see in AI a better support your career?

Speaker 2
I guess it's the same old thing. I think the way it kind of bright sometimes it's not really. It's still too rigid and not set personal enough in putting in a particular context.

Speaker 1
Do you think is is somehow to influence your critical thinking?

Unknown Speaker
Yes, definitely.

Unknown Speaker
So if you have the the

Speaker 1
chance to redesign the system, do you think which kind of the instructions or which can of the tools to plug in, maybe to deal with this kind of problem, to let people to have a better, better way to thinking this way,

Speaker 2
it's like what I will do differently for AI to make sure that it can evoke critical thinking in us. Yes,

Unknown Speaker
I guess it

Speaker 2
is, instead of giving a very detailed answer, instead is they can actually kind of ask me back, what do I think, or what do I feel, giving me open ended question instead of like, you know, detailed response.

Speaker 1
Have you ever like question it about how it generated. Like, what, how, what is the progress is generated?

Unknown Speaker
Yes, I'll be curious about it.

Speaker 1
Okay, so have you ever questioned about it, like, what is the result is already generated? Like, um, why you generate this kind of content? To me, um, uh,

Unknown Speaker
I don't I.

Unknown Speaker
Why do we generate this kind of content?

Speaker 2
So far, I think I'm still, I don't have that kind of curiosity, because most of my questions are very factual.

Speaker 1
Okay, yeah, so is there any data using by AI,

Unknown Speaker
data using

Speaker 1
like the platform management, the dashboards, the the Google Trends, this kind of thing, gonna rely

Unknown Speaker
on AI. I'm not very sure about what you said.

Speaker 1
Like, if you doing the mock analysis right, you should deal with the easy platform dashboard. So if you to deal with this data. Do you use AI to help you to analyze it normally? No, okay, you are only focusing on the content domain or marketing, okay? So if you ever feel like the the AI in one or two years, gonna play essential roles for every one who wanted to start up in this, in this area.

Unknown Speaker
Okay, can we be like, I

Speaker 1
currently like, do you see the AI is a tool? Because I feel like you are totally like, collaborate with AI, like he is one of your partner here. Yeah. So AI is a role. Do you feel like involving in the EC business era in the next one or two years gonna be more and more and more essential?

Unknown Speaker
Yes, definitely.

Speaker 1
So how do you think people gonna to revise the AI collaboration, this kind of thing to if you can write and like aI collaboration guide for the for the new startup, for the new entrepreneurship in the E commerce, which key things would you like to highlight?

Unknown Speaker
Maybe the, maybe some copyright issue,

Unknown Speaker
like, for instance, if, if I ask

Speaker 2
AI to generate a particular idea, I mean, I feel it, of course, some of the basic idea that I have right now that it kind of helped me to optimize that basic idea. Maybe, you know, that will be, I can just, you know, tell AI not to, you know, give this kind of information to other user, since it's, I don't know, you can send me a copyright for that particular idea. Yeah.

Speaker 1
Oh, the copyright usually is really important. So how can you just make sure the copyright the AI generate for you is is like, unique and can be useful?

Speaker 2
Yeah? So I think, I think that's, that's why I am if you are talking about in the future, if everybody start using AI in this right? I think copyright issue is one of our biggest concern.

Speaker 1
So right now, have you ever tried to deal with this kind of issue?

Speaker 2
No, currently, no like, that's why sometimes I will keep the most of the ideas of myself, yeah, and give it like a non sensitive information, for instance.

Speaker 1
So have you ever, like be be troubled with that?

Unknown Speaker
Yes, actually, yes.

Speaker 1
Like, which kind of the output or container names give to you to let you through into some of the issues.

Unknown Speaker
Um, we are still on the page, on the copyright issue, right?

Speaker 2
Okay, so for me is I feel very troubled you sometimes. I hope I also, I do wish that, you know, I can be AI, like information, like some ideas that I have,

Unknown Speaker
and it can help me optimize. But

Speaker 2
my biggest fear is really just about, you know, my ideas, or the ideas that I share with AI might be, you know, share with other people. And yeah, it's all about the timing.

Speaker 1
Is there any experience situation where the AI, otherwise like contradict your your intra intuitions, the AI and my might just turn out to be right or wrong. Sorry, can't repeat your question. Like, if the if somehow you feel like, oh, the AI advice is totally like contradicted to your intuition. So and

Unknown Speaker
have you ever this kind of experience?

Unknown Speaker
Early, no, no,

Speaker 1
okay, so sort of think AI is seems too smart, or you feel maybe you know, in some ways it's gonna to miss these in your task.

Speaker 2
I do, I think, yeah, AI is very smart and and, sorry, what? So I can't really understand the second part,

Speaker 1
like it's just lead you to a wrong direction.

Unknown Speaker
Okay, okay. Hey, me too. Okay.

Speaker 2
Currently, the direction wise, I am still not so I haven't dwell into it so much that I can give information about that.

Speaker 1
Okay, so maybe it's just give you an analysis that look logical, but you you might just just bought it off law. And have you ever had this kind of experience before? No, no, no. So how did you decide to fully trust AI? And want to double check

Unknown Speaker
when I want to fully trust AI,

Unknown Speaker
when it's a very factual thing, I would say,

Speaker 2
like, for instance, I asked, How old is Singapore this year? I will fully trust AI. But if I'm talking about

Unknown Speaker
finding

Speaker 2
company specializing in a certain Yeah, or wherever, can I find my source company for my product? Yeah, that one I won't trust 100% I still need you based on my own experience and my connections.

Speaker 1
So do you have a like checklist for verification

Unknown Speaker
checklist. I don't

Speaker 2
really have a checklist, but what I can, I guess, for me, if I will go back to Google itself and try to check all the reviews and stuff and ask people around me about their opinions on that particular company, particular brand, particular products, yeah.

Speaker 1
So what if it just gives you an ambiguous answer? How do you like critically to interpret it?

Speaker 2
If it gave me a very big answer, normally, I'll just do research on my own.

Speaker 1
Okay, okay, so how could you to do the research progress to rely on your your searching tools, or using another AI or something

Unknown Speaker
I normally eat,

Speaker 2
I would just ask people around me based on my your like connections, human connections, company connections, yeah, I think that's the most accurate information.

Speaker 1
Okay, so have you ever tried to like this? If you want AI to present a plan like for certain strategy, that's that you have already output you have you ever tried to do like a, a, b test to plan A, and it's just to give you A Plan B and to make a comparation. Yes.

Speaker 2
Um. Uh, A, B, man, maybe is the way I maybe I'll go back to the easiest, like Instagram posts.

Unknown Speaker
Yeah, so sometimes I will try to

Unknown Speaker
ask AI to write the pose,

Unknown Speaker
you know, in a more

Speaker 2
kind of style that I want to achieve, different style they want to achieve, and see how people react to different style.

Unknown Speaker
Like, you know the output,

Speaker 1
how do you feel like the AI output just corresponding that you have already expected to the like, different style for the audience, their feedback?

Speaker 2
I think, in a way, for me, this is a more creativity thing, so I wouldn't say there's a definitely

Unknown Speaker
like correct or wrong answer, yeah.

Speaker 1
So for you, you know, there has so much of the professional copywriters professional career, it has to doing this kind of job. Do you think they're gonna like gradually to lost their job because of AI? Yes. Why? Social? So what is the difference between you? You feel like AI is between to from the the professional copywriters of professional creators.

Speaker 2
I think their role is definitely still relevant for now, in a way. However,

Unknown Speaker
I guess in the

Speaker 2
future, if it's being trained so well that they can really accommodate to know my style of writing and stuff like that. I think it's very tough, because now I will still, sometimes I'll still ask my copywriter friend to help me read through the Instagram post, and they'll give him some input about that,

Speaker 1
because I feel like before, we are trying to understand marketing, trying to understand the case studies, and we often to see some of the web papers from default advertisement. And the reason why they are not that concerned about copyright, because everything comes from the inspiration from the sense of the creators. And I think that is a thing which AI kind of lost. So how can you deal with this kind of problem, if you trying to make your content more sensible, more with some of the web Do you think I can help you to reach that out?

Speaker 2
Apparently, I don't think can do such thing yet, so I will still, yeah, that's why I think optimization still need to be done by humans itself.

Speaker 1
Okay, so you feel like this, this gonna do two parts to to output on AI, right? Yes. One is, can rely on people on their senses, and the other thing the AI supporter to give them instruction of where to go, right? Yes. Okay, so do you think maybe it's just encourage the people who is involved in this kind of air area to maybe think less or do less,

Unknown Speaker
think late and do less. I think it actually

Speaker 2
increase the efficiency rate with the like, kind of like AI can already give you, you know, save you a lot of time beforehand, so you can actually have more time, you know, derive more creativity, instead of, you know, blogging and like, research and stuff like that.

Speaker 1
That is means that I, if I understand right, if you just doing this thing in this healthy way, which means that you have already let your AI generated content really helpful to you, right? That, yes, like, how much percentage of the content it gives to you to support your business, to running through this smoothie? Yes, like, like, how much the percentage? Maybe.

Unknown Speaker
Okay, so,

Speaker 1
so, what is the risky you feel? Like, most concerned about yourself, like, if you just said about the copyrights, and do you have any others, like the securities, like the bias, these kind of things,

Speaker 2
my my insecurities in using AI in my

Speaker 1
situation, yes. And the things you concern the most,

Speaker 2
I guess, is really about speeding is sensitive information, and the sensitive information will be linked to other people.

Speaker 1
Okay? And how could you deal with the bias problems in the the process that you trying to to deal with the information that give it to you, like, like, the gender bias, or this kind of things may be already, like, resisted in this the era for long and How could you deal with this kind of bias? Currently, I haven't thought about it yet. Okay, so

Unknown Speaker
if you trying to say that

Speaker 1
the copywriter, the curator, the this kind of creativity, jobs gonna to just gradually be, be, be lost because of AI. So do you think ever the the concern about the how much homogenization, one where the the everyone use AI and strategies look alike?

Unknown Speaker
Yes, I think that's one of the concern.

Speaker 1
So how could you feel like it's gonna to be deal in the future? And how can you feel like a is is maybe have a way or tools or a better design for the product to help this kind of issue to be deal with.

Speaker 2
Then maybe, okay, maybe I won't take back my words. We still need humans to, you know, creativity.

Speaker 1
So here. So do you feel like after this so much of questions, can you just summarize your creative thinking in the whole progress of your Collaborate with AI and which kind of the things that you feel like maybe can optimize to help you to better creative and help you to make him more human being.

Unknown Speaker
I think you

Unknown Speaker
wait. Let me bring my thoughts together.

Unknown Speaker
It's okay, just take your time. I do

Speaker 2
you mind repeating the question again? I want to make sure I didn't miss out.

Speaker 1
Okay, so, because we have already discussion about the progress of your elaborate with AI, right? So, and you change your mind that you feel like, okay, human being really essential. So, but like, gradually, I feel like I have the same concern about you, like the creative drop gonna be instead of AI. But I think the thing is, really need to optimize because of AI cannot generate any relevant things that you want have, like the sense, the odds, the other webs that you want to have. So which kind of tools, if you are like a product manager here, or which kind of like instructions you might feel can output to deal with this kind of problem.

Speaker 2
I guess is, if I want to deal with I want AI to generate a particular style with what I will do is I'll feed it like all the pictures, all the captions that I like to AI to us, AI to you know, generate the kind of

Speaker 1
stuff you mean to personalize, yes, the information that I feed. So which we can you to be more like deep into the personalized here, which you mean,

Unknown Speaker
okay, for instance,

Speaker 2
Well, recently there's the studio guilty Wu ha about that. So maybe, if I want my particular, you know, art work to have that, I will, you know, feed it like all the photos from Studio Ghibli, and say a very similar artwork. That's why I, you know, the artwork will be more personalized to what I want to my demand,

Unknown Speaker
if we come into the

Speaker 1
customer journey, okay, now you are trying to to do a case, starting with AI, and you give you try to chat with it, with GPT, and just visualize, realize your your your expectations. Could you please? Maybe you How can need to help you to more personalized

Speaker 2
in you mean from a customer journey like, Sorry, can repeat,

Speaker 1
the customer journey is between you and AI. You is AI's customer, and now you are giving him it a case, and you gave it instructions. You talk to it, and you want it to be more personalized. So which you do think maybe realization, the the whole journey is just to be more personalized. Okay,

Speaker 2
okay, maybe for me, it's like if I were to write a post that will be targeted to Singaporeans, for instance, maybe I will. I'll feed it like, you know, how Singapore or the KOLs, how they describe their pose, how how did they talk? How did it relate to, you know, Singapore audience, the way they speak, the way they write. I guess I will. I will, you know us AI to write the same kind of style. For example, it's English. If I want to make it even more personal, I prefer you know that particular statement have a lot of Singlish, but being professional enough, then I will ask it to refer to locate, how, how Singapore The mostly killers. How did it? Yeah,

Speaker 1
but if you come to kill out this kind of thing, you can feel like QL, the key opinions list. They have all their personalities here, if they want to like sustainable development in the this kind of era. So if AI just continues to development, do you think KOLs may be kind of a hard like Rose, if playing in marketing this? Come to me,

Speaker 2
I think it's still quite hard, because Kol is still, you know, a real person that people you know, can walk and look up to and stuff like that, yeah. So if I think thinking is there's still a pretty long way to that.

Speaker 1
Okay, so if we talk to this far, do you think that future entrepreneurs need most in not more AI but stronger critical thinking.

Unknown Speaker
Yes,

Unknown Speaker
more AI and more critical thinking. Yes,

Unknown Speaker
okay, so

Speaker 1
I think for now is all about your entrepreneurs and yes, so I want to have some like future things. So which kind of the roles like for your future business, I must sure that you gonna to expand it. Will you have like, more team members to help you with your marketing issues, or AI still gonna be essential just to work with you.

Speaker 2
I think I'm there. Definitely. We still, we will still hire more people in the creativity area, like population and stuff like that. But I will say we won't need it as many as what we use. We might need to have

Unknown Speaker
it due to AI,

Speaker 1
okay, okay, so I think that is all. So do you have any like questions, or do you have any

Unknown Speaker
thing you made me feel helpful?

Speaker 2
Can you actually tell me more about this experiment? Because I think the question you asked is quite interesting and really evoke a lot of different thinking. Yeah,

Speaker 1
yes, yes, because it's about creative thinking to different kind of the domains in this society we have, like lots of the audience, which actually now running into business. So it's actually a good and unique experience for me as well. So thank you so much for your participation.

Unknown Speaker
Yeah, thank you so much as well.

Speaker 1
Yes. And I just want to mention about the possible court review of the this. Our interview gonna to be related to the future studies, and if you would like to be open to follow up studies. Yes, yes, okay. And maybe we're gonna to have a in person tax of AI optimize, if you are free, do you want to come into the participation?

Speaker 2
Yes, like, if my schedule allows, because nowadays,

Speaker 1
not be in Singapore, yeah. Okay, okay, so I must most time in in Malaysia right now, right?

Unknown Speaker
Okay, thank you so

Speaker 1
for the end of question, I want to verify like, what is your professions and what did you major?

Unknown Speaker
My profession is actually entrepreneur.

Unknown Speaker
My major is like life science,

Unknown Speaker
life science, right? Life Science.

Speaker 1
And could you please let me know your age right now?

Unknown Speaker
Currently it's 27

Unknown Speaker
Oh, 27 so young.

Speaker 1
You are really great, and you have already have your own business right now. And for now, you feel like the AI, if I just summarize it, AI is an essential tools for you, and it's a kind of partnership and consultant in the whole progress of your business. And but in the end of the day, you feel like it still need people's critical thinking to help them to get more human being. And you feel like maybe you get off the thing progress of AI, maybe help you to to get the progress maybe more better, and to help you to get more creative thinking ways. So that is direct, if I, if I just summarize about your demonstration,

Unknown Speaker
yes.

Speaker 1
Okay, so yes, I think that is all. And we have spent about 15 minutes in doing this. I think it's pretty smoothly. So yes, thank you for your time. Thank you for the time as well. Thank you. And if you have any progress, I wanted to let you know. Okay, thank you so much. Thank you, bye, bye, bye.


受访人27:
WEBVTT

1
00:00:00.570 --> 00:00:01.820
Chin Ee Moon: Okay?

2
00:00:02.140 --> 00:00:12.740
yunjie.fan@ntu.edu.sg: 所以整体来说的话，这个研究的目的还是在聚焦于我们ai，还有我们本身的这个人类的critical thinking这一块，

3
00:00:12.740 --> 00:00:23.960
yunjie.fan@ntu.edu.sg: 听说有看到你刚刚在这个面试之前有看到你的这questionnaire，所以你本身是做research相关工作的，是吗？对。

4
00:00:23.960 --> 00:00:31.620
Chin Ee Moon: 是的，我是做比较多相关的那个工作，然后目前是在工作。

5
00:00:32.880 --> 00:00:39.670
yunjie.fan@ntu.edu.sg: 那你的这个研究的主要的一些方向。还有一些具体的课题，你方便介绍一下吗？

6
00:00:39.670 --> 00:00:55.570
Chin Ee Moon: 我的研究方向是，现在我主要是在做那个就是就是一个要。然后他想要把一个把一个protein

7
00:00:55.570 --> 00:01:11.050
Chin Ee Moon: 和一个拉在一起，然后要那个不好的啊。大致上的意义就是这样，就是就是一个，然后把两个东西黏在一起，然后让那个让那个让一个

8
00:01:11.150 --> 00:01:17.090
Chin Ee Moon: 好的好的去吃掉另外一个不好的，大致大概是这样的，方向。

9
00:01:17.580 --> 00:01:28.360
yunjie.fan@ntu.edu.sg: 好的，好的我能理解。所以你觉得你在你的这个研究或者说你的相关的工作过程当中使用ai比较多吗？

10
00:01:28.810 --> 00:01:39.600
Chin Ee Moon: 主要使用的还是这种ai会比较多，但是我觉得我是不会完全依赖在

11
00:01:39.600 --> 00:01:53.650
Chin Ee Moon: 就是他们给我的答案，然后我就直接拿来用，这样就是我还是会去跟我做的那个literary核对一下，然后看怎么样再选择，要不要就是接受那个ai的建议。

12
00:01:54.130 --> 00:01:58.000
yunjie.fan@ntu.edu.sg: 好的那你觉得一般什么样的场景会使用到ai啊？

13
00:01:58.550 --> 00:02:20.640
Chin Ee Moon: 就是比如说我对一个一个一个方向，或者是一个课题不太了解的时候，初步的那个我就会通过ai来缩小范围，然后后面我会再去google找相关的一些文章来看，因为我主要是我发现到那个ai给我的文章很多都不太靠谱

14
00:02:20.660 --> 00:02:38.350
Chin Ee Moon: 就是，就是他给那个对我来说是还行的。但是就是说，如果是在更深或者是更的问题，上面他们的答案还是就是跟还有相当大的差距。

15
00:02:39.120 --> 00:02:45.790
yunjie.fan@ntu.edu.sg: 那你是如何发现他的这个这个弊端是你直接通过经验吗？还是。

16
00:02:45.790 --> 00:03:03.790
Chin Ee Moon: 就是可能有时候我就会直接在那边开点开那个。就是让他也去搜那些网站，然后我就说，给我一些，然后我发现到他给我的跟我要找的资料，其实差距还蛮大的，就是等于说他乱给这样。

17
00:03:04.650 --> 00:03:11.930
yunjie.fan@ntu.edu.sg: 那你觉得如果这样子的情况之下，它有多大的可信性，或是有多大的可应用性，它的结果对。

18
00:03:12.240 --> 00:03:13.810
Chin Ee Moon: 它的结果吗？

19
00:03:13.980 --> 00:03:20.730
Chin Ee Moon: 直接应用的话，我觉得少于50。50%吧，就是现在来看的话。

20
00:03:21.880 --> 00:03:26.200
yunjie.fan@ntu.edu.sg: 这样来看的话，那你仍然还会有很高的使用频率吗？

21
00:03:26.200 --> 00:03:45.010
Chin Ee Moon: 我还是会有比较高的使用频率，就是我说我会先去一些了之后，然后我再去google上面核对，或者是更。问google一些更直接的问题，这样就是还是一个很好的工具来帮我，缩小我的范围。

22
00:03:45.970 --> 00:03:56.540
yunjie.fan@ntu.edu.sg: 请问你在这个过程当中是用你的母语，中文还是用英文去进行相关的这个疑问，或者说是相关的，这个这个信息给予啊。

23
00:03:56.740 --> 00:04:11.660
Chin Ee Moon: 我用的都是英文，因为就是我的教育背景和我的。就是做研究的那些研究所都是主要主要还是用英语比较多，华语就是口语还行，但是研究上不怎么用华语。

24
00:04:11.660 --> 00:04:28.450
yunjie.fan@ntu.edu.sg: 好的，好的，好的，那我了解。那面对这样的问题，你有没有去想过说去question ai，或者说是想要通过另外一个ai模型去跟它相辅相成的，去论证或者验证这样的事情呢？

25
00:04:28.660 --> 00:04:32.440
Chin Ee Moon: 有说法吗？在

26
00:04:32.440 --> 00:04:55.180
Chin Ee Moon: chatgpt我就会使用几个不一样的模式来来对比他们的答案就是那个O3，然后那个四，然后现在最近还有一个五，就是我会对比他们的答案。然后现在最近我也会用那个的那个来来来，就是进行一些一些对比。然后我觉得那个google的ai mode会相对来讲，给到我比较

27
00:04:55.180 --> 00:05:03.110
Chin Ee Moon: 我，我比较可可信任的答案就是，他会直接在google上面收一堆的那个给我这样子。

28
00:05:03.110 --> 00:05:08.740
yunjie.fan@ntu.edu.sg: 明白明白你有没有接受过。engineering的这个培训呀。

29
00:05:09.100 --> 00:05:10.060
Chin Ee Moon: 没有。

30
00:05:10.060 --> 00:05:10.930
yunjie.fan@ntu.edu.sg: 没有了好的。

31
00:05:10.930 --> 00:05:11.680
Chin Ee Moon: 没有。

32
00:05:11.830 --> 00:05:18.640
yunjie.fan@ntu.edu.sg: 好的。那现在你会不会有那种场景去刻意的不用ai。

33
00:05:19.130 --> 00:05:26.480
yunjie.fan@ntu.edu.sg: 比方说某一些研究工作或者某一些的这个特定的解决方式的时候，你会刻意的不用去ai。

34
00:05:27.220 --> 00:05:28.960
Chin Ee Moon: 刻意的不用ai。

35
00:05:28.960 --> 00:05:29.690
yunjie.fan@ntu.edu.sg: 对。

36
00:05:29.690 --> 00:05:45.390
Chin Ee Moon: 可能就是说，当那些资讯就是我要找的东西比较的情况下，我还是会靠自己的思考和就是google搜一些关键词比较多，不会直接把那个我的问题，到ai去

37
00:05:45.650 --> 00:05:49.610
Chin Ee Moon: 对，就是主要还是那个information safety的问题。

38
00:05:50.300 --> 00:05:55.220
yunjie.fan@ntu.edu.sg: 那您觉得就是比方说，您刚刚有提到的这种信息误差性，

39
00:05:55.220 --> 00:06:19.420
yunjie.fan@ntu.edu.sg: 你觉得多大程度之上可以可以避免这种错误存在。如果说你作为一个产品设计的人员，或者说是如果你在这个ai应用当中，不是通过自己来发现问题，你是希望如何能够看出来整个ai给你呈现的结果可以更直观的这种出错性，你有没有这种这种相关的想法呢？

40
00:06:20.030 --> 00:06:25.100
Chin Ee Moon: 就是怎么设计那个ai，它会它的那个对的几率会更大，对吧？

41
00:06:25.100 --> 00:06:36.240
yunjie.fan@ntu.edu.sg: 一个是这方面，另外一个是你认为ai这个，因为我们其实看到ai，它目前还是一个对话形式的存在，你给它一个什么信息，它可能很直观的给你看到一些信息，你其实

42
00:06:36.240 --> 00:06:55.830
yunjie.fan@ntu.edu.sg: 像你来说的话是比较有变质性思考的，人，比方比较有批判性的，这些思考的，但是很多可能并没有这一种能力的一些受众，他并不能绝对的分辨ai，的它的这个信息，它的这个准确性或者说是它的可用性，

43
00:06:55.830 --> 00:06:56.440
yunjie.fan@ntu.edu.sg: 你觉得

44
00:06:56.440 --> 00:07:05.130
yunjie.fan@ntu.edu.sg: 有哪些？一个是从直观上来看，或者说是从它的，它的这个功能上来看，可以会更好的去避免这样的薄弱环节。

45
00:07:05.530 --> 00:07:09.790
Chin Ee Moon: 我感觉就是，

46
00:07:10.360 --> 00:07:13.220
Chin Ee Moon: 把那个ai跟google

47
00:07:13.220 --> 00:07:30.480
Chin Ee Moon: 做更好的会很好，会会会更好，就是所有ai说过的话，如果都有reference去去帮助，就是去提升他说的那句话的那个意义的话，我感觉可能就能够更能够看出它的准确性有多少。

48
00:07:30.860 --> 00:07:31.810
yunjie.fan@ntu.edu.sg: 明白，

49
00:07:32.320 --> 00:07:38.670
yunjie.fan@ntu.edu.sg: 所以你在使用的时候，你大概是一个以什么样的口吻去

50
00:07:38.810 --> 00:07:54.560
yunjie.fan@ntu.edu.sg: 去费它呢？是是你扮演某种角色，还是说，你去让ai去尝试扮演某种角色，比方说你的，你的这个。你的这个peer或者说是你的一些reviewer，你会不会有这样的一些环节？

51
00:07:54.990 --> 00:08:19.870
Chin Ee Moon: 就是我会跟他说，现在我要我要做的task主要是什么，然后我需要他给我怎样的？就是。然后我会说，可能比较比较的一个方向就是多少个字啊，然后要限制在怎么样啊？然后我的观众会是什么样的群体的人，我需要他帮我写的有多professional多严谨，这些我都会先提前给他，然后之后他就给我一个，

52
00:08:19.870 --> 00:08:30.650
Chin Ee Moon: 我觉得就是把那个方向缩小了之后，他给的答案会更加的准确一些。比起你就很的问他一个很的问题，这样。

53
00:08:31.510 --> 00:08:44.980
yunjie.fan@ntu.edu.sg: 那一般ai给出的这些答案，有一些它是有reference的，你可以看出它的这个漏洞。那如果说直接性的答案，你怎么能辨证它的这个真实性或者可用性呢？

54
00:08:44.980 --> 00:08:49.540
Chin Ee Moon: 那如果有一些是

55
00:08:49.540 --> 00:09:13.030
Chin Ee Moon: 比较属于那种的问题，就是没有那么多相关的问题的话，那我觉得我还是会直接使用的，但是如果他说到了一些跟相关，我需要用一些去支持他的这种说法的时候，可能我就会把那他说的那一句话，整个看下有没有相关的资料，就是我能不能够在那些上面

56
00:09:13.030 --> 00:09:23.670
Chin Ee Moon: 到跟他同样跟ai写出同样的观点来来这样辨认，就是我能不能够把那个句子这样写下去，或者是应用去我想用的地方。

57
00:09:23.960 --> 00:09:35.600
yunjie.fan@ntu.edu.sg: Okay。所以说你还是会很大程度上需要搜索引擎的支持，然后去辅助你去验证这个ai的内容是否有足够的。这个背书性，对吧？

58
00:09:35.600 --> 00:09:36.120
Chin Ee Moon: 对对。

59
00:09:36.120 --> 00:09:44.680
yunjie.fan@ntu.edu.sg: 那在这个过程当中的话，你有没有去把你已经返补回来的资料，再重新给到ai去让它优化之类的。

60
00:09:44.870 --> 00:10:06.400
Chin Ee Moon: 我会的就是。就是我，我找了一遍之后，然后我就会收集所有的那些资料就是不管是ai给的还是我透过google找到的，我都会写下来，然后过后我再进去，然后叫他帮我improve那个内容，然后在这个过程中，其实在写作方面，我觉得他帮到我是挺多的，

61
00:10:06.400 --> 00:10:13.290
Chin Ee Moon: 就是他真的能够帮我。improve我的整个我的整个flow。那个写作方面对

62
00:10:13.560 --> 00:10:14.970
Chin Ee Moon: 这样的帮助还是挺大的。

63
00:10:14.970 --> 00:10:15.500
yunjie.fan@ntu.edu.sg: 为什么

64
00:10:15.610 --> 00:10:30.340
yunjie.fan@ntu.edu.sg: 对在你整个去进行research这个环节，我方便问一下，整个在这个流程当中，ai充当了一个大概什么样的角色吗？它只是帮你去润色你的内容吗？还是说给您提供一个相对的思路。

65
00:10:30.700 --> 00:10:31.170
Chin Ee Moon: 对对。

66
00:10:31.170 --> 00:10:34.960
yunjie.fan@ntu.edu.sg: 大概的这个，这个总体的角色会是怎样的。

67
00:10:35.350 --> 00:10:56.460
Chin Ee Moon: 我觉得它给我一个思路之后，然后我会再从那个大框架那里做出一些调整，跟自己的思考，然后过后把我的内容先编辑出来，然后之后呢，再利用它去优化，所以我觉得它就是充当一个前面跟一个后面这样的角色，然后，中间的那一段就是由我自己来补充。

68
00:10:56.680 --> 00:10:59.680
yunjie.fan@ntu.edu.sg: 明白那有没有这种情况，ai

69
00:10:59.820 --> 00:11:14.930
yunjie.fan@ntu.edu.sg: 在最一开始其实就给您误导到了一个错误的方向，然后当然，因为他本身，你认为他很聪明，他的可信性很强，你伴随着他的这个误导，走了很长的时间。有没有这种情况？

70
00:11:14.930 --> 00:11:19.900
Chin Ee Moon: 有时有时候也是也是会有的。但是。

71
00:11:19.900 --> 00:11:43.450
Chin Ee Moon: 就可能之前这样，这样就是有发生这样的情况，我没发现的时候，最后还是通过，就是问了别人比较一点的那些啊，或者还是我的这样，然后他们才给我说其实有时候A那个ai讲的不太可信，你从一开始就就就不太能相信了，这样就是还是有，但是我觉得还是比较小的情况，因为通常如果

72
00:11:43.450 --> 00:11:46.600
Chin Ee Moon: 我需要用ai来帮我制造一些

73
00:11:46.660 --> 00:12:00.380
Chin Ee Moon: 开头的思路的时候，我给他的问题都会偏比较general一些，然后我之前也说了，就是通常general没那么高的的时候，ai的那个信任度还是蛮高的。

74
00:12:00.620 --> 00:12:10.210
yunjie.fan@ntu.edu.sg: 明白你方便就是大概给我一个什么样的方向，它是相对来说依据较少可能会导致出错的吗？

75
00:12:10.370 --> 00:12:31.570
Chin Ee Moon: 因为我做的工作比较多。是，跟嘛。然后这个在化学上面，有时候我就问他说，为什么我的这一步实验会会出错呢？那个我就给他了我的。然后我就问他，那个出错的原因在哪里，为什么我的那个又会那么低，然后他就会开始给我乱说一通。

76
00:12:31.570 --> 00:12:39.610
yunjie.fan@ntu.edu.sg: 所以这种这种错误答案，您是通过什么样的思维发现它是它是错误的。

77
00:12:39.990 --> 00:12:40.580
Chin Ee Moon: 就是你。

78
00:12:40.780 --> 00:12:43.140
yunjie.fan@ntu.edu.sg: 尝试着去纠正它吗。

79
00:12:43.420 --> 00:13:08.250
Chin Ee Moon: 我就会先去询问一些比我有经验的人问他们说，就是。我发现有这样的问题，然后。然后。有没有可能我的问题，我，我的这个实验出现了这样的问题，是因为一，二，三原因，which is那个ai跟我讲的答案，然后他们就说啊，好像不太正确，好像不太是这样的，这样我就发现了，然后我就会再去google

80
00:13:08.250 --> 00:13:13.600
Chin Ee Moon: 再去找其他的有可能的那个，那个问题是是在哪里？这样

81
00:13:13.600 --> 00:13:20.620
Chin Ee Moon: 对，就是在这种比较特定的一个场景之下，可能ai给的答案会没有的那么

82
00:13:20.690 --> 00:13:22.430
Chin Ee Moon: 准确。对。

83
00:13:22.770 --> 00:13:36.710
yunjie.fan@ntu.edu.sg: 了解了解。所以你在发现问题之后，你是更大程度上依靠你自己去进行这个问题的解决，还是说我在发现问题之后，我告诉他问题在哪，然后再让他去给我另外一个答案呢？

84
00:13:36.870 --> 00:13:48.640
Chin Ee Moon: 没有比较少，就是通常我发现他错了，我就不会再问他。就是相关的问题，我就会直接去找其他的地方就是就是答的答案对。

85
00:13:49.060 --> 00:14:08.640
yunjie.fan@ntu.edu.sg: 明白。所以说正常来讲，我们整个一个一个research project过来之后。你是大概去怎么样去跟他进行沟通互动的是，你首先会告诉他我整个的这一个实验他的一个前提，然后你们会在一个对话框里面去完成你整样整个的这个协作过程是吗？

86
00:14:08.640 --> 00:14:14.570
Chin Ee Moon: 对对，对，通常我就会针对一个问题，然后我就开一个这样。

87
00:14:14.570 --> 00:14:32.840
Chin Ee Moon: 然后如果是下一个问题了，我就会去下一个，因为有时候那个ai它会一直延续下去，就是可能你到不一样的问题了，但是它，因为还在同一个那个框里面，然后它就会再refer回之前你问的问题有时候已经是没有相关的问题了，但它硬硬会跟你扯关系。这样。

88
00:14:33.490 --> 00:14:37.700
yunjie.fan@ntu.edu.sg: 方便问一下您现在用的是gpt的哪一个模型呢？

89
00:14:38.060 --> 00:14:44.210
Chin Ee Moon: 好等一下，我看一下，好像是那个gpt5吧，就是我有那个subscription。

90
00:14:44.400 --> 00:14:46.600
yunjie.fan@ntu.edu.sg: 有有到plus版本是吧？

91
00:14:47.520 --> 00:14:50.760
Chin Ee Moon: 对，就是对，就是那个plus版本。

92
00:14:50.760 --> 00:15:00.880
yunjie.fan@ntu.edu.sg: 了解了解，就您刚刚提到的问题，其实很多人会会有同样的这个，这个发现这个弊端，就是说ai它其实会。

93
00:15:00.880 --> 00:15:03.740
Chin Ee Moon: 思路混乱，而且他会记忆混乱。

94
00:15:03.740 --> 00:15:18.570
yunjie.fan@ntu.edu.sg: 可能在您继续下一个subject的时候，在某一个特性点的时候，他仍然会提起之前的答案。所以你觉得如果这种答案框定的这种这种问题你觉得有没有可能避免呢？

95
00:15:19.350 --> 00:15:44.200
Chin Ee Moon: 有没有可能避免？就是在我跟他的沟通过程中，可能我就会在开始一个新的的时候，我就先提前的跟他说，就是这已经跟上面的那个问题不一样了。所以现在是一个。我们要开启一个新的一个，然后这个是关于什么什么，然后再给他一个新的。我觉得这样或许会比较好，但我还是会比较prefer，就是开一个新的那个

96
00:15:44.200 --> 00:15:44.980
Chin Ee Moon: check boy.

97
00:15:45.320 --> 00:15:59.960
yunjie.fan@ntu.edu.sg: 了解，了解，所以您会不会去，比方说就某一个问题让他给您like a，b test2个答案，然后你自己去辩证的去给到相应的这个继续的执行应用，这样。

98
00:16:00.450 --> 00:16:19.120
Chin Ee Moon: 会的，它有时候会就会弹出来，就是给了response a跟response b。然后他问你，你比较喜欢哪一个？但是我觉得内容上来说的话，其实两个response的那个内容会差不多，一样，可能只是呈现的方式没有太大的差距，所以其实我也不太介意这个东西。

99
00:16:19.320 --> 00:16:37.420
yunjie.fan@ntu.edu.sg: 了解，但是就ai出错的这个问题，因为您是也是比较敏感度的可以发现ai的问题有没有可能就是在这个过程当中，您可能设立一些对立性的问题，让ai去提一些反问的，反对的。这些论证，这个过程有吗？对。

100
00:16:37.610 --> 00:16:39.850
Chin Ee Moon: 没有，暂时还没尝试过。

101
00:16:40.230 --> 00:16:48.230
yunjie.fan@ntu.edu.sg: 了解了解，那您觉得如果说就激发ai它本身的一种批判性思维的话，可以通过哪些方式啊？

102
00:16:50.140 --> 00:17:11.099
Chin Ee Moon: 就是跟他说就是。比如说他给了一个答案，然后就跟他说，你错了，你可以再想想嘛，然后可能他又会想一个新的，但是我觉得这个。我不确定没试过，但是我不知道这样的方法会不会让他那个思考变得更加的混乱，然后后面给的答案就越来越偏离那个那个，那个topic。这样。

103
00:17:11.490 --> 00:17:20.310
yunjie.fan@ntu.edu.sg: 明白，所以其实因为我看您年纪还小了，所以您在这个ai使用的时候是从哪一年开始的。

104
00:17:21.109 --> 00:17:37.889
Chin Ee Moon: 就是那个刚刚开始的时候吧，2023年吧，但是我就是很很时常的真的是使用它应该也是从2024年大概七，8月左右，对。

105
00:17:38.650 --> 00:17:51.480
yunjie.fan@ntu.edu.sg: 明白，那您觉得在此前跟之后您自己的这个思考有很大的变化吗？或者说针对于一些这个您本身工作，还有专业领域的这种思路。

106
00:17:51.990 --> 00:18:01.060
Chin Ee Moon: 我觉得他确实是帮我省了很多的时间，但是同时我也觉得可能我的思考会比以前

107
00:18:01.060 --> 00:18:10.560
Chin Ee Moon: 浅了挺多，因为就是我会对他有一定的那个依赖性，就是遇到了什么不知道的，我就会想要去问他，而不是先自己去思考一遍，

108
00:18:10.560 --> 00:18:21.220
Chin Ee Moon: 这样，除非我真的从他那里问不出问题，然后我才会想办法自己去再去思考，那可能就跟以前我还没开始用ai的时候，就有一一点点不一样了。

109
00:18:21.780 --> 00:18:33.240
yunjie.fan@ntu.edu.sg: 了解。所以说你觉得这种其实是对于学者或说是对于研究员来说，他是更多的prone才是更多的。

110
00:18:35.330 --> 00:18:58.130
Chin Ee Moon: 我觉得还是主要是看个人的那个使用方式吧，就是如果你有意识到，就是他可能会阻拦你一些思想，一些思考，那你就还是的去跟自己说，我一定要去想那个问题在哪里，是是因为怎么样，然后要去把自己的那个思考，投入在自己的工作里面，那我觉得他其实pro

111
00:18:58.130 --> 00:19:06.100
Chin Ee Moon: 挺多的，就是在协作上面，它帮你节省了时间，然后可能一开始的那个构架上也会帮助你，就是

112
00:19:06.130 --> 00:19:18.510
Chin Ee Moon: 只要配合的好，我觉得其实它是好的，但是如果是过度的使用，或者是就让它压过了你所有的思考，那可能就不好了。所以还是个人原因比较多。

113
00:19:19.300 --> 00:19:27.910
yunjie.fan@ntu.edu.sg: 了解，所以您现在的整个工作的协作方式，你觉得更大程度上是跟ai还是说跟您个人的团队啊。

114
00:19:31.710 --> 00:19:36.980
Chin Ee Moon: 就是写作嘛，就是在在在写东西上面，我觉得跟ai会比较多。

115
00:19:38.420 --> 00:19:56.220
yunjie.fan@ntu.edu.sg: 明白了解您，您觉得如果就您的专业领域去给到未来的研究人员跟ai协作的建议的话，如果说你要提出一个这个相关的guideline，你觉得如果说你去keep三点，你觉得哪三点是比较重要的。就您的专业来讲。

116
00:19:57.730 --> 00:20:02.890
Chin Ee Moon: 就是怎么让人跟ai有更好的合作，在写作上面。

117
00:20:02.890 --> 00:20:03.940
yunjie.fan@ntu.edu.sg: 对对。

118
00:20:06.060 --> 00:20:07.510
Chin Ee Moon: 我感觉

119
00:20:07.710 --> 00:20:10.910
Chin Ee Moon: 第一点肯定就是。

120
00:20:11.520 --> 00:20:35.430
Chin Ee Moon: 可能说吧，就是有时候ai可能两个人问他一样的问题，他就会很相似的答案，那就很容易出现那种的问题啊，或者是那个ai ai写作的那个感觉太重了。我觉得如果能够让他写作的更自然，更像人一些的话，我觉得那那会更好。

121
00:20:35.580 --> 00:20:39.690
Chin Ee Moon: 对，这是这是第一点，就是可能他也能够根据。

122
00:20:39.970 --> 00:20:49.540
Chin Ee Moon: 每个人的一些需求，或者是研究的一些不一样的地方，上面做出一些写作上面的调整。

123
00:20:49.540 --> 00:20:50.110
yunjie.fan@ntu.edu.sg: 那个。

124
00:20:50.110 --> 00:20:53.880
Chin Ee Moon: 那那就会比较好。这是第一点吧，我觉得。

125
00:20:55.990 --> 00:21:03.590
Chin Ee Moon: 这也是主要我现在看到的一点对，就是有时候它真的出来，你就很明显的看得出它就是一个ai写的。

126
00:21:05.230 --> 00:21:13.750
Chin Ee Moon: 是它的惯用词吧，对它的惯用词跟它的句子构造对这两点是主要还是比较明显的。

127
00:21:14.750 --> 00:21:29.190
yunjie.fan@ntu.edu.sg: 那本身对于这种问题，你觉得它更大程度上的给你带来的这这这个这个壁垒在于它的形式还是在于它的内容啊，因为其实如果说就研究来讲的话，

128
00:21:29.190 --> 00:21:43.700
yunjie.fan@ntu.edu.sg: 它更大程度上要看这个内容的严谨性啊，或者说是启发性啊等等的。其实ai如果说就您来讲的话，它本身在内容框架上可能更偏了一种一种这个形式或者主流，

129
00:21:43.700 --> 00:21:49.310
yunjie.fan@ntu.edu.sg: 您觉得他在于这个启发性的严谨性，上面也给到您一些比较大的困扰吗？

130
00:21:49.630 --> 00:22:14.510
Chin Ee Moon: 目前是没这么觉得因为主要的内容跟我就是我产出的内容，我是不会依赖那个ai给的答案来写。通常我都会把所有的我想要加加入的内容都写好了之后，我只是拿它来做一个修饰，所以我主要还是看他的那些啊，或者是他，他他是怎么怎么让我的整个的

131
00:22:14.510 --> 00:22:16.980
Chin Ee Moon: 的storytelling更好。

132
00:22:16.980 --> 00:22:26.400
Chin Ee Moon: 这样的一个一个功能，对，就是内容来说，我还是会比较严谨一些，就是自己自己的想法，跟自己的产出比较多。

133
00:22:27.340 --> 00:22:32.400
yunjie.fan@ntu.edu.sg: 那有没有说您比较担心的问题。比方说，

134
00:22:32.400 --> 00:22:51.360
yunjie.fan@ntu.edu.sg: 不论是合规性啊，一些伦理合规性，包括是它本身的一些数据安全性。还有像您刚刚提到的一些假文献，他可能自己去去杜撰出来的一些一些内容和文献装入是是是一些比较比较比较有用，可靠的引用

135
00:22:51.360 --> 00:22:58.540
yunjie.fan@ntu.edu.sg: 这些问题，你觉得哪？如果说对于你来你的这个领域的风险，你觉得哪些是比较比较严重的。

136
00:22:59.370 --> 00:23:02.800
Chin Ee Moon: 风险。我觉得假假文献的风险最大吧

137
00:23:02.880 --> 00:23:21.120
Chin Ee Moon: 就是如果有些人他们不去了，之后再拿来用的话，那那就一团糟了，对那那写出来的所有东西都是没有依据的，然后然后也就不这么可靠，然后可用性也不大了。

138
00:23:21.310 --> 00:23:25.090
Chin Ee Moon: 对，我觉得假文献的这一方面会比较严重。

139
00:23:25.660 --> 00:23:44.650
yunjie.fan@ntu.edu.sg: 因为我们之前在采访一些，比方说去做这个人力资源相关的一些，一些这个候选人的时候，他们可能会面临到这个ai的这个伦理性，他们可能在一些大数据的整理过程当中已经被一些bias所洗脑，比方说。

140
00:23:44.650 --> 00:23:45.570
Chin Ee Moon: 女性嘛。

141
00:23:45.570 --> 00:24:05.310
yunjie.fan@ntu.edu.sg: 这个求职方向啊。比方说，女性在某一些岗位上的她的最佳年龄啊等等的。对于您的这个医学领域来讲的话，是否也会有这样的一些歧视，或者说是一些一些不好的偏好，导致您本身会带来一定的困扰，或者带来一定的漏洞。

142
00:24:08.030 --> 00:24:14.670
Chin Ee Moon: 暂时来说，我是没有给他特别那种subjective的问题就是

143
00:24:14.670 --> 00:24:25.420
Chin Ee Moon: 会给出这种这种的答案，我主要还是依靠它来寻找fact比较多，所以那个fact来说的话我就能够直接的就是参考别的，

144
00:24:25.420 --> 00:24:35.250
Chin Ee Moon: 别别别的那个网站，然后我就知道了他是对还是错这样。所以关于这一类的问题，我还是没没怎么没怎么接触过。

145
00:24:35.750 --> 00:24:51.590
yunjie.fan@ntu.edu.sg: 所以您之前也总结出来了，您大概对于ai给您带来的这种风险困扰。所以你觉得ai系统你觉得最大的。对于你来讲，它的这个改进点在哪里？是支持这种医学类的研究，对。

146
00:24:52.880 --> 00:24:54.520
Chin Ee Moon: 哪里的thing？

147
00:24:54.730 --> 00:24:58.020
Chin Ee Moon: Ok

148
00:24:59.630 --> 00:25:04.770
Chin Ee Moon: 我觉得除了我刚刚说的那个就是他的，是要对

149
00:25:04.860 --> 00:25:23.810
Chin Ee Moon: 以外，还有就是可能他在一些还是一些上面的进步，我觉得也是可以有的就是有时候我会让他一些比较简单的出来就是增强我的那个理解。但是呢，他

150
00:25:23.810 --> 00:25:26.140
Chin Ee Moon: 通常january出来的都

151
00:25:26.170 --> 00:25:43.260
Chin Ee Moon: 不太像是我需要找的东西。对，就是它可能会复杂化，就是可能，我要的东西就是一个X1个，然后怎么样，他们的那个那个关系是怎么样，但是它就整理出了一堆非常复杂的东西，然后那个能够那个可用性就变低了。

152
00:25:43.770 --> 00:25:50.790
yunjie.fan@ntu.edu.sg: 明白，对，我觉得您觉得在这个ai的这个版本迭代的过程当中，你有没有觉得说？

153
00:25:51.330 --> 00:26:05.330
yunjie.fan@ntu.edu.sg: 其实伴随着版本迭代，他的思考可能越来越复杂，或者说是越来越可能没有像3.5或者说四的那个阶段，他给出的那个答案好像更加直接，您会不会有这种感觉呢？

154
00:26:05.860 --> 00:26:15.180
Chin Ee Moon: 我，我的确是有觉得跟著他的这个的进步，他给出的答案也的确是有一些改进，也的确是有一些进步的。

155
00:26:15.740 --> 00:26:32.710
yunjie.fan@ntu.edu.sg: 明白，那你有没有比方说，进一步的，我就是在做这个研究的时候，你要给我相关的reference dui一些department的id啊，或者一些数据库的解锁日期，这样去帮助您去更好的，有直接应用型的这种结果。

156
00:26:32.710 --> 00:26:44.920
Chin Ee Moon: 有的，有的就是他。很多时候他给的那些啊，他给的那些所有的那些资料其实都是对的，只是和我问的问题没有太大的关系。

157
00:26:44.920 --> 00:26:45.810
yunjie.fan@ntu.edu.sg: 了解没有。

158
00:26:45.810 --> 00:26:46.410
Chin Ee Moon: 对对。

159
00:26:48.140 --> 00:26:53.770
yunjie.fan@ntu.edu.sg: 好的。所以整个在您的这个使用过程当中，如果说

160
00:26:53.780 --> 00:27:18.750
yunjie.fan@ntu.edu.sg: 面临未来ai可视化的一个问题，因为现在只是一个对话的一个形式，那对话，一个形式。其实最简单的一种人机交互的一个形式，或者说是最直观的人机交互形式。如果说将来你觉得比较好的一种情况。就在这种realization的方向，你觉得比较好的一种一种协调，是更好的，是一种什么样的形式呢？如果说除了对话，或者说你认为对话，也可以去进步一下，

161
00:27:18.750 --> 00:27:19.450
yunjie.fan@ntu.edu.sg: 谢谢。

162
00:27:20.210 --> 00:27:22.240
Chin Ee Moon: 对话，

163
00:27:29.150 --> 00:27:34.050
Chin Ee Moon: 我暂时觉得对话应该是最

164
00:27:34.080 --> 00:27:46.420
Chin Ee Moon: 最有效的那个，那个方式就是它能够通过退化来进行更多的那个，那个改进和调整，可能我觉得有一点可能需要比较的就是

165
00:27:46.420 --> 00:28:09.250
Chin Ee Moon: 如果能进步的话，那就是你在问他问题的时候，他就很能能很精准的就是到你，你，你想要问的到底是什么东西，因为有时候你问了一个问题，然后他就给了你很的答案。然后你要再问多几回，他才会真的是能够get到。你原来想问的是这个啊，这样子就是可能这个是

166
00:28:09.250 --> 00:28:14.240
Chin Ee Moon: 他的sensitivity，如果能再进步一些的话，我觉得是会更好的。

167
00:28:14.780 --> 00:28:17.450
Chin Ee Moon: 对问题的敏感度对。

168
00:28:18.190 --> 00:28:19.220
yunjie.fan@ntu.edu.sg: 好的。

169
00:28:19.440 --> 00:28:29.140
yunjie.fan@ntu.edu.sg: 那本身在医学当中的话，其实它是有比较大的严谨，在我理解它是有非常大的严谨性的，

170
00:28:29.140 --> 00:28:43.710
yunjie.fan@ntu.edu.sg: 那你觉得就是因为我在采访很多的这个，这个受访人当中，其实我觉得像您本身认为百50%，它的可信性，这个其实是在大多数的受访者当中，它是偏低的，您觉得。

171
00:28:44.080 --> 00:28:46.140
yunjie.fan@ntu.edu.sg: 学科是否有关呢？

172
00:28:46.140 --> 00:28:47.050
Chin Ee Moon: 跟什么。

173
00:28:47.050 --> 00:28:49.320
yunjie.fan@ntu.edu.sg: 您的学科，您的领域。

174
00:28:49.780 --> 00:29:08.440
Chin Ee Moon: 我，我觉得是吧，因为我们需要非常精准的一些答案跟观点就是如果有稍微一些不严谨的都会被被被问这样，所以我们需要一个很准确的答案，但是ai很多时候还是会摇摆或者是它的答案还是没有说到重点

175
00:29:08.570 --> 00:29:13.810
Chin Ee Moon: 这样，所以如果是参考内容的话，我会比较少用ai。

176
00:29:14.790 --> 00:29:21.490
yunjie.fan@ntu.edu.sg: 对您更好的把ai定义成它是您的assistant还是他的，还是你的collaborator？

177
00:29:22.290 --> 00:29:25.220
Chin Ee Moon: Assistant，现阶段还是assistant。

178
00:29:25.220 --> 00:29:26.520
yunjie.fan@ntu.edu.sg: 好的好的。

179
00:29:26.650 --> 00:29:30.720
yunjie.fan@ntu.edu.sg: 那您更大情况上希望他扮演什么样的角色。

180
00:29:33.050 --> 00:29:39.510
Chin Ee Moon: 更大情况我觉得能进步，能当个collaborator，其实也还是

181
00:29:39.510 --> 00:29:53.180
Chin Ee Moon: 一个不错的方向，就是如果他能给他给出的答案就是有一定的那个思考性跟一定的那个参考程度的话，那肯定对于我们来说也会是很大的帮助。

182
00:29:53.230 --> 00:29:59.120
Chin Ee Moon: 只是现现阶段我觉得他还不太具备这种相关的能力。

183
00:30:00.260 --> 00:30:18.000
yunjie.fan@ntu.edu.sg: 那如果我们回到科研的这个层面上来说，其实大部分的目前的这个researcher，或者说是去做相关学术相关领域的这些工作人员还是会比较大量或者重量使用ai的一部分人群。但是

184
00:30:18.000 --> 00:30:26.890
yunjie.fan@ntu.edu.sg: 你如何看待这个ai协作跟学术不端就是这两个事情，它之间的这个边界呢？

185
00:30:31.380 --> 00:30:36.720
Chin Ee Moon: 就是说滥用了ai，然后导致导致学术上的一些退步，这样。

186
00:30:36.910 --> 00:30:39.630
yunjie.fan@ntu.edu.sg: 就是like academic misconduct。

187
00:30:40.010 --> 00:30:41.350
Chin Ee Moon: 喔喔

188
00:30:46.370 --> 00:30:48.080
Chin Ee Moon: 就是

189
00:30:48.190 --> 00:31:13.140
Chin Ee Moon: 所以我觉得还是就是要谨慎的使用那个ai，就是你要知道在什么情况下，那个ai是可以帮助你的，但是在什么情况下还是最好不要用ai会比较好，或者是现在就很像我们的那个公司，他也他们也出了一些比较比较严谨的一些policy，关于我们使用ai啊然后如果我们要发p

190
00:31:13.140 --> 00:31:28.870
Chin Ee Moon: paper要交paper的话，我们需要通过那个层层的那个ai的，就是把我们的paper交上去，然后一定要过了那些关之后，我们才可以交到那个的手上，就是他们也有在做这样的工作就是避免我们有这个。

191
00:31:28.870 --> 00:31:31.160
Chin Ee Moon: misconduct的这个风险吧。

192
00:31:31.500 --> 00:31:42.100
yunjie.fan@ntu.edu.sg: 那它这一方面是misconda，另一方面，您觉得有多大程度上是ai会影响医学研究的这种批判性的思维。

193
00:31:47.160 --> 00:31:49.750
Chin Ee Moon: 就是哪一种程度上，它会影响。

194
00:31:49.750 --> 00:31:53.430
yunjie.fan@ntu.edu.sg: 对，或者说是您认为多大程度上它会影响。

195
00:31:54.930 --> 00:31:57.460
Chin Ee Moon: 多大程度上它会影响，

196
00:32:00.010 --> 00:32:04.180
Chin Ee Moon: 我感觉就是因为每一个研究它都有一个

197
00:32:04.180 --> 00:32:21.530
Chin Ee Moon: 就是它主要的那个。如果你把你主要的那个，上面大部分的绝大部分的内容都依靠在ai给你的建议和那些指导上，可能那它就会非常的严重的影响这一个

198
00:32:21.650 --> 00:32:35.150
Chin Ee Moon: 这个研究的那个角度，但是如果你只是把它拿它当成一个辅助辅助的工具帮助你的，只是那些旁的一些工作的话，其实我觉得是还好的，

199
00:32:35.320 --> 00:32:36.240
Chin Ee Moon: 对。

200
00:32:36.470 --> 00:32:39.520
yunjie.fan@ntu.edu.sg: 明白，好的，那从

201
00:32:39.660 --> 00:32:48.470
yunjie.fan@ntu.edu.sg: 这里来看我，基本上我的问题就差不多了，你有没有什么要补充的，或者说是对于我们的这个课题有相关的疑问。

202
00:32:51.900 --> 00:33:06.170
Chin Ee Moon: 还还好还好吧，就是你们现在是你们的这个研究，你们是想要就是做一个新的一个网站，是像chat这样的吗？还是你们就只是要

203
00:33:06.480 --> 00:33:12.230
Chin Ee Moon: 就是粉饰这个的对我们人的这个影响。

204
00:33:12.800 --> 00:33:35.960
yunjie.fan@ntu.edu.sg: 一方面是会审视对于我们人的影响。另外一方面呢，很大程度之上，我们也需要去去optimize它整个的这个流程在后续的话，我们的这个研究人员呢？会有这样的一个步骤，就是说把我们可能已经做出来的比较demo的这些这个实验出来的。不论是

205
00:33:35.960 --> 00:33:48.670
yunjie.fan@ntu.edu.sg: ui也好，还是说是针对于直接可以反馈出数据的这种模型也好，请到你们到这个线下的这个实验室里面去体验。如果说这一方面的你，你会觉得愿意来参加吗？对。

206
00:33:48.950 --> 00:33:51.770
Chin Ee Moon: 我觉得我觉得是是愿意的。

207
00:33:52.010 --> 00:33:52.960
yunjie.fan@ntu.edu.sg: 好的，好的。

208
00:33:52.960 --> 00:33:53.390
Chin Ee Moon: 就是。

209
00:33:53.390 --> 00:34:05.350
yunjie.fan@ntu.edu.sg: 那我大概差不多啦，差不多了解了，然后在此的话还是有一个表格，因为你是没有填的，我们我需要您在场，然后开着摄像头把它填完。您是local吗？

210
00:34:05.560 --> 00:34:09.120
Chin Ee Moon: 哪哪哪一哪一个我没填，可以。

211
00:34:09.120 --> 00:34:12.400
yunjie.fan@ntu.edu.sg: 有一个我们的concent flow。

212
00:34:13.130 --> 00:34:14.260
Chin Ee Moon: 哎，我填了。

213
00:34:14.429 --> 00:34:15.419
yunjie.fan@ntu.edu.sg: 有甜吗？

214
00:34:15.690 --> 00:34:19.120
Chin Ee Moon: 我已经发回去了那个consent form。

215
00:34:20.230 --> 00:34:22.739
Chin Ee Moon: 现在挺早，对。

216
00:34:23.030 --> 00:34:43.320
yunjie.fan@ntu.edu.sg: Okok，好的，那我知道了，那就谢谢您今天的时间，然后后边的这个相关的feedback，我们会去应用到实验当中，所以提前跟您打招呼。然后如果说后续需要任何的协助，或者说是实验上的协助或者进一步采访的协助，我都会通过邮件去通知您的好吗？

217
00:34:43.320 --> 00:34:44.710
Chin Ee Moon: 好好好谢谢。

218
00:34:44.719 --> 00:34:47.499
yunjie.fan@ntu.edu.sg: 谢谢您时间再一次感谢拜拜。

219
00:34:47.500 --> 00:34:48.449
Chin Ee Moon: 掰掰。


受访人28:
WEBVTT

1
00:02:33.960 --> 00:02:35.440
Jeremy Hong: I didn't see it.

2
00:02:35.440 --> 00:02:37.270
yunjie.fan@ntu.edu.sg: Hello, hello, Jeremy.

3
00:02:37.270 --> 00:02:38.629
Jeremy Hong: Hey, nice to meet you.

4
00:02:38.820 --> 00:02:44.019
yunjie.fan@ntu.edu.sg: Nice to meet you. Is that okay for you to not turn on the camera, actually?

5
00:02:44.020 --> 00:02:45.520
Jeremy Hong: Not turned on. Okay, no worries.

6
00:02:45.910 --> 00:02:52.810
yunjie.fan@ntu.edu.sg: Due to… I'm going to record the whole session, so I just let you know.

7
00:02:53.640 --> 00:02:54.399
Jeremy Hong: Yeah, no problems.

8
00:02:54.400 --> 00:03:11.800
yunjie.fan@ntu.edu.sg: So, first of the study is to aim to understand the AI corporations, and thank you so much for joining the interview. And, I noticed that you are one of the callers here from NTU, right?

9
00:03:11.800 --> 00:03:12.530
Jeremy Hong: Yes.

10
00:03:13.190 --> 00:03:18.300
yunjie.fan@ntu.edu.sg: So, could you please briefly, like, demonstrate what you do?

11
00:03:18.810 --> 00:03:21.550
Jeremy Hong: Oh, I'm actually currently part of the,

12
00:03:21.690 --> 00:03:27.110
Jeremy Hong: I'm part of the marketing outreach and admissions team.

13
00:03:27.390 --> 00:03:32.950
Jeremy Hong: aimed at an undergraduate studies office at Nanyang Business School.

14
00:03:33.980 --> 00:03:37.170
yunjie.fan@ntu.edu.sg: Okay, okay. And

15
00:03:37.940 --> 00:03:42.920
yunjie.fan@ntu.edu.sg: Is that okay for you to interview in English, or you are more comfortable in Chinese?

16
00:03:42.920 --> 00:03:44.900
Jeremy Hong: I'm more comfortable in English.

17
00:03:44.900 --> 00:03:47.490
yunjie.fan@ntu.edu.sg: Okay, okay, sure. So,

18
00:03:47.630 --> 00:03:55.989
yunjie.fan@ntu.edu.sg: As you mentioned that you have already briefly introduced that you are responsible in outreach and admission, right?

19
00:03:55.990 --> 00:03:57.279
Jeremy Hong: Yes, correct.

20
00:03:57.310 --> 00:04:01.610
yunjie.fan@ntu.edu.sg: So, what is the main responsibility, can I know?

21
00:04:01.800 --> 00:04:11.800
Jeremy Hong: I mean, on a day-to-day basis, I'm actually the program lead for the Master's in finance program at Nanyang Business School.

22
00:04:12.610 --> 00:04:23.629
Jeremy Hong: So, I mean, anything… at the end of the day, I'm in charge of the outreach and the admissions process. So, the stuff like interviewing students or providing one-on-one consultations.

23
00:04:24.250 --> 00:04:26.669
Jeremy Hong: Yeah, that's what I'm in charge of.

24
00:04:27.370 --> 00:04:33.050
yunjie.fan@ntu.edu.sg: So, how do you think you're gonna… using AI or these similar tools as your work?

25
00:04:33.350 --> 00:04:54.119
Jeremy Hong: Okay, so I think for in terms of AI, one of the things that you, I mean, I would utilize in terms of, like, kind of, like, sorting data, for example, like, sometimes when students enter in their data, they come in many different forms. So, I mean, on a day-to-day basis, AI would help me in terms of, like, consulting data. For example, let's say they use, commerce in their names.

26
00:04:54.180 --> 00:04:57.540
Jeremy Hong: That's an excellent example. When they submit applications.

27
00:04:57.980 --> 00:05:13.000
Jeremy Hong: So, when I… for example, when I'll use AI to just automatically help me to remove all the commas from names, and make sure they're in a specific format for me when I have to enter in their details on a… onto a spreadsheet, for example.

28
00:05:13.440 --> 00:05:18.249
yunjie.fan@ntu.edu.sg: Okay, I can get that. So, which model are you most frequently to use?

29
00:05:18.390 --> 00:05:21.519
Jeremy Hong: I think… I think it's chat… chat GPT in general?

30
00:05:21.960 --> 00:05:24.370
yunjie.fan@ntu.edu.sg: In general, but you didn't pay for it, right?

31
00:05:24.370 --> 00:05:26.770
Jeremy Hong: I know, I use the free version.

32
00:05:27.280 --> 00:05:35.740
yunjie.fan@ntu.edu.sg: Okay, so could you please a little bit, like, more demonstrate about the process you're doing, the data analysis, this kind of thing?

33
00:05:35.740 --> 00:05:47.169
Jeremy Hong: Okay, so, I mean, okay, on terms of data analysis, I mean, a lot of… a lot of the times, I can't use… particularly use AI because it… I mean, it is confidential data, so I can't… I can't necessarily use AI.

34
00:05:47.170 --> 00:06:02.179
Jeremy Hong: But what I would say that AI, in terms of AI use that I would use more frequently is, for example, like, coming up with ideas, right? In terms of, like, marketing materials, and you, like, they're just trying to get used AI as a starting tool, like, oh, I want to write this in a certain way, give it…

35
00:06:02.180 --> 00:06:06.450
Jeremy Hong: Right, so I will say, okay, I want to use a specific voice for an email.

36
00:06:06.600 --> 00:06:19.179
Jeremy Hong: or I'll ask AI, okay, this is what I wrote, I wouldn't give it a prompt. Rather, I will write something already, and I'll ask AI, okay, can you make this sound a bit more, for example, professional? Make it sound more…

37
00:06:19.490 --> 00:06:38.939
Jeremy Hong: more casual, or more… or more… I mean, professional… professional generally is the term that I would use for most of the… my prompts and when I… when I do, kind of, use AI, I will write it out, and I'll ask them to help me make it some more professional, and I'll compare differences between the two.

38
00:06:39.030 --> 00:06:48.109
Jeremy Hong: reading what I wrote versus what AI generated for me, and pick the right answer, right solution in terms of what I'm intending to go for.

39
00:06:49.950 --> 00:06:55.370
yunjie.fan@ntu.edu.sg: So, were you going to use different kind of AI models to compare the results that they present to you?

40
00:06:55.370 --> 00:07:00.159
Jeremy Hong: Not in particular, I don't. I normally just use one, yeah.

41
00:07:00.380 --> 00:07:12.589
Jeremy Hong: Because the thing… the thing about it is that it's not… I'm using it more as a tool to kind of, like, just check my work, as opposed to using it as a more creative

42
00:07:12.590 --> 00:07:25.270
Jeremy Hong: or as an answer key, per se. It's just, okay, give me alternatives to what I have just done, and I will try to seek. But I don't really seek other models, per se. Sometimes I use AI for artwork as well.

43
00:07:25.460 --> 00:07:36.119
Jeremy Hong: where I use… for obvious purposes, you need to create artwork. That one, I will just rely on what's already given to me in Photoshop as… yeah, because I think that's provided by the school.

44
00:07:36.970 --> 00:07:40.029
yunjie.fan@ntu.edu.sg: Oh, so which model you are using for the artwork?

45
00:07:40.030 --> 00:07:45.469
Jeremy Hong: The artwork is, however, is provided by Adobe itself. I'm not exactly sure what model they use.

46
00:07:45.960 --> 00:07:47.160
yunjie.fan@ntu.edu.sg: Okay, sure.

47
00:07:47.160 --> 00:07:52.030
Jeremy Hong: generative fill and whatnot, like, they were just generated for me. I'm really not sure the model.

48
00:07:52.670 --> 00:08:03.540
yunjie.fan@ntu.edu.sg: Okay, I think I have two, like, aspects that you are using. One is for… to, like, give you more inspiration of your work, and the other is in the art pieces, art-relevant works, right?

49
00:08:03.540 --> 00:08:04.560
Jeremy Hong: Yes, correct.

50
00:08:04.710 --> 00:08:12.170
yunjie.fan@ntu.edu.sg: Okay, so, have you ever been trained in the, in the, like, prong engineering, this kind of thing?

51
00:08:12.170 --> 00:08:14.590
Jeremy Hong: No, I'm actually a comms major, yeah.

52
00:08:15.020 --> 00:08:23.080
yunjie.fan@ntu.edu.sg: Oh, okay. So, how can you, like, verify the… the, content that the AI generates to you is accurate?

53
00:08:23.080 --> 00:08:27.339
Jeremy Hong: Okay, so that's the thing, is that I don't really use it to generate information.

54
00:08:27.590 --> 00:08:45.580
Jeremy Hong: So, I don't… I'm not… I'm not… I'm not using it like a search engine where I, like, try to search for information. Rather, I use it as a kind of, like, spell check kind of… kind of situation, or in terms of artwork, I'm using it to… for example, I'm giving it upon to remove a certain item, which I'm actually quite proficient in, like, Photoshop, and that's where I generally use it.

55
00:08:45.720 --> 00:08:53.080
Jeremy Hong: So I would… in a way, when I see the artwork, I would know that, hey, it's what I'm looking for.

56
00:08:53.450 --> 00:08:56.179
Jeremy Hong: I don't really use it for information, I'm not using it to…

57
00:08:56.420 --> 00:08:59.220
Jeremy Hong: Captain up with something that is factual information.

58
00:09:00.110 --> 00:09:14.850
yunjie.fan@ntu.edu.sg: Okay, okay, so, because you have already mentioned about you wanted to, like, broaden your brain to have more, more information about your inspiration, so this kind of thing, I feel like you need to at least know the

59
00:09:14.850 --> 00:09:20.739
yunjie.fan@ntu.edu.sg: Correct address or the, or the, the, the accurate about the information that it, it gives, right?

60
00:09:20.740 --> 00:09:28.609
Jeremy Hong: I… okay, so here's the thing, is that generally, I'm not… I don't really look for it for information, I use it more for tonality purposes.

61
00:09:29.730 --> 00:09:48.199
Jeremy Hong: So, for example, I'm writing an email, I want it to… okay, as I said just now, as I write an email, I want it to sound professional, right? I'm not filling in information to find me all the stats that'll make it sound professional. I already have, like, say, the stats on hand already. I just want it to format it in a way that makes it look professional.

62
00:09:48.200 --> 00:09:48.790
yunjie.fan@ntu.edu.sg: Okay, okay.

63
00:09:48.790 --> 00:10:00.919
Jeremy Hong: professional. So, there's no factual information being shot at me that I'm not already providing. I'm not asking it for me to find information related to a topic. Rather, I am asking it to change what I'm saying, or

64
00:10:01.290 --> 00:10:04.959
Jeremy Hong: It changed… it's actually changed what I am saying into a different form.

65
00:10:05.700 --> 00:10:12.260
yunjie.fan@ntu.edu.sg: Okay, so it's just a… Tool and, like, assistant to help you to, like.

66
00:10:12.420 --> 00:10:16.530
yunjie.fan@ntu.edu.sg: Lecture content more for, like, format in a rather way, right?

67
00:10:16.530 --> 00:10:40.249
Jeremy Hong: Yeah, formatted in another way. And I mean, in the same way, it's like, you're kind of, like, also using it as a spell checker. Is it a spell checker in, like, Microsoft Word, for example. Like, yeah, it's not giving you new information, but it's also using some AI tool on its back end, some algorithmic tool for you to understand, like, oh, this is spelled wrongly. But I don't necessarily think that it'll give me… I'm not asking you to, like, search for information or derive information.

68
00:10:40.250 --> 00:10:41.300
Jeremy Hong: From some way.

69
00:10:41.490 --> 00:10:44.599
yunjie.fan@ntu.edu.sg: Okay, so you failed it in English, or…

70
00:10:45.290 --> 00:10:45.870
Jeremy Hong: Huh?

71
00:10:46.270 --> 00:10:50.590
yunjie.fan@ntu.edu.sg: Do you just provide the information in English, or in Chinese, or any?

72
00:10:50.590 --> 00:10:54.579
Jeremy Hong: It's all in English, yeah, that's my primary mode of conversation.

73
00:10:54.940 --> 00:11:04.489
yunjie.fan@ntu.edu.sg: Okay, so have you ever used AI in, like, the, planning or the marketing strategy, this kind of thing?

74
00:11:04.750 --> 00:11:09.529
Jeremy Hong: Not, not in particular, not really for strategy purposes, yeah.

75
00:11:10.740 --> 00:11:26.940
yunjie.fan@ntu.edu.sg: So, have you… because you have already mentioned in the creative way, so, could you please, like, let me know about, like, what information you're gonna give it, and what kind of the words that all you are ready to mention about what you want to present from here? Okay.

76
00:11:26.940 --> 00:11:46.420
Jeremy Hong: Okay, so, let's say in… okay, I'm using this particularly into… in a Photoshop sense. So, in Photoshop sense, so let's say you want something to be removed, you just give it a prompt to say, hey, remove this item from the image. Then I'll make a selection of what I want to be removed, and I'll just say, remove this from the image.

77
00:11:47.070 --> 00:11:49.269
Jeremy Hong: So, that's the prompt I give it.

78
00:11:49.780 --> 00:12:01.989
Jeremy Hong: it's not… I don't… I don't really generate the image from… I'm not asking it to generate an image, but in particular, rather than asking it to remove it from, let's say, a background or whatnot.

79
00:12:02.810 --> 00:12:07.719
yunjie.fan@ntu.edu.sg: Okay, so you have… you don't have to worry that the AI maybe caused, like, the…

80
00:12:07.860 --> 00:12:13.080
yunjie.fan@ntu.edu.sg: homogenization across the campaigns or your materials, right?

81
00:12:13.080 --> 00:12:32.660
Jeremy Hong: Not really in particular, if anything, homolisation, because we already… a lot of times, we already have, like, existing artworks within our team that we are using already, but we need to make certain tweaks to it. So, if there's hominization, that's actually good, because we want it to kind of look similar between our programs, or what… whatever we have already.

82
00:12:32.900 --> 00:12:37.870
yunjie.fan@ntu.edu.sg: Okay, so do you think it's gonna to instead the creator's job?

83
00:12:38.460 --> 00:12:39.729
Jeremy Hong: Creator, creator what?

84
00:12:40.280 --> 00:12:43.550
yunjie.fan@ntu.edu.sg: Creator's job, like, the role in the team.

85
00:12:43.550 --> 00:13:02.980
Jeremy Hong: for AI, I mean, for our artists already, I mean, our artists themselves already use AI in certain… in different ways already, so I don't… I think AI is more of a tool at this point, but naturally, I think when you create, let's say, generated artwork, so say, for non-essential items, such as stuff like backgrounds, which don't have to be factual.

86
00:13:02.980 --> 00:13:06.120
Jeremy Hong: For sure, I think AI is going to help a lot of people to

87
00:13:06.600 --> 00:13:22.999
Jeremy Hong: to probably do their own artworks, per se, but jobs-wise, I'm not too certain. Rather, how I would see it is that people who are already doing this work would already… would have an added tool for them, because I think all these works still require an eye for, like, I say artwork, for example.

88
00:13:23.000 --> 00:13:34.530
Jeremy Hong: they require… it requires somebody who is already experienced and knows what to look out for. If you're not an artist, a lot of times, you don't necessarily see the issues with it. So I don't particularly think this is a new job thing.

89
00:13:35.900 --> 00:13:46.639
yunjie.fan@ntu.edu.sg: Okay, so do you think AI plays a great role, or it takes a large, like, percentage of your job in the… your admission or outreach.

90
00:13:47.150 --> 00:13:54.369
Jeremy Hong: Not in particular, because I think a lot… a big part of my job is still talking to students.

91
00:13:54.620 --> 00:14:10.549
Jeremy Hong: Right? In terms of outreach, I do still talk to students. I have to make sure that they understand what I'm saying, and a lot of the times, they are asking me questions that are not exactly found on the… found on the web, or found anywhere else. These are more…

92
00:14:10.660 --> 00:14:18.179
Jeremy Hong: Soft… like, in a way, you are kind of, like, talking to the students and trying to assuade them in one way or the other.

93
00:14:18.650 --> 00:14:26.479
Jeremy Hong: Right? And this information sometimes is not stuff that you necessarily have noted down in one place or the other, or can be found on a web… on our website.

94
00:14:27.570 --> 00:14:30.610
yunjie.fan@ntu.edu.sg: So, have you ever maybe tried to just…

95
00:14:30.940 --> 00:14:39.250
yunjie.fan@ntu.edu.sg: give the AI about the information about your student to help… to let you to get a better way to communicate with him.

96
00:14:39.770 --> 00:14:57.900
Jeremy Hong: At the moment, we're not currently using AI bots, and I think one of the reasons for that is, I think, in the admissions process, we are also very concerned about the use of AI bots, let's say, for, like, admission essays and whatnot. Currently, we do not employ anything to, for instance, mark out, I mean, these

97
00:14:58.030 --> 00:15:03.320
Jeremy Hong: These boards, in terms of board usage for our potential students.

98
00:15:03.350 --> 00:15:21.979
Jeremy Hong: But I don't think there's any plans to actually necessarily use, like, a model so people can chat with them. Instead, I think what we do, in particular for the specialized masters at Nanya Business School, is that we actually have real-life students actually chat, and our admissions team chatting with students through a system called Unibuddy.

99
00:15:23.250 --> 00:15:30.299
yunjie.fan@ntu.edu.sg: Okay, so is that the reason that you feel like it appears, like, strong, but actually mislead?

100
00:15:30.780 --> 00:15:39.209
Jeremy Hong: Is it misleading, or what? Because I think all these people… everybody on our platform that we use to communicate with potential students are all real people.

101
00:15:39.480 --> 00:15:58.099
Jeremy Hong: I think this… I mean, right now, our… this seems to engender a lot more trust in that, in the process, having a person behind all these conversations, and meeting people in real life, because I think a large part of our job currently is also, like, going to fairs, for example, going to overseas fairs, and talking to people, and making sure that they understand

102
00:15:58.100 --> 00:16:10.730
Jeremy Hong: And they will still come down to and talk to us at these fairs, let's say when we go to India, or we go to China, or we go to Thailand, and Japan, Korea. We still have a lot of students coming in and asking questions

103
00:16:10.730 --> 00:16:20.500
Jeremy Hong: even though, I mean, a lot of the information is… can be… of the program can be found online, but they want to hear a lot. I think there's a level of trust that you get when you're actually seeing somebody in person.

104
00:16:21.330 --> 00:16:26.969
yunjie.fan@ntu.edu.sg: Yeah, sure. So, beyond your work, do you frequently use AI in your, like, daily life?

105
00:16:27.680 --> 00:16:29.620
Jeremy Hong: Do I use AI in my daily life?

106
00:16:29.620 --> 00:16:30.450
yunjie.fan@ntu.edu.sg: Yeah?

107
00:16:30.910 --> 00:16:45.039
Jeremy Hong: Beyond work, I mean, in my daily life, I think AI is… I use it, in a way, like a search engine in some ways, because, like, for example, right now, I think the people are using hacks, like, okay, using… finding cheaper flights, right?

108
00:16:45.270 --> 00:16:55.380
Jeremy Hong: So I use it to find cheaper flights. I don't… and sometimes to have inspiration, let's say, to plan a trip. Sometimes your first goal call is to ask AI, okay, help me plan a trip.

109
00:16:55.640 --> 00:17:01.839
Jeremy Hong: let's say I'm going to Thailand or Bangkok, for example, I'll say, okay, find me places to go in Bangkok.

110
00:17:01.870 --> 00:17:03.060
yunjie.fan@ntu.edu.sg: And I think…

111
00:17:03.060 --> 00:17:19.979
Jeremy Hong: they… sometimes I can actually feed it my past itineraries, so that it gives me a… they have a rough idea of… AI itself has a rough idea of, oh, what… what might interest me and whatnot. But I think it's only merely a tool, as I said. I think a lot of… in a lot of times, I do use a…

112
00:17:20.650 --> 00:17:21.920
Jeremy Hong: I do have…

113
00:17:22.030 --> 00:17:27.660
Jeremy Hong: I do, kind of, like, after I see what they give me, I still go, I don't trust it immediately, I go and Google.

114
00:17:27.660 --> 00:17:43.899
Jeremy Hong: Google it, and make sure that, okay, this is something that I'm actually genuinely… I might be interested in, and then before deciding on, like, pulling the trigger for it, I don't just, oh, trust it blindly, like, oh, because they, like, AI recommended the chat… ChatGPT recommended it to me, I go for it immediately.

115
00:17:44.560 --> 00:17:51.590
yunjie.fan@ntu.edu.sg: Oh, so what is the reason you don't trust it? Like, how do you usually, like, assess the credibility of AI?

116
00:17:51.740 --> 00:18:09.399
Jeremy Hong: I don't necessarily distrust the credibility of AI, but rather, because, I mean, I don't… I don't see it… okay, when I use AI, I don't necessarily… I… I'm somebody who likes to, clear his browsing history on quite a regular basis.

117
00:18:10.290 --> 00:18:16.599
Jeremy Hong: And because of that, and I don't use ChatGPT with an account all the time.

118
00:18:16.910 --> 00:18:23.500
Jeremy Hong: So, in a way, it doesn't really have a history of who I am, and what my preferences are.

119
00:18:24.830 --> 00:18:42.169
Jeremy Hong: And as a result, its recommendations are the… it's a generally crowd-sourced information, I would say. It's taking the best for everybody, and I'm not necessarily somebody who likes to go with the flow, or somebody who is… I won't say comfortable, but I'm not necessarily somebody who

120
00:18:42.250 --> 00:18:49.980
Jeremy Hong: is in line with everyone else. Like, my interest varies quite differently from most people. I would say when I travel.

121
00:18:50.120 --> 00:18:58.560
Jeremy Hong: in terms of, like, oh, how… where to go, or even how much time to spend outside. Like, I generally, when I go overseas, I do like to have a bit more time to…

122
00:18:59.560 --> 00:19:15.610
Jeremy Hong: to kind of relax and unwind, I enjoy the trip itself. Like, I'm not somebody who likes to go on a trip and spend, like, a thousand hours outside. Like, every working moment needs to be optimized. I do like to spend some time inside the hotel as well.

123
00:19:15.610 --> 00:19:20.809
Jeremy Hong: So, in a way, it's like, kind of like seeing pictures and seeing a general feel and vibe.

124
00:19:20.810 --> 00:19:25.779
Jeremy Hong: from the place, so I would like to… that's why I do Google… I still do my own research, lah.

125
00:19:26.320 --> 00:19:43.290
yunjie.fan@ntu.edu.sg: Okay, okay, okay. So, back to the working scenario, because you… anyway, you need to rely on AI in the… in the tool to, like, help you to optimize the format. So, do you ever try to, in the multi-round reactions or interactions?

126
00:19:43.310 --> 00:19:50.479
yunjie.fan@ntu.edu.sg: To… to give… to, like, let him to give you a, like, plan B, E, plan A, like, result A, result B, this kind of thing.

127
00:19:50.480 --> 00:20:05.609
Jeremy Hong: Not… not… not really, I've never done that, yeah. I don't… I don't particularly have a reason why I have not particularly tried this side out, I've just never done it, yeah, because I think in a lot of ways, what we do at the admission, the outreach side has been

128
00:20:05.870 --> 00:20:19.189
Jeremy Hong: has been something that we are trying as a whole, and as a result, we don't really try to go against the grain, because that would mean everybody in other programs will do the same. We try to be quite parallel in terms of the way we do things.

129
00:20:20.200 --> 00:20:25.580
yunjie.fan@ntu.edu.sg: So, what is the main purpose you feel like the AI helps you the most?

130
00:20:25.980 --> 00:20:42.440
Jeremy Hong: I think AI… how I would see AI right now is AI is some… in the past, when you would say… ask somebody, hey, help me check… check… check something, like, check this word, but there's, like, grammatical errors, or, like, spelling errors, or just general formatting issues.

131
00:20:42.440 --> 00:21:02.159
Jeremy Hong: Because I see it as that person, right? So, I don't… I have a bit more faith in terms of, like, okay, the AI tool will help me to spot mistakes, or help me to change something, but I don't necessarily use it to… I don't think I use it to come up with something, yeah.

132
00:21:02.560 --> 00:21:04.310
Jeremy Hong: It's more of a checker for me.

133
00:21:04.510 --> 00:21:10.119
yunjie.fan@ntu.edu.sg: Okay, so if it just checks some, incorrect issues in your work.

134
00:21:10.490 --> 00:21:24.099
Jeremy Hong: Yes, or I'll mix it better, but I don't see it as somebody as a… I know… what I know is some people do like to, like, collaborate with AI, like, okay, here's some ideas, or, like, give me some ideas for something. I… that's not necessarily something that I do.

135
00:21:24.640 --> 00:21:31.650
yunjie.fan@ntu.edu.sg: Okay, so will you just optimize that the incorrect yourself, and won't rely on AI anymore, right?

136
00:21:31.650 --> 00:21:35.329
Jeremy Hong: I don't, in particular, rely on AI that much, yeah.

137
00:21:35.460 --> 00:21:43.120
yunjie.fan@ntu.edu.sg: Okay, okay, so what is the, like, situation you decide to stop, like, interacting with AI?

138
00:21:44.600 --> 00:21:48.039
Jeremy Hong: I said, when did I start, or a situation in which I will stop?

139
00:21:48.040 --> 00:21:55.920
yunjie.fan@ntu.edu.sg: like, we start a conversation with AI, and, like, which kind of situation, oh, stop, stop to interaction, I gotta do it my own.

140
00:21:56.470 --> 00:22:07.319
Jeremy Hong: I… I… I don't think I ever stopped using AI. I ever started using AI in that way, so I would not say I've ever… I ever started, nor will I ever stop, yeah.

141
00:22:07.660 --> 00:22:26.969
yunjie.fan@ntu.edu.sg: No, no, I mean, just for a one specific scenario, like, you start a conversation with AI, and you just feed him any information that you want him to help you to reach, but in which situation, in this conversation, you decide to, oh, stop here, and I'm gonna do the rest of my job of my own.

142
00:22:27.400 --> 00:22:28.650
Jeremy Hong: Okay,

143
00:22:28.660 --> 00:22:35.360
Jeremy Hong: I… usually, because the thing is, AI normally comes in at the more final stage of it, so…

144
00:22:35.360 --> 00:22:50.310
Jeremy Hong: So, when AI gives me… gives me some suggestions, for example, okay, I'm going to give the example of a tone thing, where I'm writing an e… I'm writing an email to a specific audience. This, okay, it sounds like, okay, I'll give a specific example of when I was writing to alumni, right, about an announcement.

145
00:22:50.700 --> 00:22:56.139
Jeremy Hong: And I just want to make sure that, hey, the tone that I use isn't too casual.

146
00:22:56.420 --> 00:23:12.880
Jeremy Hong: Per se. So I will… I will use… I… what I will do is I'll use AI to… and I'll fill it this… the email that I already crafted, and say, okay, make this sound more professional, and I'll see the result. But from there, what I would… what I would do is I would compare the result between what AI got me.

147
00:23:12.900 --> 00:23:36.100
Jeremy Hong: and what I wrote already, and then, in a way, in a way, paragraph by paragraph, sentence by sentence, decide, okay, what's the best way to incorporate all of these things? So, I think once AI gives me a result, I will look at it and I'll evaluate it, and if I need further prompts, I would, but after that, then I would take it back and actually form my own opinion, and form my own email.

148
00:23:36.120 --> 00:23:49.020
Jeremy Hong: based on something that they have suggested. So, in the same way, like, to compare it to when I have, like, a face… I just said, and somebody else give me a face-to-face, they might suggest something for me to change, but I might think, oh, okay.

149
00:23:49.040 --> 00:24:06.080
Jeremy Hong: this suggestion is not as good as what I had originally done, so I would just neglect that information. But sometimes, when you… as in real life, when someone gives you a good idea, you say, oh, that's a good idea, then you will take that piece of advice, and then you will just incorporate it into your work.

150
00:24:07.360 --> 00:24:12.999
yunjie.fan@ntu.edu.sg: Do you feel like AI makes you more, like, independent, or, like, more dependent?

151
00:24:13.360 --> 00:24:27.409
Jeremy Hong: I don't think independent or… I don't think I'm really dependent on AI per se, because I think if you talk… if you ask me about, like, spell checkers and whatnot, yes, for sure, I think I'm dependent on those things, because they do help me

152
00:24:27.410 --> 00:24:35.949
Jeremy Hong: like, sports spelling mistakes, especially if you're typing too fast, you see the line, it's like, oh, you made a spelling error. Would I have caught it? I highly doubt so.

153
00:24:35.950 --> 00:24:41.720
Jeremy Hong: Right? But you're talking to me about, like, gen… something that is generative, something that is, like, derived from a large language model.

154
00:24:41.720 --> 00:24:54.540
Jeremy Hong: what I would say is then I don't think I would do it, because in a way, I would have, like, other people to check with, right? And I think so much of my job is subjective, is there's no right or wrong answer.

155
00:24:54.540 --> 00:25:10.080
Jeremy Hong: Like, there's no right or wrong way to talk to a student, a prospective student, and tell them more about your program. And there are better ways, for sure, but there's no way for you to ever ascertain after it's done. There's no grading system that I would say there is. So I'm not…

156
00:25:10.080 --> 00:25:17.239
Jeremy Hong: in a way, beholden to it. Because I would never know if, let's say, example, if I'm talking to a prospective student, whether I could have done a better job.

157
00:25:17.450 --> 00:25:18.310
Jeremy Hong: Correct.

158
00:25:18.650 --> 00:25:23.270
yunjie.fan@ntu.edu.sg: Yeah, so, like, which degree do you think AI helps you to efficient your job?

159
00:25:23.440 --> 00:25:37.469
Jeremy Hong: I think AI allows… allows me to have some… have somebody there to help me bounce… to make… not bounce ideas off, but someone to help me just decide to help me track my work, someone as a tracker. I don't… I don't see it as a…

160
00:25:37.720 --> 00:25:52.440
Jeremy Hong: a tool beyond that. Like, in a way, like, I see, like, spell check, right? It's this one more… one more final check of what I have done. It's not a… that's necessarily a… a, oh, I use it as the basis of my work.

161
00:25:53.580 --> 00:25:55.930
yunjie.fan@ntu.edu.sg: So, when you started to be using AI.

162
00:25:55.930 --> 00:25:56.620
Jeremy Hong: Yes.

163
00:25:57.170 --> 00:25:58.229
yunjie.fan@ntu.edu.sg: Like, when you started.

164
00:25:58.230 --> 00:26:12.900
Jeremy Hong: When did I start? So all these generative… because, like, in terms of how… what you define as AI, right? Because I think if you look way back, like, in terms of, like, Photoshop, it had this thing called generative fail.

165
00:26:13.520 --> 00:26:24.109
Jeremy Hong: Well, where it would fill in, like, it would help you, like, have content-aware feel, where it's like, you have content that is within the context, and it will help you generate something.

166
00:26:24.750 --> 00:26:30.819
yunjie.fan@ntu.edu.sg: Based on the content… it's this feature called Content Awarefail, and I think it was released in the early 2010s.

167
00:26:31.460 --> 00:26:35.029
yunjie.fan@ntu.edu.sg: Okay, so… so, like, how many years you have been using,

168
00:26:35.030 --> 00:26:40.409
Jeremy Hong: No, so what kind… so I'm asking you now is, like, what do you define as AI?

169
00:26:42.110 --> 00:26:46.080
yunjie.fan@ntu.edu.sg: Ai is the… the… the… the…

170
00:26:46.800 --> 00:27:03.510
Jeremy Hong: like, the mechanism that we are talking about, so… Yes, because the thing is, okay, so, like, how people would define AI might be, like, large language models, for example, like a chat GPT and whatnot. But in terms of AI, let's say I use AI for, say, let's say when you're on Gmail, for example.

171
00:27:03.510 --> 00:27:10.239
Jeremy Hong: and your email is sorted based on spam filters. Spam filters in itself is a form of AI, correct?

172
00:27:10.810 --> 00:27:13.139
yunjie.fan@ntu.edu.sg: We are just talking about big models, so…

173
00:27:13.140 --> 00:27:20.030
Jeremy Hong: Okay, so this large language model. So, in this case, I would say I started using it probably, like, 2022, 2023.

174
00:27:21.160 --> 00:27:21.860
Jeremy Hong: Yeah, but I mean…

175
00:27:21.860 --> 00:27:22.440
yunjie.fan@ntu.edu.sg: I'm glued.

176
00:27:22.440 --> 00:27:30.719
Jeremy Hong: during the boom of when ChatGPT became… first became a thing, so that's when I… I went… I went to, like, try it… try it out, per se.

177
00:27:31.520 --> 00:27:39.359
yunjie.fan@ntu.edu.sg: So, I started from there, you're just using it as a tool, but you never like to deeply communicate with it, right?

178
00:27:39.360 --> 00:27:41.679
Jeremy Hong: No, I generally don't, yes.

179
00:27:42.310 --> 00:27:43.719
yunjie.fan@ntu.edu.sg: Oh, okay.

180
00:27:44.340 --> 00:27:57.600
yunjie.fan@ntu.edu.sg: So do you think maybe NF, like, features or the, for the product way, you feel like future AI, maybe, can… would you like to see the update?

181
00:27:58.420 --> 00:28:11.410
Jeremy Hong: Well, I think AI in general, or is… okay, how large language models are working in general, or these models are working in general, is in terms of the form of communication, so…

182
00:28:11.540 --> 00:28:19.249
Jeremy Hong: when I see, for example, Google, Google releasing software using Gemini to have a more conversational

183
00:28:19.450 --> 00:28:38.110
Jeremy Hong: oh, conversational awareness in a way of surroundings and whatnot. I mean, that's something that I'm particularly intrigued by, but… and at the same time, I'm… in terms of the… in terms of the interaction model of how we interact with AI, I think that to me is the more fascinate… more fascinating thing.

184
00:28:38.110 --> 00:28:49.370
Jeremy Hong: Because, I mean, for… at least for me, I know… I know now there are, like, different ways for people to use AI, but for me, the primary model in which I use AI is still, like, a chat… a chat prompt, right? A text prompt.

185
00:28:49.450 --> 00:29:01.629
Jeremy Hong: into AI. But if AI itself can kind of preemptively communicate with me as I'm doing my work, that, to me, is that next interaction paradigm.

186
00:29:02.240 --> 00:29:09.239
Jeremy Hong: I'm… to which I… I am, like, it's preemptively communicating with me. That's how I… what I would want to see.

187
00:29:09.910 --> 00:29:21.109
yunjie.fan@ntu.edu.sg: Yeah, still to be… we are seeking for the audience wishes cannot be critical thinking in this process, I feel like you are fully… can be, like, critical thinking. So do you feel like…

188
00:29:21.320 --> 00:29:29.499
yunjie.fan@ntu.edu.sg: Any recommendations to… to let people who are using AI have a better, like, stronger skills in creative thinking?

189
00:29:29.730 --> 00:29:44.769
Jeremy Hong: In critical thinking… so, okay, so the thing about critical thinking and AI is that people don't really care where… I'm just… this has been a long-term issue when we talk about, like, say, search engines in general, when we talk about Google and where they index their information from.

190
00:29:44.770 --> 00:29:58.119
Jeremy Hong: Right? And I think… I think right now, for a lot of… a lot of models that we… we talk about, like, AI in general, we don't know what models they have been trained on, we don't know what content they have been trained on, or what are the sources of content that they have been trained on.

191
00:29:58.120 --> 00:30:09.210
Jeremy Hong: And I think… I think for a lot of people, I think having an awareness of AI hallucinations, for example, are… is something that we definitely need to

192
00:30:09.540 --> 00:30:21.379
Jeremy Hong: to push a lot further. I think right now, this is true for almost anything, like, for example, we talked about the radio in the 1930s, where people trusted everything they heard on the radio or television.

193
00:30:21.920 --> 00:30:25.870
Jeremy Hong: And I think it's not particularly a kind of…

194
00:30:25.970 --> 00:30:48.830
Jeremy Hong: a situation where you have to tell people, oh, AI is something that you need to worry about when it comes to, like, hallucinations or false information. However, it will take a widespread number of big incidences where AI causes issue for there to be this shift in perception towards, okay, this is merely a tool, as opposed to something that you rely on.

195
00:30:48.830 --> 00:30:49.620
Jeremy Hong: And I think…

196
00:30:49.700 --> 00:30:59.529
Jeremy Hong: inevitably, there will be a situation… situations will come up where people would… people would… would not actually take heed. I'm gonna give the example of…

197
00:30:59.810 --> 00:31:15.919
Jeremy Hong: of the rise of fake news, correct? And when you look at fake news and how people are just con… are literally falling for fake news and not checking their sources, this is going to continue on with AI, and the thing is, once this level of trust has been built, it's hard to decouple this trust.

198
00:31:17.100 --> 00:31:27.510
Jeremy Hong: And I think the only way for AI… the trust in AI to kind of be reduced is literally for another form of information gathering to come out.

199
00:31:27.820 --> 00:31:35.719
Jeremy Hong: So, like, people trusted the newspapers less, because social media became a thing, right? And then they started to trust social media more.

200
00:31:35.990 --> 00:31:51.269
Jeremy Hong: or… and then they started to distrust newspapers. I think right now, what we… what is… what we are seeing is that when to… in order for people to trust AI less, we need something that is more, in a way, accurate, right? Or something that they have more trust in.

201
00:31:51.570 --> 00:31:58.920
Jeremy Hong: to come along, and because they have more trust in this new platform, they start to see why they should distrust AI.

202
00:32:00.300 --> 00:32:06.810
yunjie.fan@ntu.edu.sg: But, you just mentioned about the… the, AI hallucinations, so how could you define this?

203
00:32:06.930 --> 00:32:24.819
yunjie.fan@ntu.edu.sg: Like, which do you think, oh, it's just triggering the people's untrust for AI? Like, which kind of the information for, like, oh, the people should realize it's not true, it's not… can be trusted? But I feel most of the people not using AI is because they don't have the ability to… to identify this kind of thing.

204
00:32:24.820 --> 00:32:25.460
Jeremy Hong: I…

205
00:32:25.710 --> 00:32:40.460
Jeremy Hong: So, that's the thing, right? Because I don't necessarily… because I don't use AI as a end-all, be-all source, and the thing is, I know, especially for the younger generation, they kind of… they use AI as a search engine.

206
00:32:41.020 --> 00:32:51.089
Jeremy Hong: And I think that's… I mean, that can be a difference in thinking, because, for example, in the past, when you googled something, you don't necessarily trust the first source or the second source, right?

207
00:32:52.100 --> 00:32:53.529
Jeremy Hong: And I think because there is

208
00:32:53.790 --> 00:33:05.509
Jeremy Hong: because AI in itself is, in a way, a singular source of truth for a lot of people. For a lot of people who use it as a singular source, it becomes very dangerous.

209
00:33:05.950 --> 00:33:09.829
Jeremy Hong: Yeah, so I… what… so what is your question… question for this?

210
00:33:10.420 --> 00:33:13.969
Jeremy Hong: How do you tell people to not trust a singular source?

211
00:33:14.750 --> 00:33:19.690
yunjie.fan@ntu.edu.sg: By which degree they need to think themselves, but not trust others that march.

212
00:33:19.690 --> 00:33:36.669
Jeremy Hong: I… I… what… the thing about AI is that I don't think you should trust AI at all, right? Because there is a… there is a level of hallucinations that cannot be quant… that is not necessarily quantifiable, even by its researchers. Especially on a topic-by-topic basis.

213
00:33:37.500 --> 00:33:45.949
Jeremy Hong: like, I think there should be… we should use AI with a certain level of distrust in general, and not use it as a single point of truth.

214
00:33:46.690 --> 00:33:47.580
Jeremy Hong: And…

215
00:33:47.580 --> 00:33:48.110
yunjie.fan@ntu.edu.sg: Yeah.

216
00:33:48.130 --> 00:33:56.270
Jeremy Hong: But I think it's, unfortunately, and this is true for almost anything that we can use for, as I said, newspapers, we talk about Google, we talk about…

217
00:33:56.680 --> 00:34:11.120
Jeremy Hong: search engines that people just use the top link and use that as, hey, this is what I want, how I want… how I see reality, because this… this source that I trust has this as its number one answer.

218
00:34:11.730 --> 00:34:20.960
Jeremy Hong: Like, because, I mean, as a comms major, right, we talk about, like, journalism, and talk about, like, having at least two credible sources from different places.

219
00:34:21.750 --> 00:34:34.179
Jeremy Hong: Right? So, in a way, anything that I see online, I take with me. Like, even you have… like, as a scientist or whatnot, you take… you don't necessarily say, this is correct. You say, I have a certain degree of confidence.

220
00:34:34.650 --> 00:34:44.880
Jeremy Hong: That something is correct, like, a percentage point, and if your number… and if you are… is accurate up to 95%, that's your benchmark of 99%, depending on

221
00:34:44.989 --> 00:34:51.290
Jeremy Hong: on your margin of error for your experiment or your findings. You don't… you never say that it's 100% true.

222
00:34:52.239 --> 00:35:09.959
yunjie.fan@ntu.edu.sg: Yeah, but actually, because you have this kind of skill, because you are experienced, but for students, or for the younger people, or for the people all in the radar just cannot have this ability to define which kind of information is right or wrong, right?

223
00:35:10.290 --> 00:35:20.879
Jeremy Hong: Yes, so that is… that is a worry, that is definitely a worry for me, but at the same point, that this… the same thing is also true for any other thing that is not AI.

224
00:35:22.090 --> 00:35:29.509
Jeremy Hong: So, it's like, when people, when you see, like, people say, don't take, don't take vaccines, because vaccines cause autism, for example.

225
00:35:29.670 --> 00:35:30.340
yunjie.fan@ntu.edu.sg: Mmm.

226
00:35:30.340 --> 00:35:48.970
Jeremy Hong: that wasn't something that was caused by AI. That was some… that was something that… that was true… that was caused by other factors, and people not having… being critical thinkers. Like, AI is just a… it's just the new wave of people not thinking critically, but people have not been thinking critically for a long time beyond AI.

227
00:35:49.410 --> 00:35:53.860
Jeremy Hong: So, is this… I don't think this is necessarily an AI problem.

228
00:35:54.250 --> 00:35:57.190
Jeremy Hong: But more of a critical thinking problem.

229
00:35:58.370 --> 00:36:02.260
yunjie.fan@ntu.edu.sg: But I think AI improves a lot, right? Like, in…

230
00:36:02.710 --> 00:36:19.710
Jeremy Hong: So, actually, the irony of it is that I think people are actually more aware of AI hallucinations at this point, because, in a way, most people who are using AI now grew up on the notion that AI… grew up in a time when AI was not perfect.

231
00:36:21.060 --> 00:36:22.000
Jeremy Hong: Correct.

232
00:36:22.000 --> 00:36:36.649
yunjie.fan@ntu.edu.sg: What if you just verify all the AIs give you the information is, like, 90 or even more accurate? Is that a… it's not a… it's not a problem, right? It's just a normal problem.

233
00:36:36.650 --> 00:36:46.279
Jeremy Hong: It's… no, the thing is, is that I think a lot of people… okay, I won't talk about the very people who are very young. I'm talking about people who are, okay, currently in university.

234
00:36:46.280 --> 00:36:58.699
Jeremy Hong: For example, and when ChatGPT came out in… when… and it grew in, like, popularity in the early 2020s, these… a lot of them grew up knowing that AI can be wrong.

235
00:36:59.900 --> 00:37:00.480
Jeremy Hong: Correct.

236
00:37:01.270 --> 00:37:18.499
Jeremy Hong: or AI has issues. So, in a way, there is… there is still skepticism behind AI. I think the issue would come in is when AI is, as you say, 90% accurate, 95% accurate, there's a certain… there is a… AI is often more correct than what we can find.

237
00:37:18.600 --> 00:37:27.160
Jeremy Hong: that all we can ever determine. That is… that, and people are so accustomed to AI always being right, that people who grew up in that generation

238
00:37:27.300 --> 00:37:33.750
Jeremy Hong: I think, in that sense, that is… those are the people who I would think is a bigger issue, because

239
00:37:33.940 --> 00:37:42.259
Jeremy Hong: To them, critical thinking would be relying on multiple AI models, as opposed to finding other sources of truth beyond AI.

240
00:37:43.090 --> 00:37:51.979
Jeremy Hong: As opposed to somebody now who, like, for example, me, who uses an AI tool, I would still use other sources to verify the information.

241
00:37:53.870 --> 00:38:06.160
yunjie.fan@ntu.edu.sg: So, what if the error comes, like, all the, like, your subnetes using AI, like, covering their 90% even more of their job?

242
00:38:06.900 --> 00:38:08.839
yunjie.fan@ntu.edu.sg: How could you just to marry this thing?

243
00:38:09.090 --> 00:38:11.429
Jeremy Hong: So they use AI for 90% of their job.

244
00:38:12.580 --> 00:38:15.009
Jeremy Hong: So what… so what's the question? What would I…

245
00:38:15.200 --> 00:38:18.870
yunjie.fan@ntu.edu.sg: You asked, like, what would you do to fix this kind of.

246
00:38:20.440 --> 00:38:33.109
Jeremy Hong: I mean, in a way, if somebody's using AI for 90% of their job, it is… I mean, if I was to talk to this person, I would say… what I would say is that then that's an issue less so on the person.

247
00:38:33.300 --> 00:38:43.049
Jeremy Hong: And more… less so on the… and more so on the job, because if the job can be replaced by a model, wouldn't this person fear for their job, right?

248
00:38:43.550 --> 00:38:47.089
yunjie.fan@ntu.edu.sg: So which, like, which degree do you think is reasonable?

249
00:38:47.680 --> 00:38:54.669
Jeremy Hong: I don't think at a certain degree or whatnot, because I think, okay, I would say this, like, let's say I'm talking about pilots, right?

250
00:38:55.520 --> 00:39:01.580
Jeremy Hong: Pilots use some sort of computational power, like autopilot, for 95% of their job.

251
00:39:02.060 --> 00:39:20.059
Jeremy Hong: Or even 98% of their job. Like, landing is automated, flying is automated, and takeoff is automated, and literally everything can be automated on the pilot's end. Yet they are paying paid such big bucks, and no one would ever say, oh, we can go on a plane that is completely… completely manned by a computer.

252
00:39:20.540 --> 00:39:37.960
Jeremy Hong: And the reason for that is that… that you need that person there for that 0.1% chance when something goes wrong, and you need human intervention. And the issue is, A, we cannot predict what kind of human intervention can be done, and B, sometimes the computer doesn't know that they need human intervention.

253
00:39:38.880 --> 00:39:54.049
Jeremy Hong: So, I will not give… I cannot say I will give a certain percentage of the time when a person's job can be wholly done by AI, because, hey, even though your job can be done by AI, you still need somebody there to know that this thing is wrong, or something is… something is going… going wrong.

254
00:39:54.990 --> 00:40:08.570
yunjie.fan@ntu.edu.sg: what you just mentioned is just a tool, so what if the people wishing, who is just super skilled with creative thinking, he can just help to… just the AI to help him to efficient his job?

255
00:40:08.570 --> 00:40:16.349
Jeremy Hong: Yeah, and I don't think there's anything wrong with doing that, as long as this person realizes when AI is doing…

256
00:40:16.430 --> 00:40:31.289
Jeremy Hong: Doing something that has an issue, or let's say you are in a line of work that requires creative thinking, or that relies on lateral thought, where it's… where thought is just not just derived from

257
00:40:31.540 --> 00:40:43.639
Jeremy Hong: from an idea that was actually crowdsourced and whatnot, because everything AI… a lot of… in always, a lot of things that AI does is through, like, models itself does, is through lateral thinking, as opposed to…

258
00:40:43.800 --> 00:40:45.580
Jeremy Hong: Real creative work.

259
00:40:46.580 --> 00:40:49.130
yunjie.fan@ntu.edu.sg: To which do you think maybe I…

260
00:40:49.550 --> 00:40:54.830
yunjie.fan@ntu.edu.sg: Good guidelines for the… for the younger generations to collaborate with it.

261
00:40:55.120 --> 00:41:02.429
Jeremy Hong: I think… I think the thing is about collab… when you have collaboration, as you talk about the younger generation and collaboration, they need to understand…

262
00:41:03.540 --> 00:41:06.560
Jeremy Hong: Understand the topic at hand before they

263
00:41:06.960 --> 00:41:08.890
Jeremy Hong: Before they use it as a tool.

264
00:41:09.640 --> 00:41:10.170
Jeremy Hong: Right?

265
00:41:10.690 --> 00:41:13.090
Jeremy Hong: Because if you don't understand what you are doing.

266
00:41:13.280 --> 00:41:14.830
Jeremy Hong: You can't use it as a tool.

267
00:41:15.110 --> 00:41:18.909
Jeremy Hong: It's very… I won't say you can't, but it's very dangerous to use it as a tool.

268
00:41:19.490 --> 00:41:20.469
Jeremy Hong: Because you don't know.

269
00:41:20.470 --> 00:41:20.840
yunjie.fan@ntu.edu.sg: Yeah.

270
00:41:20.840 --> 00:41:22.220
Jeremy Hong: don't know when something goes wrong.

271
00:41:22.460 --> 00:41:28.929
yunjie.fan@ntu.edu.sg: Yeah, yeah, but it's the thing, is some of the younger generation is just using AI to help them to better understand.

272
00:41:29.440 --> 00:41:42.969
Jeremy Hong: Yeah, and for a lot of these people, they will not have any issues. I mean, if they go through life without any issues, great, right? But when something go… if something does go wrong, they will not know it, and that could be bad.

273
00:41:43.320 --> 00:41:54.349
Jeremy Hong: But, I mean, that's the thing. In a lot of ways, right? We talk about, like, automated driving, for example, when you see cars driving on their own versus human drivers.

274
00:41:54.630 --> 00:42:07.600
Jeremy Hong: the incidence rate of a automated car crashing is lower than when there's human, like, interacted, like, driving. Like, someone drives on their own versus a computer driving or autopiloting a vehicle.

275
00:42:07.730 --> 00:42:16.079
Jeremy Hong: Oh, but the thing is, is… there's… there's a net benefit for everyone not driving their cars, but I think that… that time…

276
00:42:16.190 --> 00:42:20.469
Jeremy Hong: like, as long as the incidence rate of that is lower, I think that's fine, right?

277
00:42:21.180 --> 00:42:23.979
yunjie.fan@ntu.edu.sg: Yeah. But, so…

278
00:42:24.750 --> 00:42:39.440
yunjie.fan@ntu.edu.sg: So, which way do you think is appropriate for the collaboration? If you just cut into one of your working scenarios, like, from the 0 to 1, which do you think is the best flow for the younger generations to collaborate?

279
00:42:41.010 --> 00:42:48.290
Jeremy Hong: I think it's good as a checker of your work, or as a checker for…

280
00:42:49.060 --> 00:42:52.150
Jeremy Hong: Again, as a tool, right? I think…

281
00:42:52.510 --> 00:42:56.640
Jeremy Hong: They need their basics, well, they need foundational knowledge.

282
00:42:57.770 --> 00:43:15.880
Jeremy Hong: like, if you look at it as a, this can replace thought, then I think that's… that becomes very dangerous. I mean, I'm not exactly sure about the question that you're asking me, like, because that… in everything, you need the foundation, you need the basics, and without that information… without the basics and the foundational level.

283
00:43:16.530 --> 00:43:22.540
Jeremy Hong: I don't think… I don't think people should… it shouldn't be something that they rely upon to replace it.

284
00:43:23.320 --> 00:43:28.400
yunjie.fan@ntu.edu.sg: Yes, but I feel like most of the audience you're talking about is with experienced, but

285
00:43:28.440 --> 00:43:48.160
yunjie.fan@ntu.edu.sg: Most of the students who are the younger generations, they are using AI to help them to realize something. Like, they are into a new startup work, they don't know how to understand, how to begin, how to just executive, so it just asks AI, and it gives him a guideline to operate.

286
00:43:48.580 --> 00:43:58.840
yunjie.fan@ntu.edu.sg: So, and then he follows AI's instructions to do step-by-steps. Don't you feel like this kind of issues could be a problem, or you feel, oh, that is okay?

287
00:43:59.870 --> 00:44:04.780
Jeremy Hong: I think I… okay, so, on my part, at the near term.

288
00:44:05.000 --> 00:44:20.479
Jeremy Hong: In the near term, I think it's a problem. And the reason why I think it's a problem is not necessarily having to do with the fact that, oh, they're not critical think… they are not critical thinkers or whatnot. I think right now, in talking about the workplace, is that people

289
00:44:20.720 --> 00:44:25.660
Jeremy Hong: people see AI as the most efficient way to do certain things.

290
00:44:25.790 --> 00:44:30.989
Jeremy Hong: And not without understanding a larger issue at hand of why certain things are being done.

291
00:44:31.070 --> 00:44:43.939
Jeremy Hong: But, however, in the long term, I think this is something that would be, in general, more encouraged by the general populace. And the reason for that is in the same way as, I would say, search engines, right?

292
00:44:44.000 --> 00:44:55.869
Jeremy Hong: in the past, people saw search engines as a… as a deter… as a… something that… that is bad for the workplace, because it meant people didn't know how to search for information on their own. Let's say they go into the library to search for

293
00:44:55.870 --> 00:45:05.380
Jeremy Hong: search for a paper, now we just go onto Google, we type a few things, and we get the papers that we were looking for. People don't understand how to go through journals anymore.

294
00:45:05.410 --> 00:45:08.069
Jeremy Hong: In terms of on the research basis, right?

295
00:45:08.150 --> 00:45:14.599
Jeremy Hong: And… but then right now, everyone is saying, okay, it's been widely said, okay, Google it and see what information comes out there.

296
00:45:15.470 --> 00:45:29.489
Jeremy Hong: And I think that's how, in terms of the work… work life… work in the workplace, the attitudes towards AI will shift as people get a larger understanding of what AI is, and the workflows in it will change.

297
00:45:29.550 --> 00:45:47.269
Jeremy Hong: So, I don't… I think it's more of a short-term issue where you have two conflicting ideas of how certain things should be done, and when people converge on AI as the most efficient way of doing things, that's where, I think work becomes a lot easier, or a lot more collaborative.

298
00:45:48.520 --> 00:45:52.619
yunjie.fan@ntu.edu.sg: But don't you think that it needs some change for the younger shit?

299
00:45:53.310 --> 00:46:00.729
Jeremy Hong: I… the thing about change in… do you think AI… do I think AI needs to be changed, or do I think the younger generation needs to change?

300
00:46:01.780 --> 00:46:11.249
yunjie.fan@ntu.edu.sg: No, I mean, like, some training shit. Like, some training to them is… help them to have some… some inspiration in doing something. But…

301
00:46:11.470 --> 00:46:21.559
yunjie.fan@ntu.edu.sg: if they just lack of the training, they just have no clue to doing this kind of thing, right? So, the only thing they can rely on is the tools right now is AI.

302
00:46:22.410 --> 00:46:36.750
Jeremy Hong: Okay, I think I understand your question, and I would… what I would… what I would counter with that is that no one ever trained, like, someone like… like me, I'm not… I'm not sure who… how old you are, but no one ever taught me how do I look through things in a…

303
00:46:36.940 --> 00:46:38.700
Jeremy Hong: an encyclopedia.

304
00:46:39.130 --> 00:46:49.799
Jeremy Hong: Like, I was never… or how do I find research paper? How do I find online journals? It, like, those things were never a part of how I did my work, and I was never trained in those things.

305
00:46:49.800 --> 00:47:01.929
Jeremy Hong: However, that also meant that I can do things perfectly fine. That didn't mean, oh, I don't understand their use case or whatnot. Like, how many of the younger generation do not know how to use a dictionary, for example?

306
00:47:02.250 --> 00:47:04.460
Jeremy Hong: Yet, they are perfectly fine in spelling.

307
00:47:05.010 --> 00:47:22.669
Jeremy Hong: Like, I think AI merely is an evolution of what we had in the past. Like, Google itself, like a search engine in itself, is AI… at the heart of it is AI as well, right? In terms of how things are shown to us, what is indexed, and how it's displaying stuff to us.

308
00:47:22.670 --> 00:47:28.610
Jeremy Hong: But we… we just take… we take it as fact. Not even fact, we just live… live with it, and…

309
00:47:28.610 --> 00:47:35.860
Jeremy Hong: I don't think a lot of people were trained on how to use Google. They just learned how to use it, they learned how to incorporate that into their lives.

310
00:47:36.170 --> 00:47:43.030
Jeremy Hong: So, on that front, I don't necessarily think that, oh, because, like, they need training to change how

311
00:47:43.220 --> 00:48:00.310
Jeremy Hong: how AI should… yes, there are people that we should probably teach skills to, to better themselves, but I think the fundamental way in which we evolve is that a lot of people, when they see AI, they just treat it as such, and then there's a new paradigm that kind of

312
00:48:00.350 --> 00:48:17.409
Jeremy Hong: develops based on that. I don't think that necessary training has to do with it, because I think when we talk about a common denominator, let's say for AI tools, people, like, developers and how programs are developed go towards that lowest common denominator.

313
00:48:17.410 --> 00:48:27.540
Jeremy Hong: And people… like, right now, Google has a lot of powerful tools, like, how you… in terms of how you search for certain things, but 99% of the population doesn't use them.

314
00:48:27.810 --> 00:48:42.800
Jeremy Hong: And I think these… the same skills that people say that, oh, like, the younger generation currently lack, in terms of critical thinking and whatnot, will be replaced by other things where they are… they are much more superior in other ways.

315
00:48:44.650 --> 00:48:48.030
yunjie.fan@ntu.edu.sg: So, which thing do you think is the biggest risk for AI?

316
00:48:48.540 --> 00:48:57.089
Jeremy Hong: the biggest risk for… the biggest risk for AI is… is… comes from, as we said, a lack of critical thinking.

317
00:48:57.120 --> 00:49:08.879
Jeremy Hong: And the biggest risk that I see is that because of how AI, like, how the current state of AI, in terms of how, like, models are generated, relying on

318
00:49:09.170 --> 00:49:12.159
Jeremy Hong: On things that have already been done before.

319
00:49:12.270 --> 00:49:21.570
Jeremy Hong: Or things that are existing already, is that creative works or new forms of thinking are no longer being developed.

320
00:49:22.000 --> 00:49:41.660
Jeremy Hong: I think that in itself is the biggest risk of AI. However, there's nothing to say that AI… how AI is or was, like, AI in the past, what I used to deem as AI was algorithmic, and now it's models. I am not going to say AI is going to remain stagnant and not address the issues that I am… I am saying.

321
00:49:42.470 --> 00:49:51.030
Jeremy Hong: So, I think, based on how AI is right now, I would say, hey, critical thinking and a lack of new thought, lack of unique thought.

322
00:49:51.320 --> 00:50:03.049
Jeremy Hong: And the thing is… what I am worried about in terms of AI is that evolution of… of, let's say, speech… of speech, that evolution of…

323
00:50:03.440 --> 00:50:09.210
Jeremy Hong: of just… How… how things are developed, and how things are going to…

324
00:50:09.570 --> 00:50:25.569
Jeremy Hong: how things might remain stagnant because of AI. Like, if you see from the 1970s… from the 1970s to the 80s to the 90s, there are distinct eras of certain… certain things. But when you look at AI, if AI just, in a way, regurgitates how

325
00:50:25.810 --> 00:50:41.820
Jeremy Hong: what has been done before, or whatnot, or makes modification, or creates variation on what's been done before, culture doesn't change. People would remain, in a way, stand still on, like, singular moments. Like, right now, like, would…

326
00:50:41.820 --> 00:50:50.079
Jeremy Hong: would there be a possibility where, like, the entirety of AI kind of, like, Kind of halts cultural development.

327
00:50:50.230 --> 00:50:52.760
Jeremy Hong: That is a, that is a way, and that is a fear.

328
00:50:55.380 --> 00:51:10.260
yunjie.fan@ntu.edu.sg: Actually, but you have already mentioned about the creativity of AI, but, I feel like, really, training should be you talking with AI, like you continuously feel it the information that you want him to present.

329
00:51:10.280 --> 00:51:15.390
yunjie.fan@ntu.edu.sg: And it's just generally to be personalized based on

330
00:51:15.720 --> 00:51:26.600
yunjie.fan@ntu.edu.sg: like, different kind of information you're giving to it. Have you ever, like, noticed this kind of thing? Maybe it's just gonna… to get a revolution in the future, to be more…

331
00:51:26.600 --> 00:51:36.830
yunjie.fan@ntu.edu.sg: like, personalized, to deal with the kind of issue about creativity, or the thing you have already mentioned in the personalized, this kind of thing.

332
00:51:36.830 --> 00:51:51.309
Jeremy Hong: Personalization is… I don't see as a store for the issue. I think personalization is something that I think will definitely come to AI, and has already come to AI, in terms of… of consumer… on the consumer side of things, and how things are…

333
00:51:51.480 --> 00:52:04.280
Jeremy Hong: being generated. However, what I do worry is that what can be personalized to me, it would remain stagnant because of the way AI currently is, in terms of how models are generated.

334
00:52:04.490 --> 00:52:23.510
Jeremy Hong: I think those things are, like, separate things… things, right? Like, the fear is because everyone kind of… information is… is sought on a… when the models are generated kind of basis, it will kind of… or slow down the development of new things.

335
00:52:24.230 --> 00:52:29.480
Jeremy Hong: Because everything, like, information-seeking and whatnot is solely based on the past.

336
00:52:30.200 --> 00:52:42.880
Jeremy Hong: I think personalization is merely an extension of what is currently done on the current models. I think personalization is in itself a… won't be an evolution on what we have.

337
00:52:43.210 --> 00:53:03.179
Jeremy Hong: But it doesn't address that cultural shifting. So, okay, I'm gonna give an example of this… something that is not really AI-related, but rather how culture in itself has remained stagnant. So, if you look… if you look at when the iPhone came out in 2000 and… 2007 till now.

338
00:53:03.800 --> 00:53:13.540
Jeremy Hong: As much as we talk about the social media revolution and whatnot, the way we interact with our devices has largely remained similar.

339
00:53:13.990 --> 00:53:22.689
Jeremy Hong: In terms of a glass… a glass display, and music, and everything being siloed within apps, or in some ways, widgets.

340
00:53:23.030 --> 00:53:27.889
Jeremy Hong: But our interaction paradigm primarily is still a touchscreen device.

341
00:53:27.890 --> 00:53:43.250
Jeremy Hong: And in doing so, if you look at how the 8… like, in the 20 years before the iPhone, from the 1980s to 2007, that shift was very drastic. Like, the idea of communicating to somebody from across the world.

342
00:53:43.450 --> 00:53:54.360
Jeremy Hong: it seemed very foreign. But right now, even if you look at how we are interacting right now, we are interacting through Zoom, we are talk… well, we are talking… we are having this conversation, correct?

343
00:53:54.660 --> 00:53:55.130
yunjie.fan@ntu.edu.sg: Hmm.

344
00:53:55.130 --> 00:54:01.009
Jeremy Hong: This thing could have been done 20 years ago, maybe through a different platform, maybe through, like, Skype, or even a phone call.

345
00:54:02.210 --> 00:54:07.289
Jeremy Hong: But it's… the interaction paradigm has… had… had not… had not shifted.

346
00:54:07.610 --> 00:54:10.999
Jeremy Hong: Whereas in, like, in the 20 years before.

347
00:54:11.180 --> 00:54:16.109
Jeremy Hong: before the iPhone, this sort of communication would be unheard of.

348
00:54:17.080 --> 00:54:36.399
Jeremy Hong: the speed in which technology has evolved has shrunk significantly. The amount of queues that we have gotten has not gotten that much better. It has evolved upon itself, like, in terms of how we… in terms of talking, like, oh, Zoom is free, we don't have to use, like, a landline to communicate.

349
00:54:36.500 --> 00:54:43.450
Jeremy Hong: how that thing has worked remained the same. However, because we had that foundation on which phone calls are made on.

350
00:54:43.710 --> 00:54:55.759
Jeremy Hong: it evolved around itself, as opposed to coming with a new paradigm in which we communicate. And I think that's where AI will struggle. It will struggle to create new paradigms in which thought can be processed.

351
00:54:56.790 --> 00:54:59.589
Jeremy Hong: Because it is merely building on the past.

352
00:55:00.360 --> 00:55:04.440
yunjie.fan@ntu.edu.sg: So what if just with the development, the AI must, or…

353
00:55:04.810 --> 00:55:12.990
yunjie.fan@ntu.edu.sg: has got to be involved in the collaboration, in the teamwork, so… what do you think is the best role for him, or for the AI?

354
00:55:13.200 --> 00:55:25.570
Jeremy Hong: I think that would depend on the team itself, right? In terms of how, A, how many people are using AI, the trust of AI, and how established certain protocols are set in place. Because I think

355
00:55:25.570 --> 00:55:33.489
Jeremy Hong: If, let's say, you're in a team of 10 who has been doing something for a very long time, and you introduce AI into the program.

356
00:55:33.910 --> 00:55:44.919
Jeremy Hong: into that process, they will shun AI, because they have been doing something that works for so long. And in a lot of cases, these are the companies that would succeed because

357
00:55:44.920 --> 00:55:57.779
Jeremy Hong: they have a system that works, and they shouldn't change it, or this is the company that fail because they don't go on with the times. I think how AI should be done is, as I said from the very start, as merely a tool.

358
00:55:57.780 --> 00:56:05.800
Jeremy Hong: for them to… for them to think… think of ways that maybe the team of 10 had never thought of before in some ways. However, I think…

359
00:56:05.990 --> 00:56:11.399
Jeremy Hong: they cannot just blindly follow AI, they have to have their own expertise to… in which to…

360
00:56:12.080 --> 00:56:23.090
Jeremy Hong: to see how AI work… AI can be integrated into their work stream. And there's no, like, right or wrong answer in terms of a, oh, a certain percentage of… oh, a certain percentage needs to be done by AI.

361
00:56:23.400 --> 00:56:36.349
Jeremy Hong: I'll go back to the pilot analogy. In the past, pilots had to do everything in a flight. Now they barely do anything on a flight. It does not… they make their role any less relevant. They still need to be in the cockpit. However.

362
00:56:36.460 --> 00:56:51.309
Jeremy Hong: However, their workload has reduced. And in that same way, I see AI working in that way for a lot of places where, hey, I still need somebody who understands the product. And in a lot of ways, right, it will kind of, like, make the people who

363
00:56:51.460 --> 00:56:55.199
Jeremy Hong: Know what they are doing that much more successful.

364
00:56:56.590 --> 00:57:10.359
yunjie.fan@ntu.edu.sg: So, which do you think is more important? Like, for the future education managers or future education involvers, is to need more AI features or have, like, strong critical thinking?

365
00:57:10.900 --> 00:57:17.970
Jeremy Hong: I… I… how I see it is that I think AI Pint.

366
00:57:18.290 --> 00:57:25.919
Jeremy Hong: Let me put it this way. I think AI will fundamentally change how people perceive different things, and I think

367
00:57:26.140 --> 00:57:34.880
Jeremy Hong: as somebody in education, what I would say is that the demand for certain educational functions might be reduced.

368
00:57:36.430 --> 00:57:41.999
Jeremy Hong: And that's how I see the AI shifting education needs.

369
00:57:42.920 --> 00:57:48.339
Jeremy Hong: Because I think… I mean, in the past, a li… okay, a librarian was somebody

370
00:57:48.390 --> 00:58:06.530
Jeremy Hong: whose role was a lot more critical, because a librarian was somebody who helped you find information. It was not just, help me find a book, but where do I find certain things? And being a librarian requires a master's degree. It still requires a master's degree and whatnot. However, the role is, in a way, diminished because people

371
00:58:06.690 --> 00:58:10.650
Jeremy Hong: Can, in a way, solve, like, 90% of their job.

372
00:58:10.850 --> 00:58:13.659
yunjie.fan@ntu.edu.sg: At this point, with true search engines and the internet.

373
00:58:13.870 --> 00:58:17.000
Jeremy Hong: And then the role of the librarian became less popular.

374
00:58:17.360 --> 00:58:25.980
Jeremy Hong: And that's the way I would say AI as well, that AI will make certain… certain roles diminished, and because of that, in the educational space.

375
00:58:26.080 --> 00:58:28.899
yunjie.fan@ntu.edu.sg: It will make certain, like, degrees less popular.

376
00:58:29.880 --> 00:58:44.779
Jeremy Hong: And from there, it's like the shift for there would be, hey, to identify, okay, how can we offer programs that will make people more interested in furthering their education, and how do we create a value proposition for people who want to further their education?

377
00:58:47.200 --> 00:58:54.269
yunjie.fan@ntu.edu.sg: So, which is your, like, best decision, or for your thoughts? Is the stronger creative thinking more important, or AI features?

378
00:58:55.070 --> 00:58:56.789
Jeremy Hong: I… I would say…

379
00:58:56.940 --> 00:59:10.109
Jeremy Hong: in a way, if I think how AI features in itself would become more important, because I think critical thinking… if critical thinking reduces, these AI features have to do, kind of.

380
00:59:10.400 --> 00:59:16.249
Jeremy Hong: the critical thinking will only reduce when AI features become as strong as they are.

381
00:59:17.170 --> 00:59:22.049
Jeremy Hong: Because, I mean, even nowadays, right, we assume certain things are correct already.

382
00:59:22.050 --> 00:59:22.680
yunjie.fan@ntu.edu.sg: Hmm.

383
00:59:22.680 --> 00:59:32.249
Jeremy Hong: let's say we use Excel, correct? Then we… we just put a sum of everything. We don't assume that thing is wrong. It could be wrong, we just don't… we just never know.

384
00:59:32.840 --> 00:59:38.689
Jeremy Hong: Because we just assume that whatever we put… the formula that we enter into Excel is correct.

385
00:59:39.770 --> 00:59:40.640
Jeremy Hong: And do we…

386
00:59:40.640 --> 00:59:41.320
yunjie.fan@ntu.edu.sg: Yeah.

387
00:59:41.320 --> 00:59:43.579
Jeremy Hong: That level of critical thinking was…

388
00:59:43.900 --> 00:59:54.069
Jeremy Hong: was taken out of us, instead of, like, checking every sum row by row. We don't do that anymore, we don't double-check our work anymore, let's say we do an Excel sheet, because

389
00:59:54.600 --> 01:00:04.100
Jeremy Hong: Or else we just assume it's correct. That level of critical thinking in… in not even talking about my, like, new, the new generation, I'm talking about someone like me who uses Excel.

390
01:00:04.420 --> 01:00:06.740
yunjie.fan@ntu.edu.sg: I really don't do that.

391
01:00:06.760 --> 01:00:13.820
Jeremy Hong: And that is not an AI thing. That is just technology has gotten to a point where I trust it enough that even if it's flawed.

392
01:00:13.980 --> 01:00:16.939
Jeremy Hong: It's better than I manually count everything at once.

393
01:00:17.900 --> 01:00:31.490
Jeremy Hong: And then that's the thing of AI, where as AI features become better, the way we do certain critical thinking would be reduced in some way, shape, or form. It will naturally be because we take things for granted that they are correct.

394
01:00:31.640 --> 01:00:37.070
Jeremy Hong: But in that, new critical thinking skills might come out as a result, right?

395
01:00:37.880 --> 01:00:44.520
Jeremy Hong: People spend less time on Excel sheets, but they use the Excel sheets, the data from the Excel sheets, to draw conclusions.

396
01:00:44.680 --> 01:00:45.760
Jeremy Hong: even more.

397
01:00:47.030 --> 01:00:58.850
Jeremy Hong: And I think that's how I would say that critical thinking would… might evolve because of the AI features, but the AI features would reduce the level of critical thinking as we see it today.

398
01:00:59.020 --> 01:01:02.760
Jeremy Hong: Based on if they are trusted enough.

399
01:01:03.000 --> 01:01:16.669
yunjie.fan@ntu.edu.sg: I agree with you, I agree with you with that. But, my… so, what is my… my thought and my concern right now about the research, because this is my… not my research, it's the PhD research. I was thinking

400
01:01:16.730 --> 01:01:28.430
yunjie.fan@ntu.edu.sg: Which way do you think may be better to solve this kind of problem? If we… back to the infrastructure, we're back to the product, how can the users have stronger critical thinking?

401
01:01:28.780 --> 01:01:31.569
yunjie.fan@ntu.edu.sg: Buy, maybe, the updates of the product.

402
01:01:31.990 --> 01:01:38.170
yunjie.fan@ntu.edu.sg: And in the, in the, in the, in the pace of the AI gets stronger and stronger.

403
01:01:40.040 --> 01:01:45.300
Jeremy Hong: It's… how do we… retain critical thinking. Is that the question?

404
01:01:46.190 --> 01:02:01.540
yunjie.fan@ntu.edu.sg: like, if we're back to design the product of AI, we start from the infrastructure, we start from the technical issues, we… how can we just not damage, like, people's critical thinking, but we still make it stronger?

405
01:02:02.260 --> 01:02:04.589
Jeremy Hong: That's the thing, right? I… I think…

406
01:02:05.270 --> 01:02:13.659
Jeremy Hong: I don't necessarily think we need… we need to have all these functions, because people would just, as we say, take things for granted.

407
01:02:14.700 --> 01:02:27.929
Jeremy Hong: that there will be no glitches or anything. People… and naturally, I think as a collective, critical thinking would be reduced as a result. However, what I would say is, because it is such an efficient tool.

408
01:02:28.130 --> 01:02:45.140
Jeremy Hong: the way… the work we define as critical thinking might change, right? Is that people are no longer obsessed over counting, right? I go back to the Excel spreadsheet example. People in the past used to be obsessed with counting, making sure that all… that their calculations were correct.

409
01:02:45.730 --> 01:03:03.820
Jeremy Hong: as opposed to seeing what was calculated and trying to draw, like, for example, let's say it's a business, and trying to come profit, like, profit and loss, they were making sure that whatever their sums were correct, as opposed to saying, oh, I made less profit here, I made more profit there, how do we better the business?

410
01:03:04.450 --> 01:03:08.250
Jeremy Hong: AI, in a way, based on how useful it is.

411
01:03:08.350 --> 01:03:18.139
Jeremy Hong: can draw… make people… can shift people away from the doing… from the… from stuff that could be automated, for example, and how we thought of Excel in the past now can be

412
01:03:18.220 --> 01:03:36.280
Jeremy Hong: shifted towards, oh, finding out information, or finding out history, and how do we learn from those things? I think it will shift… it shifts that… it shifts the paradigm of what we define as critical thinking. Because in the past, what we might have defined as critical thinking was merely

413
01:03:36.290 --> 01:03:40.919
Jeremy Hong: Somebody who did a lot, who was meticulous in a certain way.

414
01:03:41.190 --> 01:03:53.079
Jeremy Hong: Whereas, I think for, like, as an AI product designer, I would not necessarily worry so much about creating a product that makes people think critically, whereas I would want a product that does

415
01:03:53.110 --> 01:04:04.810
Jeremy Hong: the features that it says, and develop more features that will remove… that will take stuff that used to be manual labor into something that could be automated. And from there.

416
01:04:04.880 --> 01:04:18.339
Jeremy Hong: create… and from there, how people think, the collective way of people, how people think, would change, and I would think it would… it might change for the better, because it… as I said, I don't see AI as an end-all be-all, I see it as a tool.

417
01:04:19.190 --> 01:04:24.870
Jeremy Hong: And there will naturally be people who just see it as an end-all, be-all, but I… I think that is true for almost any new invention.

418
01:04:25.150 --> 01:04:25.820
yunjie.fan@ntu.edu.sg: Hmm.

419
01:04:26.930 --> 01:04:34.379
yunjie.fan@ntu.edu.sg: Okay… But, you said about the change of the thinking problem, but I feel like…

420
01:04:34.520 --> 01:04:39.819
yunjie.fan@ntu.edu.sg: It's just due to the people generally cannot define right or wrong.

421
01:04:39.910 --> 01:04:57.370
yunjie.fan@ntu.edu.sg: So it's just gonna gradually turn out to be an ethical issue, like, years after years, because you… everybody generates the information. It's not about right or wrong, right? Maybe it's all about… about the… the default generates continuous going. So…

422
01:04:57.780 --> 01:04:58.690
yunjie.fan@ntu.edu.sg: Yes.

423
01:04:59.150 --> 01:05:06.630
yunjie.fan@ntu.edu.sg: You said about the paradigm, but what is the paradigm in the after days is just the wrong thing.

424
01:05:07.090 --> 01:05:26.630
Jeremy Hong: Look, but when it's just the wrong thing, that is… that can be true for even stuff that isn't AI, correct? Like, you see the idea of right or wrong, for example, the use of vaccines. Like, there is a scientific… there's scientific proof, and there is… there is belief.

425
01:05:27.090 --> 01:05:38.070
Jeremy Hong: And I think that's… that's the thing of when people… there's no matter what kind of technology there is, or whatever it is, they will all… belief will all… belief will always trump

426
01:05:38.420 --> 01:05:42.520
Jeremy Hong: effect, or reality. And… and…

427
01:05:42.810 --> 01:05:54.550
Jeremy Hong: I'm gonna give… I'm gonna give that example again, it's like, when you… when you look back into the past, where people believed that the Earth was… Earth was square… was a square, was… and we would fall off the Earth, it was not a sec… a…

428
01:05:54.550 --> 01:06:06.239
Jeremy Hong: spherical item. And a lot of people believe that, but as more information came in, as the information became more widespread, as we created tools that help us to generate

429
01:06:06.950 --> 01:06:20.859
Jeremy Hong: generate critical… we use tools that would help… that aided in our critical thought, right? Like, I mean, because, I mean, like, Google… that search engine social media gave us more access to more information.

430
01:06:21.130 --> 01:06:24.730
Jeremy Hong: And from there, people were able to form more critical thought.

431
01:06:26.430 --> 01:06:38.260
Jeremy Hong: Such entries in itself, or even social media in general, these are all algorithms that are based on a certain form of models. A certain form of… of VWAP.

432
01:06:38.450 --> 01:06:52.860
Jeremy Hong: a reward system in which they will show you things that you agreed upon in general, right? In terms of social media, that confirmed your biases and whatnot. That is true, that can be true for AI as well, where it can confirm your biases, and in that case.

433
01:06:52.890 --> 01:07:06.100
Jeremy Hong: But that does not mean that AI in itself can be a bad thing. Just because AI delivers something that can be critically… that is objectively wrong, it does not mean that people would objectively say AI is wrong.

434
01:07:06.470 --> 01:07:19.480
Jeremy Hong: I think it can give you false positives of a result, but that's the truth for almost everything, in terms of any form of information gathering system from the past. From a communication standpoint.

435
01:07:20.570 --> 01:07:30.729
yunjie.fan@ntu.edu.sg: Do you feel like it's actually a natural thing? Maybe the AI gonna have to give every users or every audience a wrong consensus.

436
01:07:31.300 --> 01:07:43.040
Jeremy Hong: Like, a wrong consensus can be built based on a number of factors, and it does not have to… like, AI is merely a platform in which a wrong consensus can come upon.

437
01:07:44.740 --> 01:07:46.870
yunjie.fan@ntu.edu.sg: Hmm, okay.

438
01:07:47.550 --> 01:07:52.450
yunjie.fan@ntu.edu.sg: So, do you have any suggestions for avoiding this kind of issue?

439
01:07:52.840 --> 01:07:59.739
Jeremy Hong: of avoiding. That's the thing, is that I would say, as we develop, we have to make sure that we hold

440
01:07:59.810 --> 01:08:15.750
Jeremy Hong: like, AI developers account… like, accountable, and making sure that errors are seen as a bad thing, as opposed to something that just happens, right? There needs to be a comfortability for these errors, and this is something that we are seeing upon, because

441
01:08:16.000 --> 01:08:32.370
Jeremy Hong: Fortunately or unfortunately, we live in an environment in which errors, like, mistakes are seen as persona non grata. They are something that people… people feel like, hey, these things have to be perfect, we have pushed AI to this standpoint that

442
01:08:32.370 --> 01:08:38.530
Jeremy Hong: Any mistake that AI makes has to be… it is something that is… makes AI completely fall on itself.

443
01:08:38.560 --> 01:08:41.720
Jeremy Hong: But I think it's every tool that we have made is not perfect either.

444
01:08:42.450 --> 01:08:47.429
Jeremy Hong: But we don't… we don't criticize when Google gives us the wrong answer for something, correct?

445
01:08:48.420 --> 01:08:55.480
Jeremy Hong: In the same way, because we see… because we see AI as this level of, like, AI has to be perfect.

446
01:08:55.620 --> 01:09:09.640
Jeremy Hong: That's the best way that we continue to do so, because until something that is better than AI, what we consider AI now to be better, we will always trust AI to be perfect, and when AI isn't, we hold it to a higher standard.

447
01:09:10.740 --> 01:09:14.540
yunjie.fan@ntu.edu.sg: Okay, okay. So, yes, I think,

448
01:09:14.710 --> 01:09:19.420
yunjie.fan@ntu.edu.sg: I am all done with my questions. Do you have anything maybe you feel like.

449
01:09:19.420 --> 01:09:22.350
Jeremy Hong: Yeah, I don't have anything on my end.

450
01:09:22.700 --> 01:09:23.700
yunjie.fan@ntu.edu.sg: Okay.

451
01:09:23.800 --> 01:09:35.800
yunjie.fan@ntu.edu.sg: Yes, I feel like it's super, like, unique for your insights, actually. I have never been about some of your… your thinkings about the paradigms, or about your…

452
01:09:35.899 --> 01:09:45.560
yunjie.fan@ntu.edu.sg: like, you're already the one that I feel like is so unique that you've, like, 100% no believing in life. That is surprised me.

453
01:09:45.600 --> 01:09:57.680
yunjie.fan@ntu.edu.sg: Well, actually, I feel like, it's a good conversation between you and us, and I feel like maybe you can just involve in the future related studies, maybe?

454
01:09:57.820 --> 01:09:59.110
Jeremy Hong: Yep, sure, no problems.

455
01:09:59.590 --> 01:10:09.900
yunjie.fan@ntu.edu.sg: Okay, and once again, thank you for your participation, and we may contact you for the… for the, like, the court rotations in the future.

456
01:10:09.900 --> 01:10:10.800
Jeremy Hong: Okay, sure.

457
01:10:10.800 --> 01:10:13.029
yunjie.fan@ntu.edu.sg: Okay, okay, thank you for your time.

458
01:10:13.030 --> 01:10:14.169
Jeremy Hong: Thank you so much.

459
01:10:14.170 --> 01:10:15.089
yunjie.fan@ntu.edu.sg: Thank you, yeah, bye.



受访人29:
WEBVTT

1
00:00:00.000 --> 00:00:01.330
Norris: 继续来吧。

2
00:00:02.790 --> 00:00:07.780
Fan Yunjie: 你是很有话语权的人啊，看来谁都要找你聊聊天。

3
00:00:07.780 --> 00:00:13.740
Norris: 不是的，单纯就是开会，我讲了一遍，然后他们没记住，说，能再来一遍吗？我说，你赶紧开录像。

4
00:00:14.410 --> 00:00:17.590
Fan Yunjie: 不是你，你的下属这样子吗？

5
00:00:18.090 --> 00:00:19.180
Fan Yunjie: 哇，你，你说。

6
00:00:19.180 --> 00:00:24.950
Norris: 不算是下属吧？我，我们没有人比谁低的，就是你也知道创业公司嘛。

7
00:00:27.010 --> 00:00:33.520
Fan Yunjie: 那你那你现在的这个项目你方便跟我介绍一些吗？因为我看到官网其实。

8
00:00:33.520 --> 00:00:39.720
Norris: 可以啊，就是官网上面不是有一个比较模糊的小动画嘛，那个动画我也不知道为什么不会搞清的就是

9
00:00:41.610 --> 00:00:44.920
Norris: 简单来讲呢，就是因为我自己

10
00:00:44.920 --> 00:01:09.630
Norris: 发薪是我同时管理很多家公司，然后我自己的个人生活，所以我邮件就很混，邮件，很混的这个结果就是我每天早上的工作流是先去开每一个邮箱，然后挨个过，因为我很怕就是错过一些比较重要的，我就要把没有用的先删掉，或者是给它已读掉，然后有用的，我看它有用，

11
00:01:09.630 --> 00:01:13.840
Norris: 然后我再把它给变成未读或者是加一个flag，我就不知。

12
00:01:13.840 --> 00:01:14.370
Fan Yunjie: 对呀，你。

13
00:01:14.370 --> 00:01:27.680
Norris: 走这么一遍之后呢？我再回来去看我有什么事情要回，然后把能回的就回掉。然后如果有后续的需要我再做什么事情，准备什么文件的，我再一个一个做掉就是这个工作有很漫长。

14
00:01:27.700 --> 00:01:46.050
Norris: 然后我就幻想说，能不能有一个工具把它给统一一下。我当然尝试了非常多的就是那种邮件整合工具，像什么superhan，像什么这个notion male not刚出嘛，还有spark就是整合在一起，也并不解决这个问题，因为内容还是需要我自己去读的，

15
00:01:46.110 --> 00:01:48.270
Norris: 所以我就

16
00:01:48.360 --> 00:02:01.280
Norris: 开发了这么一套系统，让ai去把我所有的邮件挨个读一遍，不管是垃圾邮件啊，还是诈骗邮件啊，还是广告啊？还是钓鱼啊？或者是正式的邮件啊，都读一遍读一遍之后呢？他自然就知道

17
00:02:01.280 --> 00:02:09.500
Norris: 哪些是有用的，哪些是没用的，哪些与我的相关性更强？哪些与我的完全无关，那么哪一些还是

18
00:02:09.500 --> 00:02:12.780
Norris: 就是一个类别的，它们是一起的，

19
00:02:12.810 --> 00:02:16.230
Norris: 那都可以归类，就是他可以把这些工作都做掉，

20
00:02:16.380 --> 00:02:23.080
Norris: 那我有了这个工具之后，我就可以每天只看他给我总结出来的一些智能卡片就可以了。

21
00:02:24.270 --> 00:02:31.270
Fan Yunjie: Okay，那那你现在是。有有把任何的这个api接进来吗？还是说。

22
00:02:31.760 --> 00:02:38.060
Norris: 我只接了谷歌的api，就是我把邮件获取过来呀，因为我邮件是数据源头嘛。

23
00:02:38.620 --> 00:02:40.450
Fan Yunjie: 那用的是什么模型啊？

24
00:02:40.710 --> 00:02:47.080
Norris: 用的是jamline和这个jamai，还有一个hbt。

25
00:02:48.420 --> 00:02:50.920
Fan Yunjie: 那你觉得你有多大程度上相信

26
00:02:51.060 --> 00:02:52.060
Fan Yunjie: 就是

27
00:02:52.190 --> 00:02:55.710
Fan Yunjie: 他给你管理的这种这种导向。

28
00:02:59.890 --> 00:03:00.650
Norris: 目前。

29
00:03:00.650 --> 00:03:01.600
Fan Yunjie: 他有多大，不可能。

30
00:03:01.600 --> 00:03:10.670
Norris: 使用过程中要分情况，如果是过滤我的这种垃圾邮件，钓鱼邮件，它的确是做到了百分之百。

31
00:03:10.930 --> 00:03:24.010
Norris: 然后如果说是识别我的工作，它在归类方面还不是很准，但是它的确是能帮我把里边的日程。我的机票，我的酒店

32
00:03:24.190 --> 00:03:32.790
Norris: 就是具体的时间，地点都帮我提的比较准确，包括时区也会自动换算就是我感觉还是比较靠谱的吧。

33
00:03:33.340 --> 00:03:37.980
Fan Yunjie: 明白，所以一开始的话，就是因为自己觉得这个东西

34
00:03:38.160 --> 00:03:45.240
Fan Yunjie: 比较有比较大的这个可能性，解决你的你的这个效率。所以说，想要创立这样的团队。

35
00:03:45.490 --> 00:03:46.580
Norris: 是的。

36
00:03:47.190 --> 00:03:50.500
Fan Yunjie: 他是从大概多久之前的做这个事情啊。

37
00:03:50.500 --> 00:03:51.340
Norris: 一年。

38
00:03:52.190 --> 00:03:56.690
Fan Yunjie: 那现在的团队规模也没有啊，十个人。

39
00:03:57.840 --> 00:04:01.520
Fan Yunjie: 然后现在的这个状况就是大部分是推。

40
00:04:01.520 --> 00:04:04.550
Norris: 现在状况就是没有营收，然后，

41
00:04:04.840 --> 00:04:07.320
Norris: 然后也没有融资，然后。

42
00:04:08.200 --> 00:04:10.230
Norris: 就全靠我自己撑着。

43
00:04:11.750 --> 00:04:14.540
Fan Yunjie: 那说明你很有实力啊，我不能说。

44
00:04:14.790 --> 00:04:19.260
Norris: 我，我，我我还行吧，但是

45
00:04:19.380 --> 00:04:23.550
Norris: 就就是肯定没有把这个团队做很大嘛，

46
00:04:23.690 --> 00:04:26.100
Norris: 要不然有实力的话就做更大了。

47
00:04:27.160 --> 00:04:32.690
Fan Yunjie: 那你如果你认为的话就是他最核心的，或者说他

48
00:04:32.980 --> 00:04:35.680
Fan Yunjie: 最优势的是处理哪些任务啊？

49
00:04:36.080 --> 00:04:59.730
Norris: 最优势的，最优势的是处理日程类的，就是我们有一个功能，比如说你下我们的手机app，然后你们需要看到一个poster，或者是你朋友在给你微信上发说，我们下午三点去喝个茶，就是所有这些东西只要涉及到时间，地点的一截图丢给它，它自动就帮你生成具体的calendar entry同步到你的邮箱的

50
00:04:59.730 --> 00:05:02.720
Norris: 同步到你的手机的上，就是。

51
00:05:03.270 --> 00:05:16.780
Norris: 它会做到这种全自动化，那么第二个优点也是自动化，就是包括邮件里面提到的。assign给你的一些assignment的一些工作都会变成你的，

52
00:05:17.110 --> 00:05:31.620
Norris: 然后也可以就是查找相关的邮件，比如说abcde五封邮件acc是相关的，他就可以帮你把这三个邮件总和成一个小的notes，然后直接读这个notes就像读了三封邮件。

53
00:05:33.050 --> 00:05:39.130
Fan Yunjie: 现在这种我没有太了解这种有相对来说竞品吗？比较牛的。

54
00:05:39.740 --> 00:05:40.550
Norris: 没有。

55
00:05:41.630 --> 00:05:49.990
Fan Yunjie: 那他其实还是比较多的解决一些就like unique proposition这样的，其实它还不错的呀。

56
00:05:50.670 --> 00:05:59.720
Norris: 我们应该是比较有edge的吧，只是说没钱就是我现在没有钱去宣传它，所以就还处于一个steel smote。

57
00:06:00.600 --> 00:06:03.710
Fan Yunjie: 那你是从什么时候开始就是

58
00:06:03.920 --> 00:06:08.490
Fan Yunjie: 接触到ai，或者说是开始就involve in到ai的呢？

59
00:06:08.890 --> 00:06:12.360
Norris: Involve ai的话应该是charge发布就involve了吧。

60
00:06:12.780 --> 00:06:16.510
Fan Yunjie: 你的意思是就是充当一个用户吗？还是说。

61
00:06:16.790 --> 00:06:21.810
Norris: 他充当了一个用户，然后开始开发ai的话，这应该是我的第四个产品了。

62
00:06:22.360 --> 00:06:23.990
Fan Yunjie: 之前的话也是都是。

63
00:06:23.990 --> 00:06:25.490
Norris: 尝试了三次都失败了

64
00:06:25.600 --> 00:06:26.410
Norris: 啊，刚好。

65
00:06:26.410 --> 00:06:30.150
Fan Yunjie: 也都是在做这种类似plugin，或者说是

66
00:06:30.350 --> 00:06:35.860
Fan Yunjie: 解决一些人机交互问题的这一类的产品吗？

67
00:06:36.310 --> 00:06:44.970
Norris: 没有我之前做的比较大，就是比较成型的一个产品，也拿到了。Vc的一个产品是做分布式计算的，基础框架的。

68
00:06:45.160 --> 00:06:47.420
Fan Yunjie: 喔。我说底层的。

69
00:06:47.420 --> 00:06:48.750
Norris: 多底层的infer。

70
00:06:50.070 --> 00:06:57.990
Fan Yunjie: 那您现在还会经常用ai吗？写作还是说基本上已经就停留在就是团队产品的层面上了。

71
00:06:57.990 --> 00:07:06.150
Norris: 没有啊，我个人生活的话，离不开ai的，就每天我跟ai的交互的token，绝对这20块钱的plus物超所值。

72
00:07:06.150 --> 00:07:06.770
Fan Yunjie: 哈哈哈

73
00:07:07.860 --> 00:07:11.020
Fan Yunjie: okay，大部分你都用它做什么。

74
00:07:11.500 --> 00:07:12.730
Norris: 大部分写代码。

75
00:07:13.200 --> 00:07:15.100
Fan Yunjie: 大部分写代码，那你说。

76
00:07:15.100 --> 00:07:16.140
Norris: 部分查资料。

77
00:07:16.450 --> 00:07:18.880
Fan Yunjie: 啊？你学过prompt engineering吗？

78
00:07:18.880 --> 00:07:20.360
Norris: 学没有学过啊。

79
00:07:20.360 --> 00:07:23.810
Fan Yunjie: 没有学过okay，所以说你觉得

80
00:07:24.030 --> 00:07:29.980
Fan Yunjie: 多大程度之上你会依赖ai在你的日常生活，like a percentage。

81
00:07:30.800 --> 00:07:33.820
Norris: 30%.

82
00:07:34.640 --> 00:07:38.040
Fan Yunjie: 30%，那也还好啊，其实也没有很依赖，

83
00:07:38.240 --> 00:07:44.210
Fan Yunjie: 没有没有更多的依赖的原因是因为你觉得他还是有很大程度上不可信吗？还是说。

84
00:07:44.210 --> 00:07:48.410
Norris: 不是不是单纯单纯，就是他还没有cater for其他的expect。

85
00:07:51.020 --> 00:07:52.440
Fan Yunjie: 明白。

86
00:07:52.590 --> 00:08:00.580
Fan Yunjie: 所以我想知道一下你以你个人用户来说的话，你大概跟ai互动的一个一个flow是怎么样的。

87
00:08:01.260 --> 00:08:07.320
Norris: 基本就是我想到一个问题，如果我本来可以用谷歌查的，我会变成去问查gpt，

88
00:08:07.430 --> 00:08:13.190
Norris: 因为我不知道关键词是什么，所以我就用自然语言去替代了我的关键词。

89
00:08:15.750 --> 00:08:21.800
Fan Yunjie: 那你觉得这个会影响你对于你自己的这个产品未来迭代的很多判断吗？

90
00:08:22.480 --> 00:08:24.540
Norris: 跟我的产品关系不大。

91
00:08:24.540 --> 00:08:33.990
Fan Yunjie: 跟你产品关系不大，你们，你为什么现在还会有什么在写代码的问题啊，是你现在也在这个产品，或者说是技术，这里充当什么角色吗？

92
00:08:33.990 --> 00:08:39.620
Norris: 没有，我是还有personal project，然后没有人帮我写的。我自己在写。

93
00:08:39.620 --> 00:08:41.070
Fan Yunjie: 是这样。

94
00:08:41.510 --> 00:08:43.059
Fan Yunjie: 你这学什么专业的呀。

95
00:08:43.380 --> 00:08:44.320
Norris: 有土木的。

96
00:08:46.340 --> 00:08:51.870
Fan Yunjie: 你是在美国吗？还是说还是说有有有有尝试，不同的市场。

97
00:08:52.450 --> 00:08:58.530
Norris: 市场的话，我来美国之前就是在澳大利亚，然后。

98
00:08:58.990 --> 00:09:04.700
Norris: 在澳洲当时还没有ai吗？我来美国的时候是2021年疫情期间，

99
00:09:04.910 --> 00:09:16.380
Norris: 那个时候是做了一款游戏产品，然后本来是打向澳洲市场的，结果澳洲那边就是人太少了，没什么市场，正好美国这边爆了，我就我就搬家到美国来了。

100
00:09:16.840 --> 00:09:22.310
Fan Yunjie: 那那个，那个产品是是相关的吗？跟five还是纯gam。

101
00:09:22.560 --> 00:09:31.780
Norris: 没有纯game相关的一个游戏平台，我发给你。目前目前应该也是全世界除了中国以外最大的游戏陪玩平台吧。

102
00:09:32.140 --> 00:09:35.140
Fan Yunjie: 这么牛的我对游戏不是很了解。

103
00:09:35.140 --> 00:09:46.590
Norris: 也就是游戏社交。你可以理解为一个。擦边的平台，它虽然涉黄就是，但是很多人可能就是冲著这个裸露的。

104
00:09:46.590 --> 00:09:48.480
Fan Yunjie: 带点干部的属性，是吗？

105
00:09:49.210 --> 00:09:50.310
Norris: 有一点什么。

106
00:09:50.310 --> 00:09:53.390
Fan Yunjie: 就是有点赌博的属性在是吗？

107
00:09:53.390 --> 00:09:54.490
Norris: 没有没有完全没有。

108
00:09:55.800 --> 00:09:59.800
Fan Yunjie: 明白ok，我们回来这里吧，回来这里谈回到ai。

109
00:09:59.830 --> 00:10:18.490
Fan Yunjie: 所以说就是你觉得ai，因为你是经历过前期的，不是ai相关的这种。这种时代跨到ai之后，你觉得你个人在你自己的工作，或者说你自己的产品，你的思维会发生多大的改变呢？

110
00:10:19.200 --> 00:10:21.060
Norris: 没听这个问题。

111
00:10:21.470 --> 00:10:33.720
Fan Yunjie: 就是来你现在去思考一个project，或者说你现在要去开始去进行一个项目的时候，你在ai的这个时代之前跟ai时代之后，你会有很大的差别吗？

112
00:10:33.850 --> 00:10:35.400
Norris: 呵，差别太大了，

113
00:10:36.060 --> 00:10:45.810
Norris: 就基本上你一个产品立项，你肯定要做前期的这个调研工作吧。不管是市场调研还是产品可行性调研。

114
00:10:46.150 --> 00:10:52.840
Norris: 所有的这些工作现在全都可以用ai代替了，之前需要去做客户访谈，现在不需要了，因为你会asse

115
00:10:52.970 --> 00:10:56.430
Norris: ai在训练的过程中已经有了这些人的input。

116
00:10:58.210 --> 00:11:01.280
Fan Yunjie: 但是你又有多大程度上相信他呢？

117
00:11:11.100 --> 00:11:17.610
Norris: 我一直感觉这个相信不相信的问题没有那么重要，因为世界的真相都不是很重要。

118
00:11:17.610 --> 00:11:20.750
Fan Yunjie: Okay,

119
00:11:21.210 --> 00:11:24.880
Fan Yunjie: 对，这个就是很大的问题在于，这个差别啊，就是我们。

120
00:11:24.880 --> 00:11:25.220
Norris: 我们。

121
00:11:25.220 --> 00:11:41.380
Fan Yunjie: 我们在采访一些就是business level和这个的人对于这个事情的可信性，比方说我自己啊，我之前一直是做做做工作，做企业的，你也知道我之前做什么呢？

122
00:11:41.590 --> 00:11:42.790
Fan Yunjie: 你知道吗？

123
00:11:42.790 --> 00:11:44.110
Norris: 不知道啊，金融吗？

124
00:11:44.680 --> 00:12:02.660
Fan Yunjie: 我回头会跟你说吧，然后。然后等到我进到这个这个比较学术的领域之后，我就发现大家对于这可行性的问题很追究，很咬文嚼字，一定要觉得他是要追溯的，追溯那个根源的

125
00:12:02.660 --> 00:12:21.710
Fan Yunjie: 要去看啊，我，我这个东西输出来的这些文本，每一个字，它的这个应用来源，它本身的这个dui是不是真正的是是一个可信性，还是说ai在骗我？还是说我就是不小心的进入到了这种ai的环境当中，然后被它去去

126
00:12:21.710 --> 00:12:24.470
Fan Yunjie: 去牵著鼻子走，这样子所以。

127
00:12:24.470 --> 00:12:29.530
Norris: 我感觉看就是人对于世界的认知的差别了，学术上可能比较较真。

128
00:12:30.110 --> 00:12:38.360
Fan Yunjie: 但是你会有这种困扰吗？就比方说，ai也许会把你牵到一个正好相左的方向，然后你就跟著他走了。

129
00:12:38.860 --> 00:12:42.080
Norris: 我不会啊，就是ai说错了，我肯定知道啊，

130
00:12:43.060 --> 00:12:48.070
Norris: 我干这事儿，他如果说错了，那说明我问的问题就错了。

131
00:12:48.640 --> 00:12:53.500
Fan Yunjie: 那你是怎么去判断，或者说是靠什么判断。

132
00:12:54.060 --> 00:12:55.240
Norris: 靠经验。

133
00:12:55.510 --> 00:12:56.610
Fan Yunjie: 靠经验。

134
00:12:58.290 --> 00:12:59.380
Fan Yunjie: 我想要去。

135
00:12:59.380 --> 00:13:00.790
Norris: 没错了，我肯定是知道的。

136
00:13:01.450 --> 00:13:05.840
Fan Yunjie: 你是完全靠经验去知道他给你的答案是错的。

137
00:13:07.000 --> 00:13:09.670
Fan Yunjie: 那你还会去去跟他。

138
00:13:10.250 --> 00:13:10.920
Norris: 38.

139
00:13:10.920 --> 00:13:13.150
Fan Yunjie: 奶头，你会啊。

140
00:13:13.310 --> 00:13:14.500
Norris: Beatles它

141
00:13:14.780 --> 00:13:21.590
Norris: 你给我瞎掰什么东西呢？你刚才给我改过来，你刚才说这个完全是无稽之谈，我就会这么骂他。

142
00:13:21.590 --> 00:13:32.280
Fan Yunjie: 那你你要跟我讲一下这个过程，比方说你从头从零到一开始某一个项目，然后你大概跟跟他是怎么样的一个配合协作。

143
00:13:34.090 --> 00:13:46.210
Norris: 比如说我现在的personal project需要写代码嘛，那写代码的时候，我可能由于描述的不够全面，那他就会asse去脑补一些条件，

144
00:13:46.310 --> 00:14:04.690
Norris: 那这些条件加起来之后代码的方向就不是我的方向了，他可能多给了几个看起来非常全面的supporting的这个角度，但是我并不需要那个supporting的角度，反而是由于加了那个东西之后打掉了我未来的可能性，

145
00:14:05.560 --> 00:14:22.960
Norris: 所以我就会跟他说，你不要，就是我先看他这些回答，然后他的回答里边，比如说有四五点，然后后边两点呢？完全是optional的。但是他认为说建议怎么怎么样，建议怎么样，我说你不要给我建议你刚才说的这些东西，我告诉你，为什么你不需要给我建议？

146
00:14:24.290 --> 00:14:36.080
Fan Yunjie: 但是在这个基础之上，你会不会去去靠它完完全全的输出一个最优结果，还是说你就自己去optimize了。

147
00:14:42.280 --> 00:14:51.930
Norris: 你其实你们目前研究的课题就是这个问题啊，就是人机交互啊，这个东西必然是有人有机啊。

148
00:14:52.930 --> 00:14:57.110
Fan Yunjie: 但每一个人，他人机交互的程度不一样啊对吧，

149
00:14:57.320 --> 00:15:05.480
Fan Yunjie: 有些人他可能就是大概靠ai给他一轮输出，他就即使发现他是错的，他也不会去纠错，

150
00:15:05.910 --> 00:15:15.630
Fan Yunjie: 他就自己去把它优化完之后，可能就是后面的工作，他就是自己的了，但有一些人，可能，他就是会跟ai深度的去交互。

151
00:15:15.630 --> 00:15:18.130
Norris: 你没有为什么跟他，而不是自己去纠错吗？

152
00:15:18.450 --> 00:15:22.070
Fan Yunjie: 你会是白头，然后一直让他给到你最好的答复。

153
00:15:22.070 --> 00:15:26.180
Norris: 对，你知道为什么吗？就是最核心的问题是，他们，我不会写代码呀。

154
00:15:27.080 --> 00:15:29.420
Fan Yunjie: 呵，

155
00:15:29.760 --> 00:15:32.520
Fan Yunjie: 那但是你会不会觉得有点浪费时间？

156
00:15:32.760 --> 00:15:34.720
Fan Yunjie: 从效率上来想。

157
00:15:35.330 --> 00:15:39.140
Norris: 会啊，就是我今天刚teach了一个我改了50多遍的单码

158
00:15:40.670 --> 00:15:43.710
Norris: 50多遍。我花了我大概两三周的时间。

159
00:15:45.160 --> 00:15:57.060
Fan Yunjie: 那你都不会说，觉得我要通过有效性来讲，我就减少对于ai的依赖，然后更大程度之上就是我大概把工作留我这边多做一点，这样子。

160
00:15:57.250 --> 00:16:06.320
Norris: 不会的趋势不可逆，没有必要去探讨这个问题，这个趋势就是趋势，你如果不能跟ai共存的话，你就被淘汰。就这么简单。

161
00:16:08.150 --> 00:16:22.270
Fan Yunjie: 那你如果说你是带着经验来看你所在处理的工作流，那很多的，比方说现在的就那些大学生就还没有去到研究阶段的小朋友，

162
00:16:22.360 --> 00:16:29.360
Fan Yunjie: 他们对于很多事情的判断能力就很差，然后很大程度上还在依赖ai，这样子。

163
00:16:29.560 --> 00:16:30.090
Norris: 那么。

164
00:16:30.090 --> 00:16:34.620
Fan Yunjie: 会不会影响到他本身自己的思维或者主观的判断。

165
00:16:35.020 --> 00:16:39.950
Norris: 这么说吧，影响了，也就影响了这个人该怎么长，这是他的命数。

166
00:16:39.950 --> 00:16:40.930
Fan Yunjie: 哈哈哈哈

167
00:16:42.610 --> 00:16:45.260
Fan Yunjie: okay，不要这么说。

168
00:16:45.730 --> 00:16:51.420
Norris: 这个真的不重要，就是我还是那句话，世界都是假的，你何必较真呢？

169
00:16:52.460 --> 00:17:09.339
Norris: 我们无非就是过来经历了，经历完了，体验好坏也没有差别。戴了这个虚拟眼镜，你又重新开一句新的游戏，还是戴上之后忘却前世喝了孟婆汤，整个这个过程无非就是重启，重启再重启对吧？

170
00:17:10.730 --> 00:17:20.640
Fan Yunjie: 那你觉得你现在的很大程度上还有跟他拜托五轮的能力，那未来的小朋友有跟他拜托五轮的能力吗？

171
00:17:21.079 --> 00:17:27.969
Norris: 我明白你的意思了。就是说，如果他们都raise from这个aier，他们就没有这个experience去。

172
00:17:28.019 --> 00:17:32.609
Fan Yunjie: 那不对的，因为首先ai训练，它并不是只通过一个数据员。

173
00:17:33.469 --> 00:17:38.909
Norris: 而且ai和ai之间也是有区别的，未来肯定是多智能体协作时代，

174
00:17:39.559 --> 00:17:46.399
Norris: 你不可能就一个产品里边就一个ai agent吧，它肯定是多个agent嘛，多个agent也不可能只有一个api吧，

175
00:17:46.809 --> 00:18:01.329
Norris: 他肯定是有gpt四，有可能gpt五，然后又加上一个jamai，然后又加上一个通10,000,000就是他们的训练源头都不一样，可能说八个里面有四个是对的四个是错的，但是总会有八es的。

176
00:18:02.710 --> 00:18:07.310
Fan Yunjie: 但本身其实ai它是一个概率论出来的。这种算法。

177
00:18:07.310 --> 00:18:09.280
Norris: 所以就是问题就是概率啊。

178
00:18:09.460 --> 00:18:23.440
Fan Yunjie: 对啊，但是这种东西它很大程度上是没有确切真正正的那种就是data analyses的这样子很确切的过程，你还是需要你本身自己去纠错的呀。在我认为对。

179
00:18:29.550 --> 00:18:32.420
Norris: 这不好意思，我又没跟上你的问题。Sorry。

180
00:18:32.910 --> 00:18:44.320
Fan Yunjie: 对啊，就是就是这样。我是觉得ai它本身是概率生成文字的一个模型，对吧？它不是数据分析的一个模型。

181
00:18:44.320 --> 00:18:44.960
Norris: 对。

182
00:18:44.960 --> 00:18:55.090
Fan Yunjie: 所以说，如果带著这样子的话，我会，我会有一个，我会有一个。这种这种担忧在于比方说现在

183
00:18:55.190 --> 00:19:10.630
Fan Yunjie: 大概率的，大家的靠ai使用，那你ai是通过比较大的这种信息流来给你总结，那如果说，它就是在一个错误的基础上，你如果还在存在很多很较真的行业。

184
00:19:11.280 --> 00:19:23.910
Fan Yunjie: 是在这个问题上去去迭代，而不是说在一个正确的方向之上，这个就是之前可能没有ai的过程当中，大家不会去那么大程度上犯错。

185
00:19:24.300 --> 00:19:30.080
Norris: 不会啊，怎么不会，难道是没有ai的时候，你是怎么查资料的呢？翻图书馆看谷歌

186
00:19:30.320 --> 00:19:35.410
Norris: google scholar就一定是verified source吗？所有的paper就一定是对的嘛。

187
00:19:37.140 --> 00:19:47.330
Fan Yunjie: 也是存在这个问题，但是你肯定还是会有比较多的critical thinking在你自己个人，而不是说完全就是依赖它的成果。这一类的。

188
00:19:47.560 --> 00:20:00.370
Norris: 对你想啊，你就把谷歌或者是ai，作为两个对立面，一个是你读了之后自己思考一个是，你读了之后就不思考了。但问题是为啥我读了ai，我就会不思考呢？

189
00:20:01.450 --> 00:20:09.080
Fan Yunjie: 当然啊，这个就是我们在探讨的问题啊，就是很明显，肯定还是会思考的呀，你是一个独立个体啊，你是生物啊，你有应急反应啊。

190
00:20:10.230 --> 00:20:30.230
Fan Yunjie: 可是现在很多的像你是绝对带着自自己的问题去看他给你解决的问题，但是现在很多的小朋友，他可能就是或者说用户，我不能说是小朋友，现在即使是很很有experience的这些用户，他就是喂给ai，然后直接就利用了，直接就这样子走了。

191
00:20:30.500 --> 00:20:34.320
Norris: 那没办法，社会就是分层的，它这么使用它，说明就在这个层次。

192
00:20:35.070 --> 00:20:37.220
Fan Yunjie: 你认为这是完完全全在

193
00:20:37.980 --> 00:20:43.640
Fan Yunjie: 在这个个人的思维上的问题，而不是说在ai的迭代上的问题。

194
00:20:43.640 --> 00:20:47.440
Norris: 没错，这是人怎么使用工具，而不是工具怎么驯化人。

195
00:20:48.200 --> 00:20:52.030
Fan Yunjie: 但你不会说觉得ai会加剧这个问题吗？

196
00:20:52.030 --> 00:20:57.090
Norris: 加剧，就加剧了世界永远在朝着二级化方面发展。马太效应永远在加深。

197
00:20:58.320 --> 00:21:01.760
Fan Yunjie: 你有没有觉得说想过

198
00:21:01.870 --> 00:21:06.690
Fan Yunjie: 去解决人类的这个问题，ai应该去怎么样的优化。

199
00:21:07.280 --> 00:21:19.290
Norris: 我想过啊，我不是从做了一个基数的infer，就是说做这个什么分布式计算，目的就是把个人的信息留在个人身边，不让它成为一个公众数据吗？

200
00:21:21.080 --> 00:21:21.960
Fan Yunjie: 但是肉粽。

201
00:21:21.960 --> 00:21:27.960
Norris: 更好的使用它，但是如何训练出更好的模型，这个不在我能力范围以内，对吧？

202
00:21:29.160 --> 00:21:42.490
Fan Yunjie: 这样子的，它本身okay，它是把自己的，我可以把把它变成理解为一个区块链的形式吗？就所有大家的数据都以一个block的形式单独的存储了，是这个意思吗？

203
00:21:42.760 --> 00:21:52.670
Norris: 稍微区别一点的就是区块链，它还是在链上的，我这个东西更多的是一个本地化的方案，就相当于是你买了个路由器一样。

204
00:21:53.220 --> 00:21:57.140
Fan Yunjie: 所以也没有将来的一些ethical的一些困扰了，对吧？

205
00:21:57.300 --> 00:22:03.690
Norris: 没有什么就是privacy什么的都不存在的，因为别人也不知道你的信息，aa，大脑也不知道你的信息。

206
00:22:04.310 --> 00:22:07.390
Fan Yunjie: 那他还会被有效的训练吗？

207
00:22:07.800 --> 00:22:14.100
Norris: 有啊，因为我们会通过模糊的这些参数来就是进行语音更新

208
00:22:14.260 --> 00:22:18.030
Norris: 就是我只给你更新parameters，但是我不更新数据，

209
00:22:18.140 --> 00:22:24.450
Norris: 数据只依赖于你现在本机有什么数据，比如说你照过的照片说过的话，去过的地方。

210
00:22:25.940 --> 00:22:29.190
Norris: 他就更更加变成了一个personal的system，

211
00:22:29.500 --> 00:22:30.690
Norris: 对对对，对。

212
00:22:31.360 --> 00:22:38.960
Fan Yunjie: 所以你觉得这是一个最好的。对于ai的定位嘛，就是将来来说的话，从通过人就是人机协作最好的一个方向吗？

213
00:22:39.540 --> 00:22:42.110
Norris: 对于C端用户来说应该是的，

214
00:22:42.610 --> 00:22:49.070
Norris: 对于个体用户来说，每一个人都会有一个独立的服务器，或者是租用一个专属的服务器，

215
00:22:49.620 --> 00:22:52.950
Norris: 怎么干，怎么安全，怎么来呗，数据只会越来越多。

216
00:22:53.600 --> 00:23:00.560
Fan Yunjie: 你觉得说比方现在比较理想的，这种劳动劳工分配的话，你觉得应该大概是一个什么样子的，

217
00:23:00.840 --> 00:23:11.130
Fan Yunjie: 因为现在我觉得已经很大程度之上，ai能帮助大家解决很很很多的工作了，或者说是在比方说在教育这个领域。

218
00:23:11.180 --> 00:23:18.870
Norris: 就招生来看的话，就已经ai就可以帮你去把很多的candidates都已经分配好了，然后你可能做的就是很。

219
00:23:19.070 --> 00:23:23.590
Fan Yunjie: 很personal的工作就是你跟人家面个事，谈个话，聊个天，这样子。

220
00:23:23.930 --> 00:23:29.620
Fan Yunjie: 基本上剩下的一些之前很重量型的工作都可以交给ai来做了。

221
00:23:30.240 --> 00:23:38.920
Fan Yunjie: 将来说，如果这种劳工分配，你就怎样分配，是一个比较好的一个理想的状态，然后也保留了人类的这些

222
00:23:39.490 --> 00:23:46.800
Fan Yunjie: 就是不不不不不去依赖ai的问题，或者说不去降低自己批判性思考的问题。

223
00:23:47.310 --> 00:23:51.380
Norris: 好好问题，这个绝对可以出一篇paper了。

224
00:23:51.830 --> 00:23:53.190
Fan Yunjie: 哎呀。

225
00:23:53.190 --> 00:23:58.880
Norris: 让我想一想，如何能保持自己的主观独立性，

226
00:24:00.640 --> 00:24:01.940
Norris: 然后

227
00:24:02.330 --> 00:24:04.550
Norris: 还能更好的跟ai协作。

228
00:24:04.750 --> 00:24:05.400
Fan Yunjie: 二喔。

229
00:24:07.740 --> 00:24:10.240
Norris: 如何能保持主观独立性，

230
00:24:15.190 --> 00:24:24.610
Norris: 我感觉是肯定可以保持的，因为本身ai的算法虽然是在模拟脑神经元嘛，但是

231
00:24:24.790 --> 00:24:33.020
Norris: 他毕竟没有这么多的神经元，如果真的模拟人脑的多少万亿个神经元，全世界的龟都不够用了。

232
00:24:34.900 --> 00:24:37.440
Norris: 你倒是不用担心什么

233
00:24:37.880 --> 00:24:41.500
Norris: 算法的同质化，它肯定是不一样的，

234
00:24:42.030 --> 00:24:45.000
Norris: 他还是停留在零，一的二金质层面上的，

235
00:24:45.390 --> 00:24:51.900
Norris: 我们还是停留在脑电波概率论和量子态的一个状态，

236
00:24:52.100 --> 00:24:53.170
Norris: 所以

237
00:24:53.480 --> 00:24:54.810
Norris: 我们

238
00:24:55.050 --> 00:24:58.140
Norris: 就是很难说被同化，

239
00:24:58.400 --> 00:25:00.600
Norris: 它的确是一种配合关系，

240
00:25:01.080 --> 00:25:04.020
Norris: 有一点像你养了一个小狗，

241
00:25:04.370 --> 00:25:08.540
Norris: 这个小狗时间长了，你好像感觉它挺通人性的。

242
00:25:09.350 --> 00:25:10.800
Norris: 但实际上他就是个狗，

243
00:25:12.650 --> 00:25:16.760
Norris: 他永远不会张嘴跟你聊天，他表达的方式只能是汪汪汪。

244
00:25:19.370 --> 00:25:25.490
Fan Yunjie: 但是他也会伴随着你的训练越来越像人啊，越来越像你啊，你这个主人。

245
00:25:25.740 --> 00:25:29.930
Norris: 对呀，它只是长得像你皮像肉，像骨，像可能皮像有点像。

246
00:25:29.930 --> 00:25:32.480
Fan Yunjie: 谁说他长得像那么的红。

247
00:25:32.480 --> 00:25:33.810
Norris: 谁养傻狗？我跟你说。

248
00:25:34.050 --> 00:25:34.850
Fan Yunjie: 怎么样。

249
00:25:34.850 --> 00:25:38.360
Norris: 人养什么狗真的长得有点像的。

250
00:25:38.360 --> 00:25:41.840
Fan Yunjie: 这是什么？这这

251
00:25:41.840 --> 00:25:52.380
Norris: 现吗？养沙皮狗的都是比较，就是脸比较垮的人，然后养那个面目的稍微人就会至少脑子会活一点。

252
00:25:53.200 --> 00:26:06.280
Fan Yunjie: 这个是这个原因，我的认为，是啊，人家所有人都会有自恋的倾向，所以他喜欢去养跟自己长得像的东西，而不是说这个东西因为被他养，所以跟他长得像。

253
00:26:06.720 --> 00:26:08.670
Norris: 对，对，对也是有可能的，

254
00:26:09.210 --> 00:26:15.370
Norris: 那你去用ai的时候也会尽量去选择说说话顺你意义的ai咯。

255
00:26:15.480 --> 00:26:28.650
Norris: 比如说这个他就会有情绪化的反馈，说你真棒什么的ppt就更加的专家化，就是就事论事，对吧，那肯定切的主题，用户方向就不一样了。

256
00:26:29.720 --> 00:26:36.940
Fan Yunjie: 那你觉得说你现在目前还会给每一个ai去分工吗？按照他们的。

257
00:26:37.360 --> 00:26:43.530
Norris: 不会啊，我就是从成本角度考虑，在使用过程中，哪一个成本低，我就用哪一个。

258
00:26:44.780 --> 00:26:48.170
Fan Yunjie: 你现在最最多使用的还是gpt，对吗？

259
00:26:48.340 --> 00:26:49.340
Norris: 对对，对，

260
00:26:50.120 --> 00:26:51.600
Norris: 因为包月了嘛。

261
00:26:51.600 --> 00:26:54.540
Fan Yunjie: 因为花钱了。

262
00:26:54.940 --> 00:26:55.530
Fan Yunjie: Ok.

263
00:26:55.530 --> 00:27:11.970
Norris: 另外几个呢，都是按照token算的，所以就token哪个低。比如说我们用那个flash模型就去process所有的邮件，那你看一个邮件的html，如果他是广告邮件的话剧场五笔非常费token，那你肯定不会用全模型去process他嘛，

264
00:27:12.220 --> 00:27:14.060
Norris: 你就用就好了。

265
00:27:15.040 --> 00:27:18.790
Fan Yunjie: 哎，可是我们弹回刚刚那个问题，那个狗的问题

266
00:27:18.910 --> 00:27:25.220
Fan Yunjie: 就是，但是用户，在很多情况下，他会自动化的去信任ai啊，对吧。

267
00:27:25.220 --> 00:27:25.940
Norris: 哼。

268
00:27:26.460 --> 00:27:33.010
Fan Yunjie: 对啊。但是这种信任机制逐渐形成之后就变成ai去反向训练人类了呀。

269
00:27:34.360 --> 00:27:38.040
Norris: Aa没有这个主观意识，去反向训练人类，这是我想表达的。

270
00:27:38.260 --> 00:27:39.540
Fan Yunjie: 当然，对。

271
00:27:39.710 --> 00:27:43.320
Norris: 他没有这个意图，他也不在乎他，也不在乎人类。

272
00:27:45.610 --> 00:27:56.060
Fan Yunjie: 对啊，但是人类它就会慢慢的会变成了一种有奴性的动物，然后就类似于人类对于某种熟悉刺激的这种习惯化。

273
00:27:56.210 --> 00:27:57.000
Norris: 哼。

274
00:27:57.000 --> 00:27:59.130
Fan Yunjie: 然后隐性形成。

275
00:27:59.300 --> 00:28:00.030
Norris: 哼，

276
00:28:00.370 --> 00:28:01.240
Norris: 没问题啊。

277
00:28:01.350 --> 00:28:09.500
Fan Yunjie: 这种自动化的信任就会导致可能更大程度之上人类本身它的这种credibility。

278
00:28:09.500 --> 00:28:10.330
Norris: 哼。

279
00:28:10.710 --> 00:28:11.370
Fan Yunjie: 就会。

280
00:28:11.740 --> 00:28:12.290
Norris: 明白。

281
00:28:12.290 --> 00:28:14.380
Fan Yunjie: 会受到很大的影响。

282
00:28:14.640 --> 00:28:17.040
Norris: 你喜欢吃米饭或者面条吗？

283
00:28:18.560 --> 00:28:20.280
Fan Yunjie: 我喜欢吃面条。

284
00:28:20.280 --> 00:28:21.040
Norris: Ok,

285
00:28:21.550 --> 00:28:25.670
Norris: 那么小麦驯化人类，这个事情你也接受了呀。

286
00:28:27.520 --> 00:28:35.970
Fan Yunjie: 那你是伴随自己since的判断的，呀。他也没有说，比方说现在我们的这一个世界上，小麦消失了。

287
00:28:36.350 --> 00:28:38.320
Fan Yunjie: 那我也没有办法呀，对吧？

288
00:28:38.320 --> 00:28:41.400
Norris: 对啊，今天ai主机关掉了，我也没有办法呀。

289
00:28:42.560 --> 00:28:46.360
Fan Yunjie: 但你仍然会接受。我喜欢面条这个事情。

290
00:28:46.360 --> 00:28:48.790
Norris: 当然了，当然了。

291
00:28:48.790 --> 00:28:59.300
Fan Yunjie: 而且我确实是喜欢面条这个事情。然而现在的问题是，即使ai关掉它，给到很多用户的已经既定的结果，它仍然是相信的对。

292
00:28:59.560 --> 00:29:00.680
Norris: 对对，对，

293
00:29:02.180 --> 00:29:02.980
Norris: 是啊。

294
00:29:03.940 --> 00:29:08.230
Fan Yunjie: 所以你觉得如何避免这种错误信任的这种情况，或者说是。

295
00:29:08.230 --> 00:29:18.500
Norris: 避免，我还是这个主观就是我认为能用好ai的人自然就能用好用不好的被驯化了，也就被驯化了。这是一种自然选择，这是进化对

296
00:29:18.890 --> 00:29:19.980
Norris: 不是训话。

297
00:29:21.430 --> 00:29:27.570
Fan Yunjie: 但是我觉得我们通你也是做产品呢？我也是做很多无用的工作的人，

298
00:29:27.780 --> 00:29:35.170
Fan Yunjie: 然后应该还是要思考一下怎么样帮人类变得更好，而不是说就是适者生存的问题吧。

299
00:29:35.300 --> 00:29:38.820
Norris: 人类本身能活到今天就是适者生存的结果。

300
00:29:40.040 --> 00:29:47.260
Fan Yunjie: 但是我觉得他是需要后边的更强大的人去去帮助他们的，就是说他们。

301
00:29:47.260 --> 00:29:50.060
Norris: 我跟你说，爱因斯坦也没有真的想写相对论，

302
00:29:50.840 --> 00:29:59.450
Norris: 马克思维，也没有真的想发明方城主，他们都只是兴趣使然，这个世界就是被快乐催化的吗？

303
00:29:59.660 --> 00:30:06.390
Norris: 你爱干什么就干什么，不爱干什么就不干什么？真的哪天世界灭亡了，大家一整眼发现都在母体里边重生了，

304
00:30:06.500 --> 00:30:08.340
Norris: 也没谁真的没有了。

305
00:30:09.610 --> 00:30:16.610
Fan Yunjie: 但是有这样一很多的机制可以去避免这个问题啊。比方说，你现在去设计这个clarity ai。

306
00:30:16.610 --> 00:30:17.590
Norris: 对吧。

307
00:30:17.590 --> 00:30:22.770
Fan Yunjie: 你就是比方设计一个什么认知预警的一个机制。

308
00:30:23.130 --> 00:30:26.810
Fan Yunjie: 让就让用户去激活一下他们本身的这种。

309
00:30:26.810 --> 00:30:28.220
Norris: 思考能力也是ok的，

310
00:30:28.520 --> 00:30:30.540
Norris: 我不会的，因为对我来说是成本。

311
00:30:33.060 --> 00:30:36.340
Fan Yunjie: 如果说现在都我们现在都是很有钱了。

312
00:30:36.630 --> 00:30:37.260
Norris: 嗨，

313
00:30:37.490 --> 00:30:38.040
Norris: 我觉得。

314
00:30:38.040 --> 00:30:40.730
Fan Yunjie: 化了这个当然。

315
00:30:40.730 --> 00:30:46.410
Norris: 如果说钱的无限多的话，我就不开这个产品了。好吧，我开这产品不是为了赚钱吗？现在。

316
00:30:46.410 --> 00:30:48.940
Fan Yunjie: 就是说我们要怎么去。

317
00:30:48.940 --> 00:30:56.590
Norris: 我知道你的意思，但是我真的劝你一下子这个话题真的太无解了，因为你真的改变不了什么。

318
00:30:57.230 --> 00:31:00.780
Fan Yunjie: 这个东西，写写论文还行，你真的要做点什么事，太难了，

319
00:31:01.370 --> 00:31:10.870
Fan Yunjie: 那我们就是跟他重新想知道你们这些产品研发的人员是不是有一些就是trigger，maybe可以去解决这个问题。

320
00:31:11.100 --> 00:31:16.450
Norris: 让我想一想，如果社会给予足够的奖励的话，我可以解决这个问题啊。

321
00:31:16.860 --> 00:31:26.330
Fan Yunjie: 你可以详细说一说，比方说就把它跟block team，maybe，maybe结合一下，然后通过一些token机制，什么，我觉得也是很很有趣。

322
00:31:26.930 --> 00:31:28.030
Norris: En

323
00:31:28.520 --> 00:31:41.820
Norris: 倒，不会有这么复杂的机制了。单纯就是说，如果有一个基金会它的使命，愿景就是避免人类去这个，

324
00:31:42.590 --> 00:31:44.540
Norris: 那么它将会。

325
00:31:44.540 --> 00:31:45.270
Fan Yunjie: Ye

326
00:31:45.270 --> 00:31:50.410
Norris: 直接给开发过程中的产品，或者是可以用pmf的产品就打钱嘛。

327
00:31:50.750 --> 00:31:53.880
Fan Yunjie: 就是你达到什么样的一个阶段，我就给你多少钱吧。

328
00:31:55.280 --> 00:32:00.480
Norris: 比如说你能通过什么什么测试这个产品通过什么什么反图龄测试，我就给你多少钱。

329
00:32:01.670 --> 00:32:10.460
Fan Yunjie: 你说的是一种，这是一种就是比较正向的反馈。你如果说现在就是regulation，他就是这样子约束你的话呢？你怎么样。

330
00:32:10.460 --> 00:32:18.080
Norris: 那就干呗，就是如果已经有regulation了，然后他要求说，哎，你必须有这么一个东西，不然不让你上线，那我就做呀，那怎么办？

331
00:32:18.080 --> 00:32:20.540
Fan Yunjie: 好怎么做。

332
00:32:20.540 --> 00:32:24.610
Norris: 怎么做？我想想啊，怎么去规范它，

333
00:32:25.440 --> 00:32:26.130
Norris: 我们

334
00:32:26.500 --> 00:32:29.850
Norris: 激活用户的自我认知，

335
00:32:31.000 --> 00:32:35.940
Norris: 这个他们有点反产品啊，因为产品最最核心的就是要是易用性嘛，

336
00:32:36.380 --> 00:32:40.290
Norris: 你用户用的时候，你突然打断它，就很不易用。

337
00:32:41.710 --> 00:32:46.990
Fan Yunjie: 这个。当然我跟你说，学术跟商业最大的产，这个这个

338
00:32:47.220 --> 00:32:51.320
Fan Yunjie: 差别就是在于这两个相左的方向拉扯，

339
00:32:51.470 --> 00:32:58.320
Fan Yunjie: 你就发现，好像我之前提过一个一个课题，然后都没有人来接盘的问题就是我在

340
00:32:58.470 --> 00:33:01.800
Fan Yunjie: 去探讨这个gating app上映性的问题。

341
00:33:02.440 --> 00:33:21.810
Fan Yunjie: 对，然后包括。但是你对于产产品来说的话，人家人家就是希望你上瘾啊，人家不希望你本身因为一些算法给你推荐你喜欢的人，然后你去去形成一些不良性的关系处理态度，他们才不care这个，但是我想解决的是这种对于关系的

342
00:33:21.900 --> 00:33:23.770
Fan Yunjie: 这种不认真的态度。

343
00:33:23.770 --> 00:33:26.070
Norris: 这两个相左的方向。

344
00:33:26.070 --> 00:33:28.280
Fan Yunjie: 所以我是想现在让你去。

345
00:33:28.280 --> 00:33:33.060
Norris: 去，maybe你现在是就乌托邦了，就是个学术人。

346
00:33:33.060 --> 00:33:40.520
Fan Yunjie: 对，然后现在你也是有被约束到这种情况，想知道你要怎么解决。如果说

347
00:33:40.640 --> 00:33:43.700
Fan Yunjie: 可能是你认为现在

348
00:33:43.850 --> 00:33:53.810
Fan Yunjie: 你的这个产品也通过一些系统设计去去防护。现在现在就是regulation，就是两个月之后你就要去执行了。

349
00:33:56.040 --> 00:33:57.090
Norris: 好难啊。

350
00:34:00.980 --> 00:34:03.090
Norris: 怎么做呀？我操

351
00:34:04.150 --> 00:34:05.220
Norris: en

352
00:34:08.290 --> 00:34:09.010
Norris: 就是

353
00:34:09.659 --> 00:34:10.810
Norris: en.

354
00:34:11.489 --> 00:34:13.159
Norris: 我能说我想不出来吗？

355
00:34:13.489 --> 00:34:15.030
Norris: 一时间想不出来。

356
00:34:15.239 --> 00:34:17.099
Fan Yunjie: 可以？没关系。

357
00:34:17.099 --> 00:34:19.809
Norris: 真的有点有悖于我正常思考模型。

358
00:34:20.530 --> 00:34:35.909
Fan Yunjie: 可以，没关系，所以你觉得说，就我们通过日常来讲嘛，你的你的同事啊，或者说你的下属，大家跟ai的互动方式大概是怎么样的，或者说是ai参与的程度大概怎么样？

359
00:34:36.330 --> 00:34:41.100
Norris: 如果是在我们这个项目的工作过程中，可以说是每个人都在用ai，

360
00:34:41.440 --> 00:34:47.239
Norris: 然后基本上只不过用的ai的工具不同，比如说那些后端工程师或者是

361
00:34:47.350 --> 00:34:51.650
Norris: 就不说后端工程师吧，前端工程师也好，他们

362
00:34:52.070 --> 00:34:56.440
Norris: 开了一个cursor，然后在cursor里边去管理整个的github

363
00:34:56.710 --> 00:34:57.840
Norris: 的这个，

364
00:34:57.950 --> 00:34:59.530
Norris: 然后这个技术战。

365
00:35:00.870 --> 00:35:08.460
Norris: 一个模块，一个模块的，改一行一行的就是他改一个地方，然后ai就可以帮他把其他的类似的地方都改掉嘛，就很方便。

366
00:35:10.640 --> 00:35:14.210
Fan Yunjie: 你会去认可他们多大程度之上

367
00:35:14.650 --> 00:35:17.450
Fan Yunjie: 去依赖我知道你十分认可。

368
00:35:17.450 --> 00:35:18.500
Norris: 哼。

369
00:35:20.520 --> 00:35:22.380
Fan Yunjie: 你不介意这个问题吗？

370
00:35:22.380 --> 00:35:24.210
Norris: 而且是我要求他们这么做的。

371
00:35:24.460 --> 00:35:33.510
Fan Yunjie: 你会介你也不会介意，比方说你的小朋友，他现在已经95%的工作都是ai来来做了，你也不会去介意这个事情。

372
00:35:33.720 --> 00:35:36.700
Norris: 不会，他只需要report就可以了。

373
00:35:37.120 --> 00:35:38.570
Norris: 我只看结果。

374
00:35:39.430 --> 00:35:46.890
Fan Yunjie: 明白明白，所以你觉得这样怎么去论证一个人本身他的专业能力呢？

375
00:35:47.400 --> 00:35:48.630
Norris: okay,

376
00:35:48.740 --> 00:35:53.640
Norris: 看看看报酬呗，社会赋予他多大的回报。

377
00:35:58.040 --> 00:35:59.640
Fan Yunjie: 你是指什么意思？

378
00:35:59.640 --> 00:36:06.160
Norris: 比如说名望啊，比如说金钱啊，比如说地位啊，政治头衔啊，就是所有的这些东西。

379
00:36:06.160 --> 00:36:11.840
Fan Yunjie: 不是我的意思是比方说，现在我们我们不看那么高维度的人，我们就看candidates

380
00:36:12.040 --> 00:36:16.050
Fan Yunjie: 很多的这些，这些这个候选人，他可能。

381
00:36:16.050 --> 00:36:16.490
Norris: 那啥。

382
00:36:16.490 --> 00:36:31.170
Fan Yunjie: 强的专业知识，但是他处理工作完完全全是不相信ai。我遇到过很多这样的人，他们真的完全不相信ai，特别是搞学术的人，他不相信ai，他说的话，他一定要去自己再去clarify一下。

383
00:36:31.170 --> 00:36:32.450
Norris: 那。

384
00:36:32.720 --> 00:36:34.390
Fan Yunjie: 还有一部分人可能。

385
00:36:34.390 --> 00:36:34.820
Norris: 尽量吧。

386
00:36:34.820 --> 00:36:40.160
Fan Yunjie: 专业知识并没有那么强，但他处理工作却处理到你的结果上了。

387
00:36:40.600 --> 00:36:43.830
Fan Yunjie: 在衡。量candidates方向的时候，你怎么去衡量。

388
00:36:43.830 --> 00:36:50.080
Norris: 对我个人来说就是看，结果还是一样的，他给我deliver的结果好，我就说你能力强。

389
00:36:50.830 --> 00:37:02.870
Norris: 当然你本身这个人不能有毛病，你别说话都说不明白，然后ai给你的结果你也解释不清楚，那是不行的就是你得懂得最终的结果你得解释得清楚，

390
00:37:03.510 --> 00:37:05.190
Norris: 然后这个结果还要好。

391
00:37:08.390 --> 00:37:15.070
Fan Yunjie: 那你对于效率性呢？那肯定这两个人他的效率是不一样的ai的那一个肯定会很快啊，对吧。

392
00:37:15.070 --> 00:37:16.820
Norris: 对呀，那就那就快的呗，

393
00:37:17.400 --> 00:37:20.710
Norris: 你都已经结果好的情况下了，那当然择优录取啊。

394
00:37:21.380 --> 00:37:27.820
Fan Yunjie: 这种你会在你的这个应聘过程当中会去做这方面的吗？

395
00:37:27.980 --> 00:37:34.930
Norris: 没有我，在应聘过程中我就非常简单，就是第一聊一下这个人是不是个正常人。

396
00:37:35.590 --> 00:37:45.010
Norris: 第二就是看一下它的历史都做过什么项目，因为这个时候吧，就是很难做。我们在招聘的时候还没有ai呢。

397
00:37:45.610 --> 00:37:46.260
Fan Yunjie: En.

398
00:37:47.090 --> 00:37:57.550
Norris: 就因为我们这个团队也不是就是为了ai这个项目组的，所以当时大家都只是简简单单的程序员，所以那个时候就看历史项目。

399
00:38:00.160 --> 00:38:03.740
Fan Yunjie: 所以就是完全看这个人的经验，判断能力。

400
00:38:03.920 --> 00:38:07.580
Norris: 对。至于说ai出了之后，我还真没招过什么新人。

401
00:38:07.970 --> 00:38:11.120
Fan Yunjie: 有。哼，

402
00:38:11.430 --> 00:38:15.930
Fan Yunjie: 这是是因为工作流都被aic试掉了吗？还是说。

403
00:38:16.490 --> 00:38:21.040
Norris: 对就是他们学的也挺快的，然后他们这个

404
00:38:21.260 --> 00:38:22.760
Norris: 也挺快的，

405
00:38:23.710 --> 00:38:29.660
Norris: 也不用再招一个特别会ai的人了，我还反而还开了一个会ai的人。

406
00:38:30.070 --> 00:38:31.100
Fan Yunjie: 为什么。

407
00:38:31.910 --> 00:38:34.860
Norris: 我开了这个人，是因为他个人的态度不好，

408
00:38:35.060 --> 00:38:36.800
Norris: 两次早会没来。

409
00:38:38.550 --> 00:38:40.540
Fan Yunjie: 好吧，okay。

410
00:38:40.740 --> 00:38:58.610
Fan Yunjie: 那你觉得说你再去进行ai的这个使用过程当中，我觉得因为你本身是有很强的思考能力的你，为了去让你的ai，就咱们说你训练一条狗更更符合你的调性，跟你训练一个ai，我觉得有异曲同工之处吧？对吧？

411
00:38:58.610 --> 00:39:01.030
Norris: 你在这个过程当中。

412
00:39:01.030 --> 00:39:07.210
Fan Yunjie: 我相信你去让他一轮一轮的校正，也是希望他可以变成你更好的assistant。

413
00:39:07.210 --> 00:39:07.670
Norris: 对。

414
00:39:07.670 --> 00:39:19.750
Fan Yunjie: 你会去让他。比方说，第一，我让你去给我多出几个结果，我去比较选择，或者说我让你给我出一个正好，相反的结果会有这种过程吗？

415
00:39:20.800 --> 00:39:21.660
Norris: 没有

416
00:39:21.780 --> 00:39:30.170
Norris: 没有我，我，我还是那句话，我知道我大概要的结果是怎么样子的，所以我就是看他给我的，是不是符合我的期待的。

417
00:39:32.520 --> 00:39:35.770
Fan Yunjie: 所以你在这个过程当中，你觉得prong engineering

418
00:39:35.950 --> 00:39:39.350
Fan Yunjie: 是会有帮助吗？即使你没有学过。

419
00:39:39.350 --> 00:39:41.180
Norris: 有帮助，但是帮助不大，

420
00:39:41.430 --> 00:39:46.510
Norris: 因为我也看了很多其他人的提示词就是大长串大长串的。

421
00:39:46.960 --> 00:39:56.530
Norris: 你又怎么样呢？你要扮演什么角色呀？你要想识怎么样，其实没有必要这么去prompt，你就把你要做的事情说清楚就可以了。

422
00:39:59.250 --> 00:40:01.890
Fan Yunjie: 所以你觉得在这个过程当中啊，

423
00:40:02.380 --> 00:40:07.990
Fan Yunjie: 你当然是ai开始有了这样子的一个分工跟你的团队，

424
00:40:08.130 --> 00:40:24.400
Fan Yunjie: 那你觉得多大程度之上，你的你的工作领域变成你指定这个事情是人做，而不是说这个事情变成了一个一个一个一个ai可以去分担的事情，或者说哪些岗位你觉得将来就是应该应该

425
00:40:24.530 --> 00:40:26.730
Fan Yunjie: damage掉，就是没没有了。

426
00:40:28.130 --> 00:40:29.180
Norris: 程序员。

427
00:40:31.040 --> 00:40:32.440
Fan Yunjie: 这么直接吗？

428
00:40:32.570 --> 00:40:33.180
Norris: 哼。

429
00:40:34.520 --> 00:40:37.610
Fan Yunjie: 因为我不写代码，所以我不知道你为什么这么negative。

430
00:40:38.280 --> 00:40:44.230
Norris: 不不是connective，就是单纯是因为现在ai在代码能力的进化上是最快的，

431
00:40:45.570 --> 00:40:47.230
Norris: 因为它有最多的数据。

432
00:40:47.790 --> 00:40:50.470
Fan Yunjie: 那这样会有一个很大的问题啊。

433
00:40:50.860 --> 00:40:55.600
Fan Yunjie: 那你最终ai，它背后不也是人去写的程序吗？

434
00:40:55.600 --> 00:40:56.650
Norris: 最开始是的。

435
00:40:59.710 --> 00:41:00.350
Fan Yunjie: 那现在你。

436
00:41:00.350 --> 00:41:04.840
Norris: 他们就变成了比如说十个ai写一个新A就可以自己造自己了。

437
00:41:07.020 --> 00:41:12.370
Fan Yunjie: 那也是仍仍然需要人类在上面去驯化它呀，人类去给他下指令啊。

438
00:41:13.840 --> 00:41:20.920
Norris: 不是训练和推理是两个过程嘛，对吧，咱们说，训练还是说推理

439
00:41:21.230 --> 00:41:22.600
Norris: 推理的话，你需要用这个。

440
00:41:22.600 --> 00:41:27.750
Fan Yunjie: 我这样子觉得我是这样觉得你不论是训练还是推理，都要有experience，

441
00:41:27.870 --> 00:41:33.020
Fan Yunjie: 你不能是不理解，或者说是你对于这个东西没有认知。

442
00:41:33.020 --> 00:41:36.390
Norris: 然后去进行，这个东西就变得。

443
00:41:36.390 --> 00:41:37.610
Fan Yunjie: 空中楼阁。

444
00:41:39.930 --> 00:41:46.830
Norris: 对，是的，空中落格这个词很好，就是ai在空中打转是经常有的事情，自己把自己给说服了。

445
00:41:50.230 --> 00:41:56.190
Fan Yunjie: 但是这样的问题其实会不好啊，因为你长期以来就会变成就像我刚刚说的，

446
00:41:56.230 --> 00:42:00.220
Fan Yunjie: 就是像吸了大麻一样，对于普通用户啊，

447
00:42:00.220 --> 00:42:16.500
Fan Yunjie: 我就是被ai遛着转ai，它的思维混乱，我也不知道他在思维混乱，我就正在享受在他给我的制造的假象当中里面。当然，这些人类也是你所说的可能会被

448
00:42:16.500 --> 00:42:22.450
Fan Yunjie: 适者生存淘汰掉的这些人类，但是很大程度之上，我觉得对于

449
00:42:22.660 --> 00:42:26.880
Fan Yunjie: 人工协就是人机协作来说的话。

450
00:42:27.660 --> 00:42:34.220
Fan Yunjie: 应该还是要有一个training的一个process，去让大家去更好程度上的保留认知吧。

451
00:42:36.970 --> 00:42:42.800
Norris: 有可能这个也是一个过程嘛，你不可能说，突然有一天，所有程序员都被开除了，

452
00:42:43.110 --> 00:43:02.070
Norris: 就算被开除了，这帮人还要回到90多岁，他还会有这个技能就是一直伴随他，然后学校也不可能说，我完全就不教这个编程了，因为你不需要学编程，你像难道人会说话就不教语文了吗？对吧，还是还是在教学的吗？就这个学科肯定还是存在的。

453
00:43:04.420 --> 00:43:11.890
Fan Yunjie: 你本身在对于ai的这个未来来说的话，你是一个非常非常positive的一个状态，是吗？

454
00:43:12.220 --> 00:43:18.590
Norris: 就是我是一个没有办法的positive，因为这个是一个趋势，趋势不可逆，

455
00:43:19.680 --> 00:43:27.580
Norris: 全世界没有像ai这次革命一样花过这么大的全人类的力量去做一件事情。

456
00:43:28.700 --> 00:43:29.410
Fan Yunjie: 可是

457
00:43:30.500 --> 00:43:31.010
Norris: 你说。

458
00:43:31.010 --> 00:43:35.840
Fan Yunjie: 可可是像像你所说的就是ai之前也有搜索引擎啊。

459
00:43:36.030 --> 00:43:37.550
Norris: 哼，是的。

460
00:43:37.550 --> 00:43:43.030
Fan Yunjie: 这个目前也是一个非常大的，革命型的这种事件啊，对吧。

461
00:43:43.860 --> 00:43:53.390
Fan Yunjie: 我觉得ai某种程度上来说的话，其实对于某一些用户，它只是一个集合型的一个搜索引擎，一个非常

462
00:43:54.160 --> 00:43:56.080
Fan Yunjie: ordinary的一个tool。

463
00:44:01.790 --> 00:44:04.580
Norris: 对啊，我一直认为它就是一个工具啊。

464
00:44:06.050 --> 00:44:13.400
Fan Yunjie: 但是目前但是未来的趋势，你是认为ai是跟人类会有很强烈的协作，还是说它仍然是这个脱欧。

465
00:44:13.590 --> 00:44:15.000
Norris: 他是一直都是个吐，

466
00:44:16.900 --> 00:44:19.000
Norris: 他永远的是辅助角色。

467
00:44:19.930 --> 00:44:28.260
Fan Yunjie: 你也不会去在tms这个角度上去想到将来ai跟人类的真正意义上的场景化分工。

468
00:44:29.290 --> 00:44:42.320
Norris: 场景化分工是顺其自然发生的。如果有一天扫大街的都是机器人了，那前提一定是社会福利综合性的提高，并不需要扫大街这个岗位了，而不是机器人能扫大街。

469
00:44:42.830 --> 00:44:48.010
Norris: 就算每一天程序员都消失了，不是因为他们不会编程了，是他们不需要编程了。

470
00:44:51.780 --> 00:44:55.510
Norris: 一定是基于整体社会福利去思考这个社会构架问题。

471
00:44:57.080 --> 00:45:07.380
Fan Yunjie: 但是你现在所有的这些产品定位，你都是把它当成一个图，你从来也没有想说我要把它变成personal assistant，但是你曾经在底层的时候，你就是这样思考的呀。

472
00:45:08.280 --> 00:45:12.900
Norris: 不他是personal assistant，但他就难道他就不是个two吗？

473
00:45:13.690 --> 00:45:20.880
Fan Yunjie: 的一个工具跟一个personalism。我觉得他对于一个人的印象还是不太一样的。

474
00:45:21.130 --> 00:45:25.700
Norris: 这的确是嘛，就是我不可能说我就不要人类助理了，

475
00:45:25.820 --> 00:45:30.480
Norris: 我不要人类助理了，一定是因为我秘密太多了，我需要一个ai来帮我保守秘密。

476
00:45:32.120 --> 00:45:34.800
Fan Yunjie: 那倒也不是从通过这个

477
00:45:35.620 --> 00:45:47.280
Fan Yunjie: 本身一些一些这个数据安全或者其他问题上，我觉得其实大部分人都不care这个问题就真正意义上自己的这个数据安全是不是就是在。

478
00:45:47.280 --> 00:45:47.610
Norris: 还可以。

479
00:45:47.610 --> 00:46:05.520
Fan Yunjie: 当中可以完全的就有一个好的保护，甚至对于伦理性的问题，他们也不是很在乎，他们只是在乎这个东西好不好用，有多大情况的就是有效性，我觉得这个是大部分去去去，可能更大程度上跟ai协作的一个主要原因。

480
00:46:05.640 --> 00:46:22.860
Fan Yunjie: 但是我想要就是更大程度上，在这里的产品上，我想要知道你们的方向，就仍然会把它想要定位成未来人类的使用的工具，还是说人机在未来真的程度上可以变成这种劳动分配，

481
00:46:22.860 --> 00:46:28.860
Fan Yunjie: 然后很大程度之上变成了一个团队，里面，他也变成了一种女人化personalized。

482
00:46:30.140 --> 00:46:33.050
Norris: 我们没有想过personalize这个事情。

483
00:46:35.500 --> 00:46:37.250
Fan Yunjie: 这不是好事情吗。

484
00:46:37.600 --> 00:46:38.750
Norris: 没想过而已，

485
00:46:39.070 --> 00:46:41.660
Norris: 好不好的没想过呀。

486
00:46:42.900 --> 00:46:54.410
Fan Yunjie: 因为你是处理这种这种比较底层的工作，但是很多比方其实很多market你知道吧，就做事销的人。

487
00:46:54.410 --> 00:47:00.210
Norris: 他们很依赖ai啊，去去写一些这个copy post呀，去输出一些poster啊。

488
00:47:00.460 --> 00:47:01.300
Norris: 对。

489
00:47:01.390 --> 00:47:17.600
Fan Yunjie: 这种的话，就很需要这种匿人化，然后你会认为就是比方说artist或者说是creator这这一类的工作人员，他们将来会面临着比较大的这个就业的问题吗？或者说是会被替代吗？

490
00:47:21.870 --> 00:47:25.350
Norris: 哎，这个他们这个好问题啊，因为

491
00:47:26.620 --> 00:47:34.860
Norris: 首先你得看他写这个东西是为了干什么？他是为了su嘛？为了blog嘛为了宣传这个产品嘛？为了霍克嘛，就是

492
00:47:35.180 --> 00:47:38.830
Norris: 它有个目的嘛。假设说未来

493
00:47:38.960 --> 00:47:43.840
Norris: 这个所有的数据，ai都能搜索到，是不是就不需要seo了。

494
00:47:44.890 --> 00:47:48.010
Norris: 那么假如说你的产品足够好，

495
00:47:48.190 --> 00:47:51.840
Norris: 那么是不是ai也可以搜索到？即使你并没有做过生产有效。

496
00:47:53.870 --> 00:47:58.140
Fan Yunjie: 但你通过一个branding的角度上来说，

497
00:47:58.320 --> 00:48:02.380
Fan Yunjie: 这样的东西它也没有办法帮你去品牌化呀。

498
00:48:02.680 --> 00:48:08.670
Norris: 是的，这样品牌这个东西跟ai只能是ai辅助于品牌建设。

499
00:48:09.370 --> 00:48:15.200
Fan Yunjie: 当然，那你就需要人类去在后边去，怎么样去处理了。

500
00:48:15.360 --> 00:48:27.630
Norris: 对的就是需要人类去做这个风格化的筛选，就是说ai给我出了80个版本，我就喜欢这个版本。Ai说我推荐另外这个我不，我就喜欢这个，对吧？人类拍了板说了算的。

501
00:48:28.830 --> 00:48:40.100
Fan Yunjie: 人类确实是有这个判断能力，但是它也解决不了，可能80个都是在同质化的角度上产出的结果呀，然后本身的创意啊，包括很多的，

502
00:48:40.620 --> 00:48:46.410
Fan Yunjie: 我们说人性，或者说这个东西有人味的问题，就我觉得慢慢就就已经我也。

503
00:48:46.410 --> 00:48:50.360
Norris: 也是我现在刷短视频就是看到那种ai上的视频，我直接跳过的。

504
00:48:52.910 --> 00:49:02.640
Fan Yunjie: 你会觉得说，所以你会觉得说这种创意型的岗位或者创意型的，这种输出的这些职能还是永远不会被替代的，对吗？

505
00:49:02.810 --> 00:49:08.400
Norris: 我认为创意不能说永远吧，很长一段时间是很难被替代的，

506
00:49:08.610 --> 00:49:10.880
Norris: 但是ai早晚会起来的。

507
00:49:13.130 --> 00:49:21.590
Fan Yunjie: 为什么，它怎么会能替代到人类本身的创意想法，或者说它本身的这种调性，这种调性这个东西我就很难替代啊。

508
00:49:21.730 --> 00:49:25.610
Norris: 药性，它也是有个总数的吧，你就算有68,000多种

509
00:49:25.810 --> 00:49:29.430
Norris: 调性，你最终也是会是一个数字嘛。

510
00:49:29.720 --> 00:49:33.920
Norris: 那ai永远有数字的边缘可以被触碰到的。

511
00:49:34.700 --> 00:49:38.070
Fan Yunjie: 那这很有趣。我想问你平常看电影吗？

512
00:49:39.710 --> 00:49:51.700
Fan Yunjie: 你每一个电影的这个导演，他的风格都不同，你觉得ai可以通过学习之后就是可以拍出跟这个导演的调性，一模一样的产作品，或者说同调性的作品出来，是吗？

513
00:49:51.850 --> 00:49:52.570
Norris: 可以的，

514
00:49:52.780 --> 00:49:57.010
Norris: 你要是说他能不能拍出不同调性的，我要思考一下，拍同调性绝对没问题。

515
00:49:59.390 --> 00:50:11.020
Fan Yunjie: 那这样很可怕耶对于我来讲很可怕，我还是希望可以吃一点创意的饭呢？虽然我也是没靠这个赚到钱。

516
00:50:11.310 --> 00:50:19.030
Norris: 我这个我感觉啊，咱们的有生之年，在我们的这个硬件还能支撑的，这么几十年里面，

517
00:50:19.190 --> 00:50:20.860
Norris: 你不用担心这个问题，

518
00:50:21.910 --> 00:50:23.030
Norris: 数据不够，

519
00:50:23.150 --> 00:50:25.420
Norris: ai也是要量变导致质变的。

520
00:50:26.730 --> 00:50:35.730
Fan Yunjie: 可是现在的这个educational的这个level其实很大的一个问题，我在于现在小朋友很多学习的东西，他将来会完全没有用。

521
00:50:38.030 --> 00:50:47.850
Norris: 但是他得知道吧，就是好像你说为什么水在往下流，他没有飘到天上去，他得知道是因为地球有引力吧。

522
00:50:49.650 --> 00:50:52.530
Norris: 就是如果这个都不知道，那他也没法在社会生存呀。

523
00:50:54.060 --> 00:50:58.670
Fan Yunjie: 你，你如你现在是一个单身无阿的状态，对吗？

524
00:50:58.670 --> 00:50:59.290
Norris: 是的。

525
00:50:59.880 --> 00:51:06.800
Fan Yunjie: 你将来如果有小孩的话，现在面临着就是选方向。选专业的角度，你会有什么样的建议给他。

526
00:51:07.140 --> 00:51:09.690
Norris: 我会有什么建议给他呀，我想想啊，

527
00:51:10.100 --> 00:51:11.850
Norris: 假如说你是我的小孩，

528
00:51:11.970 --> 00:51:15.120
Norris: 然后，现在你要上大学了，要选个专业。

529
00:51:16.070 --> 00:51:22.910
Norris: 我会说，我会观察你过去的18年去看你最喜欢什么，做什么最开心。

530
00:51:24.980 --> 00:51:28.710
Norris: 你如果喜欢玩，我就看你什么东西玩的最好。

531
00:51:29.990 --> 00:51:33.550
Fan Yunjie: 但他玩的东西他也不一定可以去支持他的生活呀。

532
00:51:33.550 --> 00:51:36.670
Norris: 生活不是问题，都是我的小孩怎么会有生活问题呢？

533
00:51:37.200 --> 00:51:40.490
Fan Yunjie: 开玩笑话下一个话题，

534
00:51:40.490 --> 00:51:48.730
Norris: 开个玩笑。如果说是这么说啊，就是ai如果已经把这个服务领域都充斥掉了，

535
00:51:48.800 --> 00:52:07.380
Norris: 因为ai肯定是先上来服务于人来作为工具辅助角色嘛，那么肯定会给社会创造巨大的剩余价值，这些剩余价值一定会通过一个比较平稳的方式传导给普罗大众。虽然99%还是被截留在顶层社会。

536
00:52:08.820 --> 00:52:12.050
Fan Yunjie: 你觉得呢？我想听一下你的意见，就是。

537
00:52:12.050 --> 00:52:13.100
Norris: 所以成。

538
00:52:13.100 --> 00:52:13.480
Fan Yunjie: Left.

539
00:52:13.480 --> 00:52:15.380
Norris: 也会非常的乌托邦。

540
00:52:17.670 --> 00:52:25.500
Fan Yunjie: 你可以给我几大类吗？你可以给我几大类吗？大概是你认为无法被被替代的。那些遗留的。

541
00:52:25.810 --> 00:52:26.880
Norris: 一流的专业吗？

542
00:52:26.880 --> 00:52:27.540
Fan Yunjie: 对啊。

543
00:52:29.200 --> 00:52:30.590
Norris: 前沿物理。

544
00:52:34.170 --> 00:52:38.090
Norris: 前沿物理就包括了理论和那个实验的话，就。

545
00:52:38.090 --> 00:52:38.610
Fan Yunjie: 然后。

546
00:52:39.350 --> 00:52:40.400
Norris: En

547
00:52:41.370 --> 00:52:42.560
Norris: 体育运动，

548
00:52:44.640 --> 00:52:45.610
Norris: 然后

549
00:52:47.990 --> 00:52:49.990
Norris: 娱乐吧，娱乐，艺术。

550
00:52:54.280 --> 00:52:56.300
Norris: 反正就是anything cues time。

551
00:52:58.020 --> 00:52:59.020
Fan Yunjie: 明白，

552
00:53:00.150 --> 00:53:13.620
Fan Yunjie: 所以本身的话你，你认为如今的这个这个社会上来讲，因为我我认为不论是人际啊，人跟人也是我觉得现在这个社会已经发展到了一个

553
00:53:13.920 --> 00:53:17.530
Fan Yunjie: 你如果想长线发展一个关系的话，其实

554
00:53:17.800 --> 00:53:27.890
Fan Yunjie: 我觉得最最核心，或者说最重要就是坦诚，就是比聪明啊，比什么都重要。如果说你因为长线的关系，

555
00:53:28.050 --> 00:53:33.840
Fan Yunjie: 那你觉得通过你自己的产品，你怎么去跟用户建立这种信任关系？

556
00:53:34.700 --> 00:53:37.000
Norris: 就是怎么让用户感觉，

557
00:53:37.550 --> 00:53:40.790
Norris: 但不是，但是你这个只是一方面呀，

558
00:53:41.160 --> 00:53:47.460
Norris: 就是坦诚，只是你打动用户的这方面，但如果我对你特别好，掏心掏肺，

559
00:53:47.780 --> 00:53:50.790
Norris: 但是我又没钱又丑，你也不会喜欢我啊，

560
00:53:54.460 --> 00:53:58.060
Norris: 首先你得产品好才行，其次才是真诚。

561
00:53:59.340 --> 00:54:09.070
Fan Yunjie: 那信任都很是是一个很重要的点啊，你当然产品好是每一个产品，我相信你们技术出身的这些，这些大佬们

562
00:54:09.150 --> 00:54:22.670
Fan Yunjie: 首先priority的事情是把自己的产品打磨的够漂亮，然后才去有信心让他去面对普罗大众这个事情，我把我把它放到第一个我不去讲，但是信任这个关系，你怎么去跟用户建立。

563
00:54:24.220 --> 00:54:29.810
Norris: 不是我就要把产品做得多好的问题是用户接不接受我的产品

564
00:54:30.010 --> 00:54:32.880
Norris: 就是我的理念，是不是跟用户能match

565
00:54:34.120 --> 00:54:44.580
Norris: 对吧？然后至于说如果卖去了之后他会不会。喜欢我这个产品，他又见不到我的人，我有什么好，跟他真诚的。

566
00:54:44.750 --> 00:54:49.310
Fan Yunjie: 没有我的意思，就是现在把比方说我们现在把clarity。

567
00:54:50.030 --> 00:54:52.000
Fan Yunjie: 把它变成了一个拟人化。

568
00:54:52.040 --> 00:54:57.000
Norris: Ok，我们现在就是做marketing给他出去，让他跟audience去建立connection。

569
00:54:57.940 --> 00:55:00.200
Fan Yunjie: 他的这个信任机制是怎么形成的？

570
00:55:00.840 --> 00:55:12.850
Norris: 你做了一个我不会去做的假设哈，首先我，我会回答你这个问题，但是我先澄清一点，就是我们不会去给搞一张脸出来。

571
00:55:13.000 --> 00:55:15.270
Fan Yunjie: 当然，当然，当然我只是这样。

572
00:55:15.270 --> 00:55:15.830
Norris: 太吓人了。

573
00:55:15.830 --> 00:55:18.100
Fan Yunjie: 对，当然不会这样啦。

574
00:55:18.100 --> 00:55:20.340
Norris: So creepy就是

575
00:55:21.250 --> 00:55:23.450
Norris: 如果

576
00:55:23.820 --> 00:55:30.920
Norris: 建立这个形象就是这么说吧，你首先要信任的是一个你已经认知的物种，

577
00:55:31.150 --> 00:55:37.800
Norris: 你不可能信任一个你不知道是什么东西的东西，你不可能信任一团烟，你可能会信任皮卡丘，

578
00:55:39.640 --> 00:55:41.990
Norris: 所以你必须要有一个实体形象，

579
00:55:42.490 --> 00:55:45.120
Norris: 这个实体形象可以是一个二维的动画，

580
00:55:45.750 --> 00:55:47.780
Norris: 也可以真的就是一张脸。

581
00:55:50.250 --> 00:56:09.890
Fan Yunjie: 我们说的东西现在有点太抽象了，我们现在就是要说本身对于产品来讲，你比方说比方说它是不是要去处理模糊性，他是不是要跟用户解释它的推理，它是不是要要就是比方说这一类对。

582
00:56:10.290 --> 00:56:13.810
Norris: Okay，首先，他不需要跟用户去解释，推理，

583
00:56:14.250 --> 00:56:17.250
Norris: 他，直接给用户答案就好了，用户要的是便捷性。

584
00:56:18.840 --> 00:56:34.770
Fan Yunjie: 但这样子伴随著ai的这些产品层出不迭。这样出现，你觉不觉得也许或许啊，跟用户解释，推理会是一个更好程度上打消用户疑虑的，一个一个好的一个切入点。

585
00:56:35.000 --> 00:56:36.430
Norris: 用户没那么多想法，

586
00:56:36.610 --> 00:56:47.950
Norris: 用户不会去看你的社交软件的mac算法是怎么写的，或者是机智讲解，他也不想听他要看的是youtube视频打开我就想看我最想最想看的频道，

587
00:56:48.600 --> 00:56:52.710
Norris: 他看的是抖音的推荐算法给我最喜欢的视频博主。

588
00:56:53.560 --> 00:56:57.110
Fan Yunjie: 你更大程度上认为大家还是

589
00:56:58.060 --> 00:57:07.710
Fan Yunjie: 在依赖ai，它变成一种一种拐棍，一种认知型的拐棍，而不是说去靠它去锻炼我们的思维，对吗？

590
00:57:07.710 --> 00:57:11.650
Norris: 没有人想成长，所有人都想留一辈子当一个baby。

591
00:57:14.330 --> 00:57:16.130
Fan Yunjie: 但也这也是问题啊，

592
00:57:16.290 --> 00:57:29.030
Fan Yunjie: 就像你所说的，你希望你的员工再去用ai处理出来数据，他可以知道这个数据本身的这个详细的意义，他可以说出来，123，四五万。

593
00:57:29.430 --> 00:57:43.050
Norris: 是的，我当然希望他是这个样子的，但是这是我的筛选标准，你来我公司上班，或者是咱们合作，那咱们要有一个基础嘛，但是你不能要求全社会的人都有这个基础呀。

594
00:57:44.430 --> 00:57:45.200
Fan Yunjie: 那我们就是。

595
00:57:45.200 --> 00:57:51.370
Norris: 大部分人，或者是我们把社会改造成这个方向。我跟你说，成年人了，不做改变，只做筛选。

596
00:57:52.070 --> 00:57:59.570
Fan Yunjie: 当然，但是我们还是要去做一些这种可以去批判性的

597
00:58:00.230 --> 00:58:02.330
Fan Yunjie: 这种动作吧，对吧，我觉得。

598
00:58:02.330 --> 00:58:03.350
Norris: 看性的。

599
00:58:06.400 --> 00:58:12.950
Norris: 哎呀，这个就跟那个那个那个alia一样嘛，就是说super lines

600
00:58:13.380 --> 00:58:14.790
Norris: 对吧，超级队齐。

601
00:58:16.660 --> 00:58:19.050
Fan Yunjie: 超级队齐对袭的是什么

602
00:58:20.230 --> 00:58:29.320
Fan Yunjie: 你这样子的话，我觉得你就要去去去挑战自己了，你就要挑战自己当初的假设，然后你还要去考虑你的一些

603
00:58:29.320 --> 00:58:48.550
Fan Yunjie: 反方的论点，才能真正意义上构成一种批判性思维的一种，很很具象的一种，一种一种轮廓在，但是不要说那些有没有悲哀淘汰掉的人了，我觉得可能2%的人，他们都没有这样子批判性思维这样强的能力。

604
00:58:48.550 --> 00:58:53.770
Norris: 你说的太对了，人本身就是一个概率。的

605
00:58:53.990 --> 00:58:58.930
Norris: 你现在在新加坡生活。我在美国生活都是一个概率，

606
00:58:59.180 --> 00:59:00.550
Norris: 这些概率

607
00:59:00.690 --> 00:59:03.380
Norris: 是过去的一系列的巧合导致的

608
00:59:04.420 --> 00:59:05.460
Norris: 就是

609
00:59:07.010 --> 00:59:14.330
Norris: 就是他们有没有批判性思维，他们所在的那个位置并不重要，有可能一个贫民窟的百万富翁，他最有批判性思维，

610
00:59:14.890 --> 00:59:17.510
Norris: 但是他可能连用ai的机会都没有。

611
00:59:19.500 --> 00:59:28.230
Fan Yunjie: 然后你这个，你这个点很有趣，就是你认为即使全人类，他都失去了批判性思维，但是这不是一个问题。

612
00:59:30.010 --> 00:59:34.370
Norris: 这本身这个思维都是人类给定义的，人类，

613
00:59:34.610 --> 00:59:37.340
Norris: 对吧？也没有那么清晰的对。

614
00:59:38.520 --> 00:59:46.350
Fan Yunjie: Okay，这个很有趣，所以说，你觉得你觉得就是人类的最大不可替代性还是在哪里？

615
00:59:46.980 --> 00:59:48.860
Norris: 人类最大的不可替代性

616
00:59:50.060 --> 00:59:51.130
Norris: en

617
00:59:51.800 --> 00:59:55.970
Norris: 人类的不可替代性。人类存在就是个巧合，没什么不可替代的。

618
00:59:57.840 --> 01:00:03.950
Fan Yunjie: 哇，你这个人我不行，我觉得人类还是完全完全。

619
01:00:03.950 --> 01:00:15.490
Norris: 保持人类文明的火种，但是实际上人类就只是一种巧合呀，你说自然进化论，它是个巧合或者是可造论，它是个神创造的一些有机体对，

620
01:00:15.730 --> 01:00:17.650
Norris: 或者，我们就是一堆数据，

621
01:00:17.900 --> 01:00:20.270
Norris: 这个真的重要吗？

622
01:00:22.630 --> 01:00:28.130
Fan Yunjie: 如果这么抽象来说的话当然不重要，那世界都可以爆炸的对吧？

623
01:00:28.130 --> 01:00:28.870
Norris: 还可以。

624
01:00:29.660 --> 01:00:35.410
Fan Yunjie: 那那你如果说再去具象的说的话，你我，你觉得不可替代吗？

625
01:00:35.770 --> 01:00:37.080
Norris: 非常可替代。

626
01:00:39.670 --> 01:00:46.160
Fan Yunjie: 你是通过这种你本身你的程序员想法来说的，但是你如果感性一点来说呢。

627
01:00:46.310 --> 01:00:50.920
Norris: 不是你这么说吧，没有人想死你让我现在去跳楼，我不想跳，

628
01:00:52.370 --> 01:00:53.120
Norris: 对吧，这个。

629
01:00:53.940 --> 01:01:01.790
Norris: 这个是一个最基础的就是人活著，就是为了生存基因，活著是为了生存，所有人都是为了生存。

630
01:01:04.350 --> 01:01:07.150
Fan Yunjie: 对呀，外形思维并不是生存的一部分呀。

631
01:01:08.290 --> 01:01:19.810
Fan Yunjie: 确实，所以我们现在来说的话，你认为人类它是本身可替代的这个问题，我觉得它其实是值得被拜托的比方说，我现在在跟你聊天，我觉得很开心。

632
01:01:19.810 --> 01:01:21.440
Norris: 你也不会是。

633
01:01:21.610 --> 01:01:26.280
Fan Yunjie: 可替代的呀。我可能这一个礼拜，我跟谁聊天，都没有跟你聊天，开心。

634
01:01:26.280 --> 01:01:26.860
Norris: 对。

635
01:01:29.720 --> 01:01:30.620
Fan Yunjie: 是不是？是？

636
01:01:30.620 --> 01:01:32.760
Norris: 这个我没有空跟你说，呵。

637
01:01:32.760 --> 01:01:36.580
Fan Yunjie: 没有我在最喜欢别人跟我唱反调，真的。

638
01:01:36.580 --> 01:01:38.270
Norris: Okay. Good.

639
01:01:38.270 --> 01:01:41.430
Fan Yunjie: 只是谈恋爱，不要跟我唱反调就可以了。

640
01:01:41.780 --> 01:01:42.510
Norris: 啊啊。

641
01:01:43.510 --> 01:01:48.110
Fan Yunjie: Okay。所以其实你是认为首先，第一个

642
01:01:48.320 --> 01:02:03.260
Fan Yunjie: 人类本身它也它就不是一个完全不可替代的生物，即使将来我们说的什么？Ai与人类道德伦理战真的开始了，人类可能变成了一个弱势，你也是在心里welcome这个状态的，对吗？

643
01:02:05.630 --> 01:02:13.960
Norris: 我不敢说我welcome这个状态，就是说，我如果作为一个个体来说，没有想被统治的欲望。

644
01:02:16.780 --> 01:02:18.840
Fan Yunjie: 对呀，当然了。

645
01:02:18.840 --> 01:02:19.660
Norris: 二，至少。

646
01:02:20.600 --> 01:02:27.360
Fan Yunjie: 当然了，这个就是我们觉得我们现在讨论这个课题的存在的很大的意义吧？对吧？

647
01:02:28.500 --> 01:02:29.580
Norris: En

648
01:02:29.860 --> 01:02:38.920
Norris: 对你我这么说吧，这个你们现在的这个thesis非常有意义，就是你能尽量延缓人类被灭绝的时间。

649
01:02:41.240 --> 01:02:43.950
Fan Yunjie: 你是不是延缓了这个词。

650
01:02:43.950 --> 01:02:50.470
Norris: 对，但是从事实上来讲就是这个人类被灭绝有好多好多好多种可能性，

651
01:02:50.850 --> 01:02:54.650
Norris: 但是最可能的反而是小行星坐地球这种

652
01:02:54.770 --> 01:02:58.570
Norris: 我们完全不是通过地球上做的努力，能改变的事情。

653
01:03:00.800 --> 01:03:02.960
Fan Yunjie: 但是你会觉得说，

654
01:03:03.970 --> 01:03:11.920
Fan Yunjie: 只要人类不master ai，只要人类不去，不去限制它的发展，它就

655
01:03:12.040 --> 01:03:18.270
Fan Yunjie: 他就真的不会真正意义上的去去去去存在这种很大程度上的

656
01:03:18.400 --> 01:03:21.150
Fan Yunjie: 争议性的结果，对吗？

657
01:03:25.120 --> 01:03:27.940
Norris: 如果人类不master的话

658
01:03:28.400 --> 01:03:32.080
Norris: 还是master人类啊？没没太搞搞清楚这个逻辑。

659
01:03:32.600 --> 01:03:33.760
Fan Yunjie: 就是说

660
01:03:34.090 --> 01:03:41.870
Fan Yunjie: 现在我们都要谈论的是ai发展，那我觉得很大程度上是限制ai发展，对吧？怎么样去

661
01:03:42.050 --> 01:03:45.090
Fan Yunjie: 操控他，怎么样的去他。

662
01:03:45.660 --> 01:03:51.740
Fan Yunjie: 如果说我们现在就是把这个问题就抛开，不看我们去完全让它发展，

663
01:03:52.410 --> 01:03:55.470
Fan Yunjie: 你觉得也不会存在最终的那个问题，对吗？

664
01:03:56.780 --> 01:03:58.280
Norris: 完全让它发展，

665
01:03:58.610 --> 01:04:07.980
Norris: 完全让他发展，就是我们给他足够的算力，然后足够的资料，然后他就可以发展成超级人工智能。

666
01:04:08.210 --> 01:04:12.430
Fan Yunjie: 就是也不不不不去care什么道德伦理学这些全部都不care。

667
01:04:12.430 --> 01:04:16.220
Norris: Okay，okay。然后你想说它会发展成什么样子。

668
01:04:16.830 --> 01:04:18.810
Fan Yunjie: 就我们最最去

669
01:04:19.210 --> 01:04:23.790
Fan Yunjie: 担心的普罗大众最会担心的人类被ip那种事情。

670
01:04:25.310 --> 01:04:26.060
Norris: Ok

671
01:04:26.460 --> 01:04:27.640
Norris: 会被替代啊

672
01:04:29.730 --> 01:04:31.640
Norris: 肯定啊？因为

673
01:04:33.970 --> 01:04:39.100
Norris: 超级人工智能就像人类治愈蚂蚁嘛，你不在乎一个医学崩不崩溃，

674
01:04:39.230 --> 01:04:43.110
Norris: 你也不在乎人类生不生存，你不会有意的去毁掉一个医学。

675
01:04:43.420 --> 01:04:47.780
Norris: 但是如果他真的打了你的路，你交一罐立星上去铺个了路，

676
01:04:48.060 --> 01:04:49.550
Norris: 也是很正常的事情。

677
01:04:54.480 --> 01:04:56.750
Fan Yunjie: 你是觉得这种你可以接受吗？

678
01:04:57.180 --> 01:05:00.920
Norris: 我不接受这个世界上没有什么我的意愿。

679
01:05:04.080 --> 01:05:11.890
Fan Yunjie: 但是这就是变成比较相悖的了。像像学术者为什么要出去做这个问题，就是因为他不接受啊，他要去。

680
01:05:12.740 --> 01:05:15.470
Norris: 对啊，那他去努力嘛，那他去fight嘛。

681
01:05:16.600 --> 01:05:28.940
Fan Yunjie: 所以你觉得本身ai的存在，它是为了去节省一些生产力的指标，还是说它本身是辅助人类去去有一个更好的一个输出，对。

682
01:05:29.350 --> 01:05:37.160
Norris: 应该boss吧，就是首先，社会生产力总就是在下降嘛，不然也不会出现经济萎缩的问题，

683
01:05:37.970 --> 01:05:47.690
Norris: 然后人口也在减少，所以需求在减少，需求在减少，所以市场在萎缩，市场在萎缩，所以生产在萎缩，就是整体都在下行嘛。

684
01:05:48.110 --> 01:05:54.730
Norris: 那你说需要什么样的资源来输入呢？需要外部资源，一个

685
01:05:55.070 --> 01:06:05.310
Norris: 一个内部系统，它的商混乱是由于只有内部系统能量消耗造成的，它唯一的解决办法就是输入外部能量，那外部能量来自于哪里？

686
01:06:05.470 --> 01:06:10.450
Norris: 来自于更多的人口，或者来自于外太空的资源。

687
01:06:11.510 --> 01:06:16.020
Norris: 那么这两个都导致说，我们必须要由aa来

688
01:06:16.150 --> 01:06:18.140
Norris: 就是帮助人类吧。

689
01:06:18.530 --> 01:06:22.580
Fan Yunjie: 人类发展ai肯定是希望能解放生产力的。

690
01:06:22.750 --> 01:06:27.070
Norris: 就像把这个妇女平权也是为了解放生产力是一样的。

691
01:06:29.310 --> 01:06:38.360
Fan Yunjie: 那有，那那就存在问题了呀，那又存在真正意义上让ai变成更好的ai，人类变成更好的人类的这个问题了呀。

692
01:06:38.570 --> 01:06:44.100
Norris: Ai会变成更好的ai，但是一旦这个轮子已经启动了，它是没有刹车的，

693
01:06:45.450 --> 01:06:47.370
Norris: 而且你人加不上刹车。

694
01:06:48.290 --> 01:06:50.620
Fan Yunjie: 我想知道这个壁垒在哪里，真正的壁垒。

695
01:06:50.620 --> 01:06:58.640
Norris: 人类加不上砂石的壁垒在于它的底层逻辑。Ai有两种算法，一个是diffusion模型，一个是transformer模型对吧？对。

696
01:06:59.350 --> 01:07:02.750
Norris: 你可以理解为你现在有一盘军司，

697
01:07:03.100 --> 01:07:09.230
Norris: 然后你在A点放了一个食物，他一定会找到最短的路径，把这个军思蔓延到那个a，那个地方去。

698
01:07:11.130 --> 01:07:16.510
Norris: 这个就是。这个defeat模型就是它先假设所有的路径，然后再收拢。

699
01:07:17.980 --> 01:07:26.470
Norris: 然后呢？Transformer模型是说我不断的预测下一个字节是什么，只有足够的电力，我就必然能预测出最优解的概率。

700
01:07:28.860 --> 01:07:33.340
Norris: Okay，那么这两种模型你看得到有任何的

701
01:07:33.530 --> 01:07:35.160
Norris: 拐弯的可能吗？

702
01:07:35.980 --> 01:07:39.250
Norris: 就是如果能量充足的情况下，他是不会拐弯的。

703
01:07:42.040 --> 01:07:45.100
Fan Yunjie: 能量充足就是一个先决条件啊。

704
01:07:45.540 --> 01:07:47.390
Norris: 能量充足是必然的呀。

705
01:07:48.830 --> 01:07:55.520
Fan Yunjie: 怎么说能让政治变？那现在本身你要说顺利的消耗现在已经也是很大的一个问题啊。

706
01:07:55.520 --> 01:08:05.410
Norris: 是的，但是你单从这个晶圆厂或者是需要产能的这个角度来说，能量其实是无限的。由于太阳能的供给对。

707
01:08:06.940 --> 01:08:15.720
Fan Yunjie: 你如果说一定要这样子说的话，那大家肯定会认为它是无限的，但是人类会相信别人告诉它的呀。

708
01:08:16.420 --> 01:08:20.090
Norris: 没没没听懂这个怎么跟相信又扯上关系了。

709
01:08:20.670 --> 01:08:32.870
Fan Yunjie: I mean。你现在告诉所有人现在是这个地球面临了能源危机，大家就会相信他有能源危机，其实他的能量他是很荣誉的。

710
01:08:32.870 --> 01:08:35.250
Norris: 对吧？所以你看人多傻，要人干嘛。

711
01:08:38.000 --> 01:08:41.580
Fan Yunjie: Ok，以后就我们两个，这种存在在世界上就可以了。是不是。

712
01:08:41.580 --> 01:08:46.620
Norris: 太聪明不好，太聪明，活累。

713
01:08:46.620 --> 01:08:51.060
Fan Yunjie: 我不聪明，我这个人很笨，我这个人的脑子都不拐，弯我跟原来一样的

714
01:08:51.520 --> 01:08:52.170
Fan Yunjie: 对。

715
01:08:52.170 --> 01:08:56.830
Norris: 你看你还不聪明？So so，so good，so great pong。

716
01:08:57.450 --> 01:09:05.560
Fan Yunjie: 你你看我的那个网名都是那个oic。所有人问我，那个osc是什么意思？我说我就是个operation systems。

717
01:09:05.609 --> 01:09:08.410
Norris: 这个意思吗？我以为是oversee。

718
01:09:08.880 --> 01:09:09.649
Fan Yunjie: 对，对，

719
01:09:09.859 --> 01:09:12.290
Fan Yunjie: all standings.

720
01:09:12.960 --> 01:09:14.210
Fan Yunjie: 哈哈哈。

721
01:09:14.580 --> 01:09:15.630
Norris: Outstanding.

722
01:09:15.630 --> 01:09:24.540
Fan Yunjie: Okay，anyway。我觉得我觉得你的，你的想法都很有趣，我真的觉得跟你聊天挺好有意思的。

723
01:09:24.720 --> 01:09:26.810
Norris: 好吧，是不是？

724
01:09:26.810 --> 01:09:33.250
Fan Yunjie: 多聊吧，多聊吧。没想到这样子，跟跟你聊天，我就先把它stop recording一下吧。

725
01:09:34.590 --> 01:09:36.220
Norris: 我都忘了还在record。

726
01:09:36.760 --> 01:09:41.790
Fan Yunjie: 对啊，你，你都回头回头，我都要给你打码，说了好几次脏话。

727
01:09:41.979 --> 01:09:43.579
Norris: 是吗？这个还在乎吗？


受访人30:
WEBVTT

1
00:00:01.280 --> 00:00:04.570
Fan Yunjie: Recall开始，我们可以把摄像头先关掉了。

2
00:00:04.790 --> 00:00:05.440
Vincent ZOU: 好的。

3
00:00:10.160 --> 00:00:11.430
Fan Yunjie: ok，您可以先。

4
00:00:11.430 --> 00:00:11.750
Vincent ZOU: 哎呀。

5
00:00:11.750 --> 00:00:13.210
Fan Yunjie: 稍微介绍一下。

6
00:00:13.950 --> 00:00:18.820
Vincent ZOU: 对，然后我们就是三个合伙人都是从阿里出来的，其中

7
00:00:18.920 --> 00:00:38.650
Vincent ZOU: 有一个呢？它就是算法的就是以前我们叫阿里巴巴算法，然后包括餐饮喜欢，然后包括像这种应用类工具的算法的一方面，然后是我们的cto，所以我们公司的全称叫smarty metrics，直接矩阵。然后我们核心现在在主营的业务呢？基本上是以这个。

8
00:00:38.950 --> 00:00:47.790
Vincent ZOU: 电商，然后电商的消费品在电商的运营以及包括在全域。像类似于抖音啊，天猫啊，小红书啊，

9
00:00:47.970 --> 00:00:54.230
Vincent ZOU: 包括唯品会，京东这种平台渠道，那么现在合作的深度的合作伙伴有很多。

10
00:00:54.330 --> 00:01:00.630
Vincent ZOU: 都是一般这种全球的进口品牌，那类似于有一个是日本的dhc，

11
00:01:00.800 --> 00:01:13.450
Vincent ZOU: 然后是一个内装的跟进的牌子，我相信你可能也用过。然后还有一个呢，对对，对，是我们跟lv投资的一个品牌就是他投资了，有一个牌子叫standards，

12
00:01:13.450 --> 00:01:27.250
Vincent ZOU: 是一个高端的洗浴品牌，浴户品牌，然后这个品牌呢？在中国，现在大概有80多家门店，且它在这个拉脱维亚的一个一个北欧算是偏东北欧的这么一个牌子吧。

13
00:01:27.790 --> 00:01:43.900
Vincent ZOU: 然后他在线上的这一块呢？是我们相当于跟他进行了合，资，然后也成为了他线上的partner，就持有大概他9.67以上的线上的电子商务的公司的股份，那我们这群人就变成了他ec的这个

14
00:01:43.900 --> 00:01:58.420
Vincent ZOU: 操盘手，他就没有在team了。那为什么会跟我们合作呢？就讲到了ai的这个板块就是除了我们几个人在线上互联网的经验之外，其实我们还是有做大量的这种。

15
00:01:58.700 --> 00:02:07.800
Vincent ZOU: 应该叫就是ai的。这种算法的那些东西，我们现在自己做了一个自己的小的应用工具，我们叫smart投放，

16
00:02:07.910 --> 00:02:26.650
Vincent ZOU: 然后这个smart投放呢？其实在我们核心的应用场景有三个，第一个就是在线上的，中国的互联网其实投放是非常艰难的，就是你的投手，需要去分析大量的这个数据，然后其实用人的效率其实并不高的，所以我们会接入他，只让他去做策略。

17
00:02:26.650 --> 00:02:41.450
Vincent ZOU: 我们用ai来模拟它的策略，但是用ai去把结果都已经计算出来，那这样它能更好的，透过策略然后进行输出，因为这样的话，在投放的过程当中已经全部ai化了，所以它可以更好的调整它的策略，然后让我们的这个系统

18
00:02:41.450 --> 00:02:58.090
Vincent ZOU: 更好的打磨。那其次呢？就是在operation这个label其实有大量的底层的工作是需要大家拉表，然后透过excel，然后再去做透视，其实这是一个非常消耗人效的东西，所以我们就会自己写一些小的程序，然后透过底层的ai，

19
00:02:58.090 --> 00:03:09.680
Vincent ZOU: 我们去爬取一些进队的数据，因为各种品德在不同平台的销售的数据取决于我在不同平台的投放的策略，以及包括我的效率，

20
00:03:09.750 --> 00:03:27.530
Vincent ZOU: 那我的可能在京东会是1.4，但是我在，淘气可能是1.5或者1.6，那这个过程当中跟价格啊，服务啊，正品啊，都是有相关联性的，那这些关联呢？以前都是用人去算，然后再拿来做汇报。我们其实就可以运用自己的场景来去自己做算，

21
00:03:27.910 --> 00:03:30.090
Vincent ZOU: 所以就是非常应用级的啊。

22
00:03:30.240 --> 00:03:43.820
Vincent ZOU: 然后这种功能的过程当中呢？就把这个爬数据的这个工作也做了，因为很多时候我们需要像拼多多也好，其他的平台也好，它会叠各种各样的券，那叠完券之后，你消费者实际到什么价格跟

23
00:03:43.870 --> 00:03:52.140
Vincent ZOU: 我们投放的这个价格其实是有一个gap的，那这个gap就可以用ai来去做测算，这也是我们在应用当中的一个工作。

24
00:03:52.140 --> 00:04:04.220
Vincent ZOU: 那还有呢，就是像ai的一些基础应用了，比如说像一些ppt啊，然后包括像一些比较方便的东西，我们其实基本上都是用ai在解决。大概像我们公司的三个应用方向。

25
00:04:05.800 --> 00:04:12.820
Fan Yunjie: 好的了解。所以您现在您之前是学这个cs相关专业的吗？还是说。

26
00:04:12.820 --> 00:04:13.610
Vincent ZOU: 现在不是

27
00:04:13.820 --> 00:04:15.150
Vincent ZOU: 我是商业出身。

28
00:04:16.880 --> 00:04:21.790
Fan Yunjie: 那您本身为什么会跟合伙人去去进入到这个赛道啊。

29
00:04:23.660 --> 00:04:27.190
Vincent ZOU: 哎，这个要从我简历里面来比较复杂，我，

30
00:04:27.530 --> 00:04:34.560
Vincent ZOU: 我从哪段开始讲呢？我法国大概待了十年，所以我讲法语的，然后我在法国的工作主要是fashion。

31
00:04:35.790 --> 00:04:38.210
Vincent ZOU: 然后从fashion以后呢？我又

32
00:04:38.330 --> 00:04:47.520
Vincent ZOU: 家里面是做珠宝的时候去过珠宝的业务，后来就去了，阿里巴巴就开始转型了。那比较核心的是，说到了阿里之后大概是在一五年，

33
00:04:47.540 --> 00:05:00.250
Vincent ZOU: 然后我应该一七年的中旬，我就离开那里自己创业了，然后做了一个快速的这种经销类的公司也在三年之内吧，我们做了大量出海以及包括这种经销商的业务

34
00:05:00.300 --> 00:05:09.610
Vincent ZOU: 反正做了大概15个亿左右的样子，最后就疫情嘛。然后我又回了阿里去，出任一些更高的职位，然后，在这个过程当中，

35
00:05:09.650 --> 00:05:23.100
Vincent ZOU: 就就是跟一帮志同道合的朋友吧。就现在两个合伙人，那然后就大家一起出来，就是去年的时候又从阿里出来嘛，然后又开始做这个，因为我在阿里的最后一段经历，是菜鸟国际的副总裁。

36
00:05:24.420 --> 00:05:26.340
Fan Yunjie: 是这样所以说。

37
00:05:26.340 --> 00:05:28.060
Vincent ZOU: 有时候做过物流啊。

38
00:05:28.630 --> 00:05:35.520
Fan Yunjie: 那那你本身也没有受过engineering或者相关的这些培训，或者说是自学过，也没有。

39
00:05:35.910 --> 00:05:44.820
Vincent ZOU: 那就是在工作当中，因为你在阿里当行业总经理，其实你是要对你的板块和你的产品需要有足够了解的，我们跟产机之间的互动是比较高的，

40
00:05:45.030 --> 00:06:01.150
Vincent ZOU: 所以我去年离开阿里之后，其实我第一步是先成立了有一家自己的技术公司，我们已经在接加拿大的几个物流的这个这个这个开发的业务了，就技术开发，然后我们这两块业务其实接了加拿大第一大物流集团，

41
00:06:01.250 --> 00:06:08.020
Vincent ZOU: 它叫strea，然后它现在的wms，tms以及包括oms都是我们在做啊。

42
00:06:09.120 --> 00:06:25.010
Vincent ZOU: 对，然后去年的年底吧，我们又成立了这家smartrics，祝您服务于商业，因为本身我又是行业总经理出身，所以跟快销品的品牌的关系都比较好，所以我们就快速做了类似于像这样的model。

43
00:06:25.790 --> 00:06:31.560
Fan Yunjie: 所以你现在在就是ai这个模块里面，你在团队当中的角色是什么呀？

44
00:06:32.070 --> 00:06:36.940
Vincent ZOU: 我偏运营了，就是我会去打磨这个产品，或者对结果做交付

45
00:06:37.890 --> 00:06:41.020
Vincent ZOU: 开发不在我这儿。开发是我们cpu在搞。

46
00:06:41.430 --> 00:06:43.550
Fan Yunjie: 那目前呢？这个渠道分布。

47
00:06:43.830 --> 00:06:51.780
Fan Yunjie: 除您刚刚提到的还会有一些就是比较比较主流的核心，或者说是目前主要的盈利的渠道呢？

48
00:06:53.170 --> 00:06:58.100
Vincent ZOU: 盈利的渠道，我们的渠道就是我们所服务的品牌，给我们的费用啊。

49
00:07:00.450 --> 00:07:06.690
Fan Yunjie: 盈利的品牌给您的费用，那您本身这个渠道的这个布置的这个策略是怎么样啊？

50
00:07:07.730 --> 00:07:09.860
Vincent ZOU: 没懂你能不能再讲一遍？

51
00:07:10.050 --> 00:07:20.510
Fan Yunjie: 就比方说您本身这一个电商业务，或者说是您本身自有的这种dpc，它是您自己研发的ai工具站对吧？

52
00:07:21.140 --> 00:07:21.750
Vincent ZOU: 对啊。

53
00:07:22.270 --> 00:07:30.360
Fan Yunjie: 那他肯定还是要有目标的受众以及客群的呀，对吧？然后您本身是怎么样的一个策略去切入到市场里面呢？

54
00:07:33.000 --> 00:07:35.830
Vincent ZOU: 就是我们服务的这些客户的需求啊。

55
00:07:37.180 --> 00:07:40.260
Fan Yunjie: 获课的这个过程大概是怎么样的呢？

56
00:07:40.640 --> 00:07:48.740
Vincent ZOU: 过客的过程，前期创业肯定是靠朋友嘛，或者靠资源嘛，因为现在这个时候出来创业，肯定是把资源做整合整掉。

57
00:07:49.100 --> 00:07:52.730
Fan Yunjie: 比比较比较还是私欲一点的，前期的时候。

58
00:07:53.090 --> 00:08:00.540
Vincent ZOU: 对你像dc，现在中国区的总经理可能跟lvon的投资的几个副总裁都是我们的朋友嘛。

59
00:08:00.540 --> 00:08:09.060
Fan Yunjie: 了解了解，所以说，那您在这个部署的过程当中的这个策略是怎么分布的，比方说预算呀，

60
00:08:09.170 --> 00:08:14.920
Fan Yunjie: 或者说是一些注重的兴趣吧，或者说是本身，您

61
00:08:15.040 --> 00:08:19.820
Fan Yunjie: 在marketing啊，或者人力啊等等，这一块的一个输出策略是怎么样的。

62
00:08:20.500 --> 00:08:34.860
Vincent ZOU: 我们现在因为首先中国政府在开发这一侧是有补贴的嘛。所以我们其实在开发，只要立项，立完项之后，其实这个都可以记住到未来的开发成本啊，这个我们现在大概公司有

63
00:08:35.360 --> 00:08:39.669
Vincent ZOU: 一，二，三，三个代码，两个产品，然后四个投放。

64
00:08:40.270 --> 00:08:47.840
Vincent ZOU: 目前是这么个情况，但这些成本是我们公司的产品开发成本，所以原则上我不会分摊到我的

65
00:08:48.260 --> 00:08:49.670
Vincent ZOU: 客户头上啊，

66
00:08:49.980 --> 00:08:55.910
Vincent ZOU: 因为我们现在整个的这个工具还是非常的internally的，我们并没有外化啊。

67
00:08:56.600 --> 00:09:03.890
Fan Yunjie: 那一开始在这个ai工具建立之初是先喂给他的什么数据啊，是历史的。

68
00:09:03.890 --> 00:09:04.680
Vincent ZOU: 的时候，

69
00:09:04.810 --> 00:09:09.540
Vincent ZOU: 或者是一些，就是我们会把我们会接到天猫的投放系统里面。

70
00:09:10.380 --> 00:09:20.820
Fan Yunjie: 那它主要的是你你接近来的，它是历史的转化吗？还是说一些。like cvr啊，或者说是频控啊这一类的。

71
00:09:21.140 --> 00:09:27.920
Vincent ZOU: good question。这是一个就是天猫的后台的投放系统，我们简称叫后台管理的

72
00:09:28.010 --> 00:09:47.070
Vincent ZOU: 一个帐号，我们叫生物参谋啊这个生物参谋，如果我一旦能够接到他的ai api之后，我是可以读取他的数据的，然后我可以用它的数据来去做。to因为天猫的后台还有包括像投放系统的达模盘，我们简称叫dmp，那其实都是有在跟我们合作的，

73
00:09:48.000 --> 00:09:50.020
Vincent ZOU: 我们同事也是sv嘛。

74
00:09:50.330 --> 00:09:56.180
Fan Yunjie: 明白，那它的产出呢？产出主要是是是在哪哪，哪个，哪个频段的建议。

75
00:09:56.610 --> 00:10:10.800
Vincent ZOU: 产出的频段，核心是我们在店铺里的某一个宝贝的链接，然后在销售的转化和我投放的这个从正比嘛，我们在阿里巴巴的广告系统里面做投放，然后根据我的投放拉通效率。

76
00:10:11.810 --> 00:10:16.340
Fan Yunjie: 了解，那他也会给到直接的出价呀。这些建议吗？

77
00:10:16.340 --> 00:10:24.350
Vincent ZOU: 会给啊，会给会给他不同的关键词，不同的人群包都会有万亿级以上的组合方式啊。

78
00:10:24.890 --> 00:10:31.040
Fan Yunjie: 明白，那那本身它你有没有去评估过它的这种偏差率，或者说是

79
00:10:31.160 --> 00:10:32.300
Fan Yunjie: 可信性。

80
00:10:33.880 --> 00:10:41.380
Vincent ZOU: 陶内的广告产品原则上还是用陶内的数据，所以一般来讲还是比较准确的。

81
00:10:42.200 --> 00:10:49.720
Fan Yunjie: 这个这个有相应的比率吗？比方说，七日的xpa偏差的大概百分之多少，这样子。

82
00:10:50.420 --> 00:10:54.050
Vincent ZOU: 目前我们现在没有，我们的roi是持续在放大的。

83
00:10:54.840 --> 00:11:06.820
Fan Yunjie: 明白明白那有没有相应的一些手段。针对如果说将来这个模型，对外的话给到给到一些商家话，他们有有这个自己的方式可以去验证。

84
00:11:07.990 --> 00:11:15.330
Vincent ZOU: 可以啊，就是。但是我们现在还没有外化吧，因为也是有很多机型找我们聊，所以我们还没想好要怎么外化啊。

85
00:11:15.950 --> 00:11:24.960
Fan Yunjie: 您能方便给我介绍一下比较近期的两次跟这个您的工具去整个的一个workflow吗？

86
00:11:26.120 --> 00:11:27.720
Vincent ZOU: 稍等啊，我找一个

87
00:11:27.820 --> 00:11:29.910
Vincent ZOU: 爹，你让我们家里

88
00:11:34.020 --> 00:11:36.140
Vincent ZOU: 有没有phone的

89
00:11:37.810 --> 00:11:39.680
Vincent ZOU: 产品概念啊，有的

90
00:11:41.010 --> 00:11:44.650
Vincent ZOU: 稍等啊，我看一下我这个ppt里面有哪些。

91
00:11:46.090 --> 00:11:47.850
Fan Yunjie: 没关系，不要著急。

92
00:11:48.860 --> 00:11:49.850
Vincent ZOU: 然后他们

93
00:11:54.120 --> 00:11:55.550
Vincent ZOU: ye有案例，

94
00:11:57.160 --> 00:11:58.850
Vincent ZOU: 应该有两个案例。

95
00:12:00.940 --> 00:12:06.870
Vincent ZOU: 你比较简单啦，我发给你，要不然你可以针对此而提问，我觉得可能更简单一点。

96
00:12:06.870 --> 00:12:07.720
Fan Yunjie: 好呀。

97
00:12:09.560 --> 00:12:11.010
Vincent ZOU: Checking again.

98
00:12:11.860 --> 00:12:12.570
Vincent ZOU: 对啊

99
00:12:17.050 --> 00:12:18.160
Vincent ZOU: 哎，我找一下啊

100
00:12:19.370 --> 00:12:20.320
Vincent ZOU: what's gonna be.

101
00:12:20.320 --> 00:12:22.550
Fan Yunjie: 来不著，急。

102
00:12:38.670 --> 00:12:39.620
Vincent ZOU: 不是

103
00:12:42.090 --> 00:12:44.150
Vincent ZOU: 可以大概看一下。

104
00:12:44.630 --> 00:12:46.340
Fan Yunjie: 稍等啊，我接收一下。

105
00:12:46.720 --> 00:12:47.560
Vincent ZOU: Ok.

106
00:13:10.410 --> 00:13:15.940
Fan Yunjie: 你本身在日常生活中还是会经常使用到ai嘛。就工作以外的。

107
00:13:16.390 --> 00:13:21.600
Vincent ZOU: 客户包跟gpt吧最近使用的比较多。前两天我在研究那个嘛。

108
00:13:23.610 --> 00:13:27.570
Fan Yunjie: 为什么会研究他？是因为有一些marketing的需求吗？

109
00:13:28.170 --> 00:13:40.200
Vincent ZOU: 因为我不是有两家公司了吗？还有一家叫技术公司，那个技术公司是没有ppt介绍，因为我们只是接技术类的外包服务。然后呢，我们其实也有在做一些类似于像。

110
00:13:40.730 --> 00:13:56.840
Vincent ZOU: 好的好的就是那个，那个，那个，那个。视频，短视频的这种制作，然后服务于电商这种ai的东西。所以呢，我就想再看一下nobana是不是有更好的模型能够帮我们做驱动，大概是这个样子。

111
00:13:58.790 --> 00:14:02.430
Fan Yunjie: 本身就是您本身使用ai的话

112
00:14:02.710 --> 00:14:06.970
Fan Yunjie: 有多大程度上依赖性啊？从现在的这个工作来说的话。

113
00:14:09.320 --> 00:14:22.590
Vincent ZOU: 跟工作的依赖性不高的，除非我要去看某一个品牌在海外的市场。就举个例子，我们可能接触了一个日本品牌，但我对他不是特别了解，我极有可能会透过gpt上去看一下。大概是这个样子。

114
00:14:23.110 --> 00:14:26.020
Fan Yunjie: 就是说工作维度上还是不是太依赖他的。

115
00:14:26.460 --> 00:14:30.230
Vincent ZOU: 我，我们自己的ai，我是会看的，每天会看数据。

116
00:14:30.730 --> 00:14:41.010
Fan Yunjie: 因为您本身也是提到你在运营这个方面嘛。所以说运营的策略这一块是完全不会去截留到ai，完全是靠自己的经验来输出的，对吗？

117
00:14:41.560 --> 00:14:42.190
Vincent ZOU: 对的。

118
00:14:44.130 --> 00:14:54.990
Fan Yunjie: Okay。所以你觉得说目前来说，我不知道ai它在国内电商是个什么样情况？它的价值是在效率吗？还是说在。

119
00:14:54.990 --> 00:14:55.650
Vincent ZOU: 哪里。

120
00:14:55.650 --> 00:14:57.300
Fan Yunjie: 还是在效率这一块。

121
00:14:57.300 --> 00:14:58.020
Vincent ZOU: 对啊，

122
00:14:58.310 --> 00:15:02.770
Vincent ZOU: 人家ai的投放目前我们看起来应该现在是

123
00:15:03.050 --> 00:15:04.810
Vincent ZOU: 是最快的，88秒。

124
00:15:05.320 --> 00:15:24.710
Fan Yunjie: 大家都会应用ai到哪些领域上，就我知，我比较熟悉的，可能是很大程度上把团队给它给它弱化了。除此以外，然后like，用户运营这一块可能也会有一定的这个这个削弱性，那除此以外，还会有一些哪一些，哪些价值，在。

125
00:15:28.880 --> 00:15:35.740
Vincent ZOU: 其实核心最多还是在投放的时候，他不需要再去做执行了，他只要做策略的输出。

126
00:15:37.590 --> 00:15:46.530
Fan Yunjie: 您本身这个策略输出是指他完全吃以前的样本，然后输出的这种概率性的策略吗？对。

127
00:15:46.690 --> 00:15:59.070
Vincent ZOU: 对。就比如说我举个例子，我们今天透过后台的数据可以观测到山东的购买人群在上升，那我以前可能就需要调用我的后台里的人群包，我要一个一个去看，

128
00:15:59.070 --> 00:16:07.530
Vincent ZOU: 那我今天只要点击山东，然后我的ai可能就大面积跳出来，这部分的人群包，我可以直接做投放，就相当于我的策略会被

129
00:16:07.530 --> 00:16:11.840
Vincent ZOU: 优化，就是我不用再去干一些更执行类的东西。

130
00:16:12.350 --> 00:16:19.980
Fan Yunjie: 这个策略，它的这个准确性有多高呢？或者说是它可以直接应用性有多高。

131
00:16:20.450 --> 00:16:26.460
Vincent ZOU: 我们现在看下来，我们整体的这个roi环比一直在涨，那就说明我们的

132
00:16:26.630 --> 00:16:28.620
Vincent ZOU: 效率和准确率都是高的呀。

133
00:16:30.190 --> 00:16:37.270
Fan Yunjie: 那这个我还是想知道一下本身。在这个过程当中，ai直接输出的策略就直接应用了吗？还是说。

134
00:16:37.520 --> 00:16:39.760
Vincent ZOU: 人输出策略ai执行。

135
00:16:40.340 --> 00:16:41.480
Fan Yunjie: 这这个。

136
00:16:41.480 --> 00:16:49.700
Vincent ZOU: 以前是以前是用用人在既输出策略又要执行，所以人就很累人，要建无数个计划，

137
00:16:50.290 --> 00:16:53.460
Vincent ZOU: 这个可能跟后台有关系，所以

138
00:16:53.740 --> 00:17:00.160
Vincent ZOU: 要要要先看到后台，可能你可能会有概念，但这个后台我可能还没法给你看。

139
00:17:00.160 --> 00:17:12.099
Fan Yunjie: 没关系，您就告诉一下我本身在，因为因为您肯定是经历过没有ai的那个时代，到今天就是你整个的workflow，它有多大的差别？

140
00:17:12.950 --> 00:17:19.250
Vincent ZOU: 能优化。就是说，以前人一天只能建十到15个计划，但我今天应该可以建100个计划。

141
00:17:22.430 --> 00:17:23.640
Fan Yunjie: 注册。

142
00:17:23.650 --> 00:17:26.310
Vincent ZOU: 测完之后，譬如说这100个计划，里面有

143
00:17:26.589 --> 00:17:31.600
Vincent ZOU: 70个都是不好的，30个是好的，那我可以快速挑选，再做优化，再做执行。

144
00:17:32.240 --> 00:17:36.940
Fan Yunjie: 这个优化的过程，您是会调教ai去做，还是自己去做优化呀？

145
00:17:37.280 --> 00:17:45.350
Vincent ZOU: Ai底层在算我们自己的话，主要是输出，因为我们运投放的同学，他可能感受不到ai的

146
00:17:45.850 --> 00:17:46.560
Vincent ZOU: 工作。

147
00:17:48.450 --> 00:17:51.150
Vincent ZOU: 其实我就让我在ai在学习它的策略。

148
00:17:51.630 --> 00:17:58.350
Fan Yunjie: 那在这个过程当中，他的，他优化的依据点在哪里？是完全凭借经验吗？还是说。

149
00:17:58.860 --> 00:17:59.980
Vincent ZOU: 经验跟数据。

150
00:18:00.260 --> 00:18:01.960
Fan Yunjie: 经验跟数据。

151
00:18:02.240 --> 00:18:02.910
Vincent ZOU: 对啊。

152
00:18:03.150 --> 00:18:08.260
Fan Yunjie: 所以本身在这个过程当中，我还是想要比较细化的，知道这个这个

153
00:18:08.570 --> 00:18:21.840
Fan Yunjie: 和协作的这个流程，比方说一开始的insights，然后怎么样去给他投入到什么样的。然后他大概是一个怎么样的一个一个一个结构底下，这个过程maybe。

154
00:18:26.770 --> 00:18:28.380
Vincent ZOU: 这个应该怎么回答呢？

155
00:18:34.270 --> 00:18:44.460
Vincent ZOU: 我没有想，就是我们现在应该还没有像你说的这种解构到这个阶段，就是因为我们的终极目标是想把我们aa打造成

156
00:18:44.680 --> 00:19:00.580
Vincent ZOU: 就是我们比如说现在在市面上找到三个非常强大的投手，我可能需要读他的投放策略，然后透过一年的时间线来让我的ai就形成一个A的投投放策略。B的投放策略和C的投放策略，然后把这三个再做融合。

157
00:19:00.870 --> 00:19:09.120
Fan Yunjie: 所以它就是a，b test的一个方向就是对吧？然后那那他如何替代这种渠道性的操盘啊？

158
00:19:10.520 --> 00:19:13.570
Vincent ZOU: 如何替代它，只需要在后台里面接入就好啦。

159
00:19:14.020 --> 00:19:17.250
Vincent ZOU: 我们后台里面只要跟阿里的后台绑定就好了。

160
00:19:18.260 --> 00:19:25.650
Fan Yunjie: 您觉得这样的跟跟这种传统的渠道操盘有什么有什么直接性的差别。除了效率上。

161
00:19:26.720 --> 00:19:40.260
Vincent ZOU: 效率跟就是出错率。就是说人会出错的嘛。人在执勤的时候，主要的策略讲得很清楚，结果他的点的时候可能一不小心把人群包的A包点成了B包，那可能投出来就不准了嘛。

162
00:19:41.140 --> 00:19:43.470
Vincent ZOU: 对，那这种是不会出错的。

163
00:19:44.350 --> 00:19:54.740
Fan Yunjie: 它不会出错，是因为您验证之后不会出错，还是说本身它的这个出来的结果就是打眼一看呢，它的可信性就很高。

164
00:19:55.260 --> 00:19:57.570
Vincent ZOU: 哎，你看就是从看的。

165
00:19:57.980 --> 00:20:04.490
Fan Yunjie: 就凭经验看了。您大概凭经验直接看到它的准确性有多高能达到百分之多吗？

166
00:20:05.260 --> 00:20:10.480
Vincent ZOU: 行经验，看到它的准确性90%以上吧。

167
00:20:10.930 --> 00:20:12.640
Fan Yunjie: 90%以上。

168
00:20:14.060 --> 00:20:19.260
Fan Yunjie: 那他剩下的是他的这个弱弱项，或者说他weaken是在哪里啊？

169
00:20:20.930 --> 00:20:24.460
Vincent ZOU: 就是他策略还没有学会嘛，我们现在就是一直在让他学策略嘛。

170
00:20:25.840 --> 00:20:33.280
Fan Yunjie: 明白了解那它目前来说的话，不论是像文本处理啊，像lp啊，或者是数据优化呀，

171
00:20:33.430 --> 00:20:36.400
Fan Yunjie: 哪一个roi的提升是最明显的。

172
00:20:38.300 --> 00:20:41.830
Vincent ZOU: 在我们的案例里应该有解析吧，稍微等啊，

173
00:20:42.650 --> 00:20:45.140
Vincent ZOU: 我们应该有讲了一下这个

174
00:20:45.820 --> 00:20:49.690
Vincent ZOU: 关于我们在standards的投放上

175
00:20:50.370 --> 00:20:53.180
Vincent ZOU: 应该是有讲过一些case，

176
00:20:54.250 --> 00:20:56.130
Vincent ZOU: let me check it on.

177
00:21:02.930 --> 00:21:13.210
Vincent ZOU: 对，我们在我们跟有一个叫师丹，然的合作，就是你看就是你看这里他有一个step one，我们的商品决策嘛。我们先从商品决策开始，看。

178
00:21:13.370 --> 00:21:17.650
Fan Yunjie: 对，那我如果你会看到我的商品角色其实是更加的。

179
00:21:17.750 --> 00:21:18.800
Vincent ZOU: 简单。

180
00:21:19.630 --> 00:21:22.580
Vincent ZOU: 我在发布，然后我的智能飞机，我的产品，

181
00:21:22.690 --> 00:21:30.240
Vincent ZOU: 然后同时大家的消费的决策因素可能就是税前放松，解压成分，安全品牌未输对吧？

182
00:21:30.300 --> 00:21:46.240
Vincent ZOU: 那根据它，我们就可以更快速的去看我们的品牌目标客户的，这个它的这个核心人群和匹配啊，是否匹配到乡分啊？还是匹配到这个人群的心智啊，就是会根据它再去做决策。

183
00:21:47.590 --> 00:21:51.940
Vincent ZOU: 那，然后我们ai辅助能启动创意分组，那我们就会做大量的创意，

184
00:21:53.300 --> 00:22:02.060
Vincent ZOU: 然后基于我的人群分层创意分子，创意脚本，分镜头创意，然后再做我的这个玩法，最后那我们就是

185
00:22:02.060 --> 00:22:16.340
Vincent ZOU: 得到说人工冰镇，我能启动，我现在就应该是通过造水，分乳油这三套打法来贯穿玫瑰香到正宫箱的这种概念，然后去触打，所以我的roi在抖音可以干到2.8，

186
00:22:16.410 --> 00:22:26.980
Vincent ZOU: 且我的高脚本转化，复制达人分发之后，我的效率就不停的提升，我们从零接手机的店铺到达到月销2,000,000，只，用了三个月。

187
00:22:27.890 --> 00:22:36.550
Fan Yunjie: So so what is like？你从roi从一到2.8到2.9，这过程当中，你ai跟人工分别起到了什么作用？

188
00:22:37.460 --> 00:22:44.010
Vincent ZOU: Aa就可以更好的辅助我把效率拉出去，更好的做分析。那人呢？来去校正它所有的执行。

189
00:22:44.710 --> 00:22:54.450
Fan Yunjie: 它所有的像是达人共创啊，爆款的dna啊，什么冷启动啊这些全部是依赖于原本淘宝的数据库吗？

190
00:22:55.280 --> 00:22:59.560
Vincent ZOU: 对这个是个抖音的数据库啊，所以就是依赖于抖音的数据库。抖音的天真。

191
00:23:00.330 --> 00:23:03.490
Fan Yunjie: 明白，那那那如果说我们就是

192
00:23:04.020 --> 00:23:11.730
Fan Yunjie: 就是有一个问题，就比方说有些爆款在我理解啊，它可能是内容，它是不可预测的。那这种问题怎么解决啊？

193
00:23:13.150 --> 00:23:16.870
Vincent ZOU: 抖音就是不可预测的呀，所以它就把不可预测变成了可预测了。

194
00:23:17.120 --> 00:23:22.260
Vincent ZOU: 我们一开始启动这套方案的时候，也没想到它能够做到这么好，对于我们来讲也是surprise，

195
00:23:22.940 --> 00:23:31.380
Vincent ZOU: 其实就是我们更更精准的捞到了他A3到A5的人群或者A1到A3的人群来进行二次到三次解代码。

196
00:23:32.730 --> 00:23:44.490
Fan Yunjie: 但是我我还是没有特别理解，就是说它是针对于这种预测型，它是完全是按照概率来来来去推断，然后就可以推断出来这个商品，它是有可能是爆款，是这样子吗？

197
00:23:45.490 --> 00:23:46.090
Vincent ZOU: 对啊，

198
00:23:46.810 --> 00:23:51.730
Vincent ZOU: 他因为读完了抖音的大数据，他大概就是做了一个分解嘛。

199
00:23:52.130 --> 00:23:53.590
Fan Yunjie: 明白明白，

200
00:23:53.920 --> 00:24:05.720
Fan Yunjie: 那因为我看到你是一个其实还是一个全渠道的这样一个跨平台的，那这种数据整合它的，它应该还是有有点难难度的吧。那种难点怎么解决。

201
00:24:05.970 --> 00:24:10.150
Vincent ZOU: 第二阶段，我们现在还在做很多这条新的系统还在调整。

202
00:24:12.170 --> 00:24:19.660
Fan Yunjie: 在这个过程当中的话，就是你，你将来会要把它变成一个付费的，还是说

203
00:24:19.770 --> 00:24:23.520
Fan Yunjie: 更大程度上去免费的一个使用的一个业态啊。

204
00:24:24.070 --> 00:24:25.250
Vincent ZOU: 确实没想好。

205
00:24:25.450 --> 00:24:26.250
Fan Yunjie: 这个还没有写。

206
00:24:26.250 --> 00:24:35.800
Vincent ZOU: 反而现，但现在不开放，因为所有的投资方过来跟我们聊完都想投我们，而且我们现在都不想让他投，呵，目前是这么个阶段，

207
00:24:36.170 --> 00:24:40.170
Vincent ZOU: 因为很多现在国内大量的ai其实还蛮

208
00:24:40.800 --> 00:24:58.370
Vincent ZOU: 骗人的吧。然后呢？对于他们来讲，看到我们有这么一种雏形，然后也已经在去做，他们其实还是比较惊讶的，所以很多人找我们在谈，我们就现在还没有谈我们现在没有想到那一步，我们还是想先把这个产品打磨好，这就是最核心的一个。

209
00:24:59.080 --> 00:25:04.820
Vincent ZOU: 当然，当然，当然我觉得现在很大的问题就是很多融资都是白皮书项目，然后。

210
00:25:04.820 --> 00:25:17.750
Fan Yunjie: 可能真正意义上把产品打磨好的像方反而是有这个问题。对。所以说，您本身我，我想如果说投真过来他看到的是哪一方面的可信度

211
00:25:18.530 --> 00:25:20.550
Fan Yunjie: 就完了就转化的吧。

212
00:25:21.570 --> 00:25:25.190
Vincent ZOU: 我们的产品就是这些。其实国内这些投资人对ai

213
00:25:25.540 --> 00:25:31.700
Vincent ZOU: 比我们想象中的要牛得多。然后他们已经接触了大量的，包括硅谷的ai

214
00:25:32.040 --> 00:25:37.660
Vincent ZOU: 国内人员，他们看了很多很多项目，就是我们现在做的还是比较务实的吧。应该这么讲。

215
00:25:38.510 --> 00:25:47.450
Fan Yunjie: 目前来说，没有您所知的任何竞品，或者说是任何跟您的这个usb可以可以可以竞竞标这样的产品吗？

216
00:25:48.150 --> 00:25:49.380
Vincent ZOU: 目前我没发生。

217
00:25:50.460 --> 00:25:51.110
Fan Yunjie: 目前没有。

218
00:25:51.110 --> 00:25:55.820
Vincent ZOU: 或者说市面上没有人做，或者说我们的cto其实就决定了

219
00:25:56.050 --> 00:25:59.280
Vincent ZOU: 现阶段的人应该没有，他其实是一个

220
00:25:59.880 --> 00:26:12.440
Vincent ZOU: 就像用ai写代码是一个道理的，就是你如果没有处理过万级以上的交易规模。原则上来讲，你在写代码的过程当中，你是并不可能知道你的你的ai犯了什么样的错误。

221
00:26:14.060 --> 00:26:18.410
Fan Yunjie: 那这个出这个问题很好，但是您本身也没有写过代码呀。

222
00:26:19.090 --> 00:26:23.430
Vincent ZOU: 我，我，我有合伙人写啊，他写就好了呀，我是商务加运营的，

223
00:26:24.310 --> 00:26:28.480
Vincent ZOU: 我是往回捞资源的那个人，我是未来要去卖ai的，这个人。

224
00:26:29.900 --> 00:26:39.880
Fan Yunjie: 那本身这个错音您觉得是在哪里。如果说他发现了这个相相对于来说可以推翻的一些论据，它的错音是在

225
00:26:40.030 --> 00:26:47.490
Fan Yunjie: 数据啊，还是说ai自己的这种我们说，nation就是ai自己的幻觉啊，和它自己的混乱的记忆。

226
00:26:47.490 --> 00:26:48.410
Vincent ZOU: 这样的就是我。

227
00:26:48.410 --> 00:26:48.910
Fan Yunjie: 你不喝。

228
00:26:48.910 --> 00:27:04.730
Vincent ZOU: 我的技术合伙人肯定是一个，就是他在阿里的时候是已经就是处理过慢一级这种交易规模以上的这种人了，所以他脑子里有的数据量远不是现在的这些同学能比的吧。对。

229
00:27:05.510 --> 00:27:06.490
Fan Yunjie: 对。

230
00:27:06.950 --> 00:27:07.530
Fan Yunjie: 对啊。

231
00:27:07.800 --> 00:27:09.630
Vincent ZOU: 对这个是核心，

232
00:27:09.910 --> 00:27:12.860
Vincent ZOU: 就是他们的脑子可能更值钱。你可以这么理解。

233
00:27:14.050 --> 00:27:17.500
Fan Yunjie: 对。当然，处理量化的脑子是不是很厉害。

234
00:27:17.500 --> 00:27:24.560
Vincent ZOU: 对，所以我的cto加我的，我两边公司cpu都是现在市面上不可或缺的。

235
00:27:25.570 --> 00:27:37.500
Fan Yunjie: 明白，我其实是想知道就是针对于他们发现的普遍性的数据错音是在ai的处理问题，还是说他们提示的问题，还是说一些。

236
00:27:37.910 --> 00:27:38.640
Vincent ZOU: 经验嘛，

237
00:27:38.900 --> 00:27:41.110
Vincent ZOU: 如果你讲代码的话，这是经验。

238
00:27:42.120 --> 00:27:49.560
Fan Yunjie: 那如何降低这种风险呢？或者说当本身这种不确定性出现的时候。

239
00:27:51.080 --> 00:27:53.420
Vincent ZOU: 这样，那这个肯定就会被取代的呀？

240
00:27:53.540 --> 00:27:56.170
Vincent ZOU: 那不就趁著这两年的风口先往前跑吗？

241
00:27:56.380 --> 00:27:58.690
Vincent ZOU: 我们先打磨出来，我们就快人一步嘛。

242
00:28:00.920 --> 00:28:03.250
Fan Yunjie: 了解，了解

243
00:28:03.810 --> 00:28:10.270
Fan Yunjie: 现在竞品的数据，第三方的数据，api接口是完全也是靠关系

244
00:28:10.520 --> 00:28:12.790
Fan Yunjie: 搞进来，咱们还是说需要一定成本啊。

245
00:28:13.960 --> 00:28:15.990
Vincent ZOU: 正常申请就好了呀。

246
00:28:16.210 --> 00:28:17.770
Fan Yunjie: 申请api，让他们。

247
00:28:17.770 --> 00:28:18.150
Vincent ZOU: 这样子。

248
00:28:18.350 --> 00:28:22.530
Fan Yunjie: 这种是可以，就是外包出去的，对吗？

249
00:28:23.100 --> 00:28:26.380
Vincent ZOU: 这个怎么？这个是这个是符合规则的。

250
00:28:28.340 --> 00:28:30.360
Fan Yunjie: 了解，那有没有

251
00:28:30.530 --> 00:28:32.400
Fan Yunjie: 像做过这种？

252
00:28:34.740 --> 00:28:40.110
Fan Yunjie: 像爬取数据的收入，一旦有这种缺失的话，这种回馈策略是有的嘛。

253
00:28:41.310 --> 00:28:44.500
Vincent ZOU: 对肯定有的呀，我们现在最重要的就是要合规嘛，

254
00:28:45.220 --> 00:28:47.100
Vincent ZOU: 不合规的事情我们都干不了。

255
00:28:49.580 --> 00:28:57.990
Fan Yunjie: 那一旦他这种策略，如果说翻车的话，他是一键它就复盘了吗？还是说也是需要比较漫长的一个爬的路径啊。

256
00:28:59.690 --> 00:29:03.750
Vincent ZOU: 这个我还真不知道，我可能要跟技术对一下你问的这个问题难到我了。

257
00:29:03.750 --> 00:29:06.080
Fan Yunjie: 没关系，它不重要。

258
00:29:06.080 --> 00:29:07.240
Vincent ZOU: 然后。

259
00:29:08.720 --> 00:29:18.150
Fan Yunjie: okay，那比方说你最常看到的这种ai测算跟真实的用户行为有没有很很大的或很直接的偏差。

260
00:29:18.570 --> 00:29:19.830
Vincent ZOU: 没有，目前没有。

261
00:29:20.120 --> 00:29:24.080
Fan Yunjie: 目前没有这么这么这么准确的吗？这个。

262
00:29:24.080 --> 00:29:27.050
Vincent ZOU: 可能已经外化到我们这儿就已经是准确版了吧。

263
00:29:27.190 --> 00:29:35.490
Vincent ZOU: 我不知道技术产系是不是它真的在过程当中有碰到一些困难，但是外化到我们这儿来说，我们其实基本上已经都没有什么

264
00:29:35.750 --> 00:29:37.330
Vincent ZOU: 太大的问题的话。

265
00:29:38.200 --> 00:29:41.890
Fan Yunjie: Ok他的那所以它的这个gap的定义是什么？

266
00:29:43.010 --> 00:29:45.510
Vincent ZOU: Gap的定义，你说的是哪一段？

267
00:29:45.510 --> 00:29:55.890
Fan Yunjie: 就比方说从预测呀到到实际呀，然后可能你拆成数据模型在执行啊，什么这种这种这种业态的分层啊，这一类的。

268
00:29:56.870 --> 00:30:01.440
Vincent ZOU: 目前我还真不知道这个这个我可能要问一下技术同学。

269
00:30:02.640 --> 00:30:04.910
Fan Yunjie: Okay，没关系，这里不重要。

270
00:30:04.910 --> 00:30:05.760
Vincent ZOU: Ok.

271
00:30:06.430 --> 00:30:10.950
Fan Yunjie: 好，那那你认为ai有没有可能忽略到了某一些

272
00:30:11.290 --> 00:30:14.880
Fan Yunjie: 就是颗粒细化的人群的信号。

273
00:30:14.880 --> 00:30:16.280
Vincent ZOU: 会会会。

274
00:30:16.790 --> 00:30:18.190
Fan Yunjie: 好这个，这个。

275
00:30:18.190 --> 00:30:18.710
Vincent ZOU: 对的。

276
00:30:18.940 --> 00:30:20.030
Fan Yunjie: 比方说。

277
00:30:20.960 --> 00:30:25.730
Vincent ZOU: 比方说A人群跟B人群的慢速度，不高嘛，这就有可能啊。

278
00:30:27.250 --> 00:30:32.120
Fan Yunjie: 这种一般是出现在什么样的人群呢？比方说复购嘛，或还是说

279
00:30:32.480 --> 00:30:42.090
Fan Yunjie: 价格敏感度比比比较弱的呀，还是说怎么样，某些场景可以给我介绍一下吗？或者直接细化的这种人群的画像。

280
00:30:45.660 --> 00:30:48.850
Vincent ZOU: 比如说我可能想打山东济南的他跳去了泰安。

281
00:30:49.060 --> 00:30:49.950
Vincent ZOU: 举个例子。

282
00:30:51.330 --> 00:30:55.660
Fan Yunjie: 但是这样的这个山东济南到太阳，没有什么概念。

283
00:30:56.140 --> 00:30:58.300
Vincent ZOU: 在那个100公里左右的距离差距。

284
00:30:59.130 --> 00:31:02.640
Fan Yunjie: 这样子会对未来的这个结果偏差很大吗？

285
00:31:03.200 --> 00:31:05.390
Vincent ZOU: 会啊，因为我人群不想打那儿啊。

286
00:31:07.310 --> 00:31:10.990
Fan Yunjie: 你，你在一开始的时候就可以发现这个问题，还是说。

287
00:31:11.970 --> 00:31:18.510
Vincent ZOU: 一开始我们发现不了这个问题就是要到结果出来的时候才会发现它错了嘛，然后再去纠正。

288
00:31:20.120 --> 00:31:25.860
Fan Yunjie: 是因为看不到你这个ai工具，他的思考过程吗？还是说本身对。

289
00:31:25.860 --> 00:31:37.940
Vincent ZOU: 这因为我们跟技术其实并没有这么因为到运营和交付的这个阶段，其实你可能不一定要太细化的去看这些数据集中，因为我可能更添结果导向。

290
00:31:38.130 --> 00:31:43.150
Fan Yunjie: 那那是在哪一个阶段发现出来了，这种。

291
00:31:43.470 --> 00:31:44.120
Vincent ZOU: 结果

292
00:31:44.220 --> 00:31:47.290
Vincent ZOU: 就是推算出来的这个结果的时候，你会发现了呀。

293
00:31:48.760 --> 00:31:56.850
Fan Yunjie: 推算结果的时候，那你发现这个情况还会去让ai重新的再来一遍吗？比方说跟他跟他拜头。

294
00:31:56.850 --> 00:31:59.350
Vincent ZOU: 让他再校准呢？让他再跟他读一下吧。

295
00:31:59.960 --> 00:32:04.580
Fan Yunjie: 这种的，这个过程大概会几轮，它能够交流过来。

296
00:32:05.300 --> 00:32:07.920
Vincent ZOU: 这可以问一下技术了，我还真不知道。

297
00:32:08.570 --> 00:32:18.930
Fan Yunjie: Okay，那您也不会参与到这种返利啊，或者说是逼迫ai去给出直接准确视角的。这个过程对吗？

298
00:32:19.200 --> 00:32:19.890
Vincent ZOU: 不会。

299
00:32:20.900 --> 00:32:33.980
Fan Yunjie: 了解了解，所以也不会说要求ai给出一些就是假设，或者说是给出plan plan b plan c，然然后去比较相对来说增加一些准确性。

300
00:32:36.240 --> 00:32:37.460
Vincent ZOU: 吴大会。

301
00:32:37.640 --> 00:32:38.790
Fan Yunjie: 舞大会。

302
00:32:39.250 --> 00:32:40.060
Vincent ZOU: 对，对，对。

303
00:32:40.490 --> 00:32:45.160
Fan Yunjie: 觉得说这样会好一些吗？如果说未来的应用里面。

304
00:32:46.050 --> 00:32:58.360
Vincent ZOU: 这个现阶段我还真的，没有跟他们校准过这个产品啊，我可能要到下次开产品约会的时候才能有到具体的数据的回流。我们下次什么时候开，应该要10月了。

305
00:32:58.720 --> 00:32:59.690
Fan Yunjie: 然后。

306
00:32:59.830 --> 00:33:06.860
Vincent ZOU: 对10月份会再开一次产品会，那那个时候可能大家才会针对这个板块再去做优化。

307
00:33:07.610 --> 00:33:09.180
Fan Yunjie: 明白，明白。

308
00:33:09.180 --> 00:33:09.910
Vincent ZOU: 对啊。

309
00:33:09.910 --> 00:33:21.600
Fan Yunjie: 所以这个您的这个ai应用，它的门槛高不高，或者说是针对于本身的客群来讲，需不需要去有一些基础才能够使用啊。

310
00:33:23.710 --> 00:33:25.430
Vincent ZOU: 需要懂一定的投放。

311
00:33:26.390 --> 00:33:28.820
Fan Yunjie: 需要懂投放对，然后。

312
00:33:28.820 --> 00:33:32.910
Vincent ZOU: 但是我们逐渐想把它变成傻子模式嘛，但这个需要时间。

313
00:33:34.380 --> 00:33:40.830
Fan Yunjie: 目前来想您，如果按照您对于现阶段的to be的这些用户，理解

314
00:33:41.230 --> 00:33:45.520
Fan Yunjie: 他有哪些直接低门槛可以复用的这些应用模块吗？

315
00:33:48.570 --> 00:33:50.360
Vincent ZOU: 目前没有了。

316
00:33:51.000 --> 00:33:52.650
Fan Yunjie: 目前还是没有的。

317
00:33:52.650 --> 00:33:53.230
Vincent ZOU: 对对。

318
00:33:53.230 --> 00:34:01.230
Fan Yunjie: Ok，他现在可以直接画出来就是很准确的说众画像，除了说他自在一开始就错了，他就跑跑偏了。

319
00:34:04.180 --> 00:34:07.750
Vincent ZOU: 比概率比较少，没有这么高。

320
00:34:08.060 --> 00:34:09.710
Fan Yunjie: 没有那么高。

321
00:34:09.719 --> 00:34:10.759
Vincent ZOU: 对对。

322
00:34:12.150 --> 00:34:22.489
Fan Yunjie: 就是是概率的问题是在于调校的这个。频率以及长短，还是说它就是没有办法达成。

323
00:34:23.489 --> 00:34:36.109
Vincent ZOU: 这个我们也不清楚，因为它是基于规则的问题。其实我们在对抗人群包的时候，是不能够获取消费者信息的，因为要符合法律法规嘛，所以只能在黑格里边算。

324
00:34:36.699 --> 00:34:38.389
Fan Yunjie: 了解。

325
00:34:38.389 --> 00:34:42.919
Vincent ZOU: 我，我没有办法白，核白核就就违规了嘛，我们不会干这样的事情，

326
00:34:43.329 --> 00:34:46.979
Vincent ZOU: 所以就必须要黑盒，黑盒里面其实你看不到。

327
00:34:47.969 --> 00:34:49.519
Fan Yunjie: 明白，明白。

328
00:34:49.759 --> 00:34:51.719
Vincent ZOU: 国内是不允许白说话的。

329
00:34:53.280 --> 00:34:54.489
Fan Yunjie: 可以，

330
00:34:55.750 --> 00:35:07.160
Fan Yunjie: okay，所以本身其实在您的dashboard这一块产品的dashboard这一块他是完全可以支撑这个品牌，从认知到兴趣，

331
00:35:07.340 --> 00:35:14.570
Fan Yunjie: 或者说到这个用户的购买，一直到后期用户的留存，这样全链路的它目前都可以整合起来，对吗？

332
00:35:15.220 --> 00:35:16.190
Vincent ZOU: 分，谢谢

333
00:35:16.940 --> 00:35:22.780
Vincent ZOU: 这个。其实阿里本身就有，我们只是把以前在阿里内部的一些外化了。

334
00:35:24.520 --> 00:35:28.600
Fan Yunjie: 它跟之前的这个阿里本身存在的系统

335
00:35:28.720 --> 00:35:33.630
Fan Yunjie: 在链路上有哪些优点，或者说有哪些特殊的卖点呢？

336
00:35:36.100 --> 00:35:48.660
Vincent ZOU: 个速的慢点，我没想，但至少我现在看到的数据后台，我觉得是我在阿里面应用的时候的差不多，相似度是差不多的，那壁垒的话，可能就是开发这个产品的人就是我cpu，

337
00:35:49.230 --> 00:35:52.420
Vincent ZOU: 所以他本来就是开发这个东西的人。

338
00:35:53.860 --> 00:36:00.200
Fan Yunjie: 那它对于成本的节流呢？比方说对于一些

339
00:36:00.520 --> 00:36:06.680
Fan Yunjie: 策略上的付费呀，免费啊这样的一些配瓶的问题，它它解决的如何呢？

340
00:36:08.180 --> 00:36:11.120
Vincent ZOU: 你roi都这么高了，我目前来看还好，哎，

341
00:36:11.580 --> 00:36:15.370
Vincent ZOU: 这个没有什么成本的问题，因为rtp的流量本来就长成这个样子。

342
00:36:15.730 --> 00:36:20.020
Fan Yunjie: 他怎么如何，他怎么动态的去调整这个这个配比问题的。

343
00:36:20.830 --> 00:36:22.390
Vincent ZOU: 结果还得问他就是。

344
00:36:22.390 --> 00:36:23.060
Fan Yunjie: 上访的时候。

345
00:36:23.060 --> 00:36:24.780
Vincent ZOU: 了，他应该不会告诉我他。

346
00:36:24.780 --> 00:36:25.360
Fan Yunjie: 好好玩。

347
00:36:25.360 --> 00:36:26.570
Vincent ZOU: 谁来告诉我们也不懂。

348
00:36:26.900 --> 00:36:27.860
Fan Yunjie: 赢吧。

349
00:36:29.250 --> 00:36:31.760
Fan Yunjie: 那有没有那种就是

350
00:36:32.470 --> 00:36:37.920
Fan Yunjie: ai的策略输出一下子就被你推翻掉了。这种情况。

351
00:36:39.110 --> 00:36:43.730
Vincent ZOU: 这个可能要问一下我们投手呗，具体的执行一般是小朋友在做嘛。

352
00:36:44.780 --> 00:36:46.680
Fan Yunjie: 明白了解，

353
00:36:47.120 --> 00:36:51.980
Fan Yunjie: 所以您觉得目前来说的话，就未来的趋势是上来讲

354
00:36:52.130 --> 00:36:56.140
Fan Yunjie: 人工跟ai大概的占比会有多多大。

355
00:36:57.550 --> 00:37:05.330
Vincent ZOU: 我们现在看下来，人工已经降低了很多，就是现在公司也才十几个人吧。

356
00:37:06.810 --> 00:37:14.480
Fan Yunjie: 大概的一个比率是怎么样的，像什么环节是必须要人工像什么环节就是逐渐就可以被稀释掉了。

357
00:37:15.020 --> 00:37:22.250
Vincent ZOU: 策略，我觉得未来是会被稀释掉的，就可能从两个人出策略变成一个人出策略了，至少是这个方向去模拟的。

358
00:37:23.910 --> 00:37:25.390
Fan Yunjie: 那哪些环节是

359
00:37:25.500 --> 00:37:29.330
Fan Yunjie: 不可去替代的呢？就您电商的这个角度来。

360
00:37:30.890 --> 00:37:36.260
Vincent ZOU: 上下架运营商品，这都取消不了。现在阿里后台只要不改，我们就得用人。

361
00:37:37.300 --> 00:37:40.310
Fan Yunjie: 为什么？其实我对于这个链路不是很理解，

362
00:37:40.540 --> 00:37:41.840
Fan Yunjie: 为什么说这些。

363
00:37:41.840 --> 00:37:58.090
Vincent ZOU: 就是今天我们探讨的这个方向，里面有大量的后台工具，就是假定今天你你没有去打过阿里的后台其实或者抖音的后台或者京东的后台，其实你可能理解不了，就是可能在我的

364
00:37:58.540 --> 00:38:08.550
Vincent ZOU: 脑子里面，这些东西就是被优化掉的，但是我，你让我怎么解读它被优化？我们需要非常细化的打开每一个后台去给你看，但是这个就

365
00:38:09.150 --> 00:38:11.640
Vincent ZOU: 这不行，因为是我客户的信息嘛啊。

366
00:38:11.640 --> 00:38:12.870
Fan Yunjie: 明白，明白。

367
00:38:12.870 --> 00:38:13.810
Vincent ZOU: 对，对，对。

368
00:38:14.730 --> 00:38:18.840
Fan Yunjie: 所以你觉得人工在里面起到的角色是怎么样的。

369
00:38:19.920 --> 00:38:20.670
Vincent ZOU: 流行。

370
00:38:21.280 --> 00:38:21.680
Fan Yunjie: 就是。

371
00:38:21.680 --> 00:38:23.150
Vincent ZOU: 大量的执行，对。

372
00:38:24.190 --> 00:38:28.010
Fan Yunjie: 那将来变成了ai先算，然后人类。

373
00:38:28.010 --> 00:38:29.980
Vincent ZOU: 所以运营的执行是取消不了的，

374
00:38:30.300 --> 00:38:34.980
Vincent ZOU: 还是我那句话，我得打开后台你才能理解，但是我不打开后台，其实你理解不了。

375
00:38:36.440 --> 00:38:53.960
Fan Yunjie: 那本身我想了解一下变成了一个理想化的一个ai coordination的。这样的一个情况下，人工跟这个ai它是怎么样的一个是ai，它先去算出一个数据，然后人类去确认，然后人类再去执行是这样子吗？对。

376
00:38:53.960 --> 00:39:01.160
Vincent ZOU: 在平台没有改变的前提下，我觉得未来其实在所有的电商运营侧的人是省不掉的，

377
00:39:02.120 --> 00:39:11.740
Vincent ZOU: 很难省除了客服这个板块啊，然后这个投放这个板块是可以进行ai优化的，且你的废笔和你的投流都会逐渐的被优化掉，

378
00:39:11.840 --> 00:39:13.660
Vincent ZOU: 效率会更高。

379
00:39:14.820 --> 00:39:25.940
Fan Yunjie: 那我，我还是想就是就详细的去了解一下，这种协同的分工，就是从一个整个的工作流来讲的话，协同分工

380
00:39:26.100 --> 00:39:27.910
Fan Yunjie: 从哪从零到一。

381
00:39:30.930 --> 00:39:32.250
Vincent ZOU: 儿童分工，

382
00:39:34.720 --> 00:39:35.680
Vincent ZOU: 因为

383
00:39:37.300 --> 00:39:38.620
Vincent ZOU: and what's your role

384
00:39:40.590 --> 00:39:43.810
Vincent ZOU: 就是它。如果要上传商品的话你得用，

385
00:39:44.260 --> 00:39:49.220
Vincent ZOU: 就是说小红书的后台，那这个这是条人去传的，一个一个向上传的。

386
00:39:50.020 --> 00:40:13.000
Fan Yunjie: 我明白，就比方说，因为我之前是做marketing的，然后像我们的话，一般接到乙方在接到甲方的案子，首先他是需要去出策略，然后策略出来之后，你的creative才能去过去去做相关的这个执行creation里面会有一个比较senior的一个角色就是senior out director，

387
00:40:13.000 --> 00:40:20.080
Fan Yunjie: 它是既起到你把策略把控，又要去有非常强的sense，把你整个的这个调性给算

388
00:40:20.080 --> 00:40:35.820
Fan Yunjie: 给整合出来。我觉得这样的一个岗位，它就是完全可以被ai替代，变成了一个，因为ai你是有很强整合能力，还有你的概率性算的算法的一个能力，这个岗位完全可以替代到下面之后，人类进行执行到最后。

389
00:40:35.820 --> 00:40:42.710
Vincent ZOU: 电商没有吗？这种电商跟martin的工作其实并不相并轨，电商更多的，可是更偏

390
00:40:43.190 --> 00:40:46.650
Vincent ZOU: 偏赤。所以我的marketing其实属于true的marketing。

391
00:40:47.000 --> 00:40:50.560
Fan Yunjie: 但是他的workflow是怎样的？他本身的一个。

392
00:40:50.950 --> 00:41:09.180
Vincent ZOU: Hard work很复杂，就是说假定丢过来一个牌子，我们首先是靠经验去看，然后靠某一个品，当然相对比较成熟的品也看起来会速度比较快嘛，加上我们的行业经验，什么品类，在长什么品类在掉，某些人群在涨，有些人群在掉，原则上我们都是看不到的。

393
00:41:09.740 --> 00:41:13.740
Fan Yunjie: 卷品的这个过程有必要，必须保留人工吗。

394
00:41:14.750 --> 00:41:17.820
Vincent ZOU: 有啊，因为他品牌商可能就要求他要推这个图啊，

395
00:41:17.970 --> 00:41:20.200
Vincent ZOU: 跟我们建议的并不相符啊。

396
00:41:22.540 --> 00:41:30.990
Fan Yunjie: 但是这个品牌方的一些要求，但品牌方的要求过来，v给ai，他不可以直接去输出直接品类的这个推荐吗？

397
00:41:32.180 --> 00:41:34.380
Vincent ZOU: 不可以啊，它供应链就没这么多做呀。

398
00:41:37.760 --> 00:41:38.310
Fan Yunjie: 我的意思是。

399
00:41:38.310 --> 00:41:45.330
Vincent ZOU: 供应链，它供应链就就没有这么多货，它只能先做它自己有效或者有效去应用的这个部分。

400
00:41:45.330 --> 00:41:50.100
Fan Yunjie: 但是这个判断的依据，只要把数据给他，他就可以去输出啊。

401
00:41:52.410 --> 00:41:59.730
Vincent ZOU: 不可以，它没有供应链，它供电可能产能啊，产品啊，这都是问题，这个我们也无法接受太深。

402
00:42:00.450 --> 00:42:02.120
Fan Yunjie: 了解，就是说他的本。

403
00:42:02.120 --> 00:42:05.820
Vincent ZOU: 举个例子，我现在玫瑰做得非常好，结果突然间玫瑰就断货了。

404
00:42:06.170 --> 00:42:09.370
Fan Yunjie: 这样子其实做的有效，所有的ai都是无效的。

405
00:42:09.370 --> 00:42:15.640
Vincent ZOU: 我们就要回到他有有供应链的这个品上，那我们就要重新去看薰衣草，但薰衣草就是不行，

406
00:42:15.820 --> 00:42:16.810
Vincent ZOU: 那也没有办法。

407
00:42:17.680 --> 00:42:22.140
Fan Yunjie: 明白了解那选品之后呢？是什么样的环节？

408
00:42:22.520 --> 00:42:36.060
Vincent ZOU: 选笔之后就我们进入到运营啊，就是开始自己就人工上传啊，改图啊，修图啊，在这个过程当中可能会用一点A啊，然后这个然后就开始运营嘛？那你根据你运营设定的

409
00:42:36.060 --> 00:42:43.390
Vincent ZOU: 方案跟投放，一起看一下整个在这个渠道内部，它是怎么做投流最新方案。目前就是这样。

410
00:42:44.110 --> 00:42:47.060
Fan Yunjie: 在运营的这个过程当中，

411
00:42:47.190 --> 00:42:50.940
Fan Yunjie: 它ai大概可以替代的环节有哪些呢？

412
00:42:53.940 --> 00:42:59.400
Fan Yunjie: 一个是我刚刚可以听到一个。可能输出一些一些。

413
00:42:59.400 --> 00:43:06.690
Vincent ZOU: 图片，图片，然后比如说动图除了之类没有了，目前来讲没有了，因为我们受限于平台的要求。

414
00:43:08.440 --> 00:43:10.080
Fan Yunjie: Okay? Okay?

415
00:43:11.390 --> 00:43:15.100
Fan Yunjie: 了解，所以，本身

416
00:43:15.460 --> 00:43:24.750
Fan Yunjie: 验证出来，ai给出的投放还有策略这一块的这个这个这一块，它是放到哪一，哪哪一个步骤里了。

417
00:43:25.930 --> 00:43:28.040
Vincent ZOU: 验证平台出来的什么。

418
00:43:28.610 --> 00:43:33.920
Fan Yunjie: 就是ai。您刚刚提到，他是会可以直接给出你策略的嘛，就是你说这一块。

419
00:43:33.920 --> 00:43:34.580
Vincent ZOU: 投放策略。

420
00:43:34.760 --> 00:43:35.540
Fan Yunjie: 对，对，对。

421
00:43:35.540 --> 00:43:36.410
Vincent ZOU: 这土豆。

422
00:43:36.970 --> 00:43:40.260
Fan Yunjie: 在投放这一块，这一块是可以改掉的。

423
00:43:40.630 --> 00:43:41.890
Vincent ZOU: 对，对，对，对，对，对，对。

424
00:43:41.890 --> 00:43:42.580
Fan Yunjie: Ok.

425
00:43:42.950 --> 00:43:44.820
Vincent ZOU: 运营策略，目前没有办法。

426
00:43:45.360 --> 00:43:47.080
Fan Yunjie: 了解，了解，了解，

427
00:43:47.390 --> 00:43:54.430
Fan Yunjie: 所以系统会不会直接给出投放的这个策略的一些不确定性的这种区间呢？

428
00:43:55.420 --> 00:43:58.660
Vincent ZOU: 还不知道啊，现在还没到贝加2.0了，我们现在才1.0。

429
00:43:59.520 --> 00:44:03.990
Fan Yunjie: 他还是会有这个这个可能的，对吧，就是给出一些执行的时间。

430
00:44:04.310 --> 00:44:09.390
Vincent ZOU: 就是现在我并不能够回答的原因是因为我还没跑到这个阶段。

431
00:44:09.890 --> 00:44:10.300
Fan Yunjie: 并。

432
00:44:10.300 --> 00:44:12.570
Vincent ZOU: 对，还在还在咬嘛？

433
00:44:12.580 --> 00:44:14.840
Fan Yunjie: 他没有让他做任何策略的支出问题，

434
00:44:16.130 --> 00:44:17.580
Fan Yunjie: 明白了解

435
00:44:18.700 --> 00:44:28.240
Fan Yunjie: 本身就是您刚刚有提到，他也会可以输出直接向的这种内容，不论是这个copy，还是说这个poster，

436
00:44:28.700 --> 00:44:30.280
Fan Yunjie: 那它会不会导致。

437
00:44:30.280 --> 00:44:31.410
Vincent ZOU: 没有很少。

438
00:44:31.940 --> 00:44:32.930
Fan Yunjie: 为什么呢？

439
00:44:33.250 --> 00:44:38.440
Vincent ZOU: 这个不在我们的ai的范围里，因为现在市面上有大量的ai，我们还没开始用。

440
00:44:38.740 --> 00:44:44.870
Fan Yunjie: 我知道我知道，但是这样的一种现象，您觉得会不会让让这个行业变得同质化特别严重。

441
00:44:47.000 --> 00:44:49.120
Vincent ZOU: 我现在没看到，

442
00:44:50.020 --> 00:44:57.150
Vincent ZOU: 其实你看ai视频也好，ai内容也好，大量的一个涌现了，已经大家都在用这个方向来去驾驭。

443
00:44:59.430 --> 00:45:06.720
Fan Yunjie: 对，但是它本身的样本量还是大家使用的可能相对来说比较集中，或者说

444
00:45:07.040 --> 00:45:18.390
Fan Yunjie: 类似的数据群。所以说，不论是你给出多大的指令，很大可能上会造成你的内容或者策略是同质化，然后你将来就很难做branding的这个事情了。

445
00:45:19.480 --> 00:45:30.710
Vincent ZOU: 这个是有可能的，但是现阶段你看在全网也没有看到有什么ai出来的策略能够被取代的嘛？他可能是某一天突然爆发，但现阶段也没有发现任何

446
00:45:31.250 --> 00:45:32.510
Vincent ZOU: 这样的story。

447
00:45:33.910 --> 00:45:40.550
Fan Yunjie: 但是我觉得从这个视觉呈现上的这个角度上来讲，还是会有比较大的

448
00:45:40.880 --> 00:45:42.070
Fan Yunjie: 跟

449
00:45:42.560 --> 00:45:44.750
Fan Yunjie: 人工的差别，我觉得。

450
00:45:45.710 --> 00:46:05.010
Vincent ZOU: 这就看大家谁的ai底下嵌套，自己怎么做了嘛？那你比如说它可以用chept，它，可以用它，也可以用nobana，然后自己再融一套自自己想要的ai视频的输出，对这个完全看技术路径，未来ai在短期内应该都是技术性的，而不是

451
00:46:05.280 --> 00:46:08.900
Vincent ZOU: 就是同质化这个东西应该还是会有一定时原料的。

452
00:46:09.040 --> 00:46:10.330
Vincent ZOU: Oh, well, that is ye

453
00:46:11.520 --> 00:46:12.750
Fan Yunjie: 明白，了解。

454
00:46:12.750 --> 00:46:13.260
Vincent ZOU: 当局。

455
00:46:13.260 --> 00:46:19.630
Fan Yunjie: 所以您觉得本身现在团队来讲的话，ai融入进来，他们是变得

456
00:46:19.870 --> 00:46:26.030
Fan Yunjie: 独立性更强了，就就就员工来说，还是说，对于ai工具的依赖性更强呢？

457
00:46:26.980 --> 00:46:29.830
Vincent ZOU: 依赖情况下，这个投放程度比较明显。

458
00:46:30.850 --> 00:46:34.290
Fan Yunjie: 那会不会影响他们本身个人的一些判断。

459
00:46:35.350 --> 00:46:37.110
Vincent ZOU: 那我不知道啊，没聊过。

460
00:46:39.220 --> 00:46:39.600
Fan Yunjie: 明白。

461
00:46:39.600 --> 00:46:43.630
Vincent ZOU: 确实没聊过，我已经很久没跟他们聊过了。我刚飞过来。

462
00:46:43.630 --> 00:46:44.730
Fan Yunjie: 然后这样，

463
00:46:44.860 --> 00:46:49.480
Fan Yunjie: 那这种情况，您觉得就您自己的话会会影响你吗？

464
00:46:51.210 --> 00:46:52.260
Vincent ZOU: 不大，会影响我。

465
00:46:53.020 --> 00:46:58.470
Fan Yunjie: 不大，会影响你的原因是因为你还是要靠经验去再去verify一下。

466
00:46:59.280 --> 00:46:59.910
Vincent ZOU: 对对。

467
00:47:01.170 --> 00:47:03.030
Fan Yunjie: 了解了解。

468
00:47:03.260 --> 00:47:20.460
Fan Yunjie: 所以如果说将来团队大概95%的工作都是可以让ai cover的，然后很大程度之上，你的员工也不具备去判断ai输出内容的经验性，这样的你觉得是

469
00:47:20.590 --> 00:47:22.540
Fan Yunjie: ok的吗？可以接受的吗？

470
00:47:23.970 --> 00:47:24.740
Vincent ZOU: 可以啊。

471
00:47:26.480 --> 00:47:27.540
Fan Yunjie: 为什么呢？

472
00:47:28.320 --> 00:47:30.320
Vincent ZOU: 其实没什么原因啊，挺好的呀。

473
00:47:32.100 --> 00:47:35.100
Fan Yunjie: 那你还会花多大的成本在人工上呢？

474
00:47:37.440 --> 00:47:41.710
Vincent ZOU: 就是该用的岗就用人嘛，不该用的岗就就赶赶掉嘛。

475
00:47:42.120 --> 00:47:45.560
Vincent ZOU: 这个是一个比较简单的逻辑啊。

476
00:47:46.000 --> 00:48:01.660
Fan Yunjie: 您认为就是这本身是个趋势，就是说把不该用的岗位给他裁掉，然后也不会去在意未来的候选人，他是否能够去晋升，给到您更好的帮助。这一块就是，即使是机器人

477
00:48:01.800 --> 00:48:03.970
Fan Yunjie: 代替了95%的工作量。

478
00:48:04.700 --> 00:48:05.280
Vincent ZOU: 是的。

479
00:48:06.470 --> 00:48:17.710
Fan Yunjie: 那如何解决掉未来团队的这种portfolio就是他的分配呢？那那不就变成了完完全全可能不会有这个培养机制的一个问题了吗？

480
00:48:19.020 --> 00:48:22.620
Vincent ZOU: 那作为所有的企业，老板不应该都是好的吗？

481
00:48:24.210 --> 00:48:27.440
Fan Yunjie: 但是企业来说的话不也是应该

482
00:48:27.710 --> 00:48:33.870
Fan Yunjie: 会去有一定的机制去保证人才可持续发展的吗？

483
00:48:35.300 --> 00:48:38.310
Vincent ZOU: 我们没那么大，我们现在是创业阶段，

484
00:48:38.610 --> 00:48:40.560
Vincent ZOU: 能省钱肯定是最好的。

485
00:48:41.150 --> 00:48:44.460
Fan Yunjie: 那比方说您给到一些将来

486
00:48:44.770 --> 00:48:50.250
Fan Yunjie: ai跟这个人工协作的这种建议的话，你会有什么建议？

487
00:48:54.540 --> 00:48:57.670
Vincent ZOU: 我，我我觉得ai可能要变得

488
00:48:57.870 --> 00:49:00.980
Vincent ZOU: 正在拟人化了，现在其实还是有很多漏洞。

489
00:49:02.370 --> 00:49:04.300
Fan Yunjie: 拟人化的意思是。

490
00:49:04.680 --> 00:49:07.120
Vincent ZOU: 就更像人嘛，现在还是很像机器人。

491
00:49:08.090 --> 00:49:13.420
Fan Yunjie: 但是你人画的话，你不会介意你本身的数据隐私问题吗？

492
00:49:14.760 --> 00:49:19.220
Vincent ZOU: 不是相当于你只要用手机，你就隐私就有问题。

493
00:49:20.230 --> 00:49:29.790
Fan Yunjie: 但从伦理道德上来说的话，未来来讲这样子我觉得也不符合伦理道德吧，就是每一个人的数据都曝光在外面，然后每个人都会。

494
00:49:29.790 --> 00:49:33.600
Vincent ZOU: 除非这个世界回退到原始社会了，对吧？

495
00:49:34.780 --> 00:49:41.010
Vincent ZOU: 他只要不回退，你就一定会发生这样的问题，除非你不用手机，你只要用手机，

496
00:49:41.130 --> 00:49:45.670
Vincent ZOU: 自然而然的你就在在写，你每天都在写。

497
00:49:46.470 --> 00:49:52.400
Fan Yunjie: 那这样的情况之下不会，未来很大程度上人就会被ai替代掉吗？

498
00:49:53.430 --> 00:49:55.260
Vincent ZOU: 那应该我死之前看不到了。

499
00:49:55.590 --> 00:49:56.610
Fan Yunjie: 哈哈

500
00:49:58.370 --> 00:49:59.080
Fan Yunjie: ok.

501
00:49:59.080 --> 00:50:02.320
Vincent ZOU: 我，我并不是很care，还有来生这件事情。

502
00:50:03.400 --> 00:50:07.590
Fan Yunjie: 但是如如果小孩呢，小孩很大程度上可能就会。

503
00:50:07.590 --> 00:50:09.120
Vincent ZOU: 上海，所以我也不care。

504
00:50:09.310 --> 00:50:11.290
Fan Yunjie: 一，一旦有呢。

505
00:50:11.640 --> 00:50:13.130
Vincent ZOU: 那等有了再说吧。

506
00:50:14.640 --> 00:50:15.390
Fan Yunjie: 好吧，

507
00:50:15.560 --> 00:50:16.180
Fan Yunjie: 好吧。

508
00:50:16.180 --> 00:50:18.110
Vincent ZOU: 以前倒是没有的

509
00:50:19.080 --> 00:50:20.300
Vincent ZOU: 明白。

510
00:50:20.570 --> 00:50:25.180
Fan Yunjie: 但是您最期待的话，您最期待的话，ai还是

511
00:50:25.480 --> 00:50:28.880
Fan Yunjie: 有一些哪些能力的突破。除了个性化，

512
00:50:29.460 --> 00:50:32.300
Fan Yunjie: 或者说人就是personalized这一块

513
00:50:32.570 --> 00:50:39.030
Fan Yunjie: 还会有哪些的能力突破？您认为是最大程度上辅助您的工作，或者说是

514
00:50:39.330 --> 00:50:42.100
Fan Yunjie: 是您比较理想的一个ai形态。

515
00:50:43.010 --> 00:50:45.840
Vincent ZOU: 游戏吧。如果游戏未来都是ai引擎就比较好玩，

516
00:50:46.340 --> 00:50:48.630
Vincent ZOU: 我除了这个，我没有什么期待。

517
00:50:51.180 --> 00:50:58.310
Fan Yunjie: 了解，但是游戏的话现在有有很多应该是有做了吧，就是那种陪玩啊之类的。

518
00:50:58.700 --> 00:51:08.960
Vincent ZOU: 那没用了。我是说那种真正的在ai的什么魔术啊什么的发生那种巨量级的改变，不是这种跟人没关系啊。

519
00:51:09.920 --> 00:51:13.130
Fan Yunjie: 明白，所以你也完全不care。

520
00:51:13.130 --> 00:51:13.810
Vincent ZOU: 七的人。

521
00:51:14.230 --> 00:51:24.390
Fan Yunjie: 人家完全不care出这种ai的，像您刚刚所说，这种需要特别大算力了，那种这么大算力造成的一些能源的负担。这种你也完全没有想过是吗？

522
00:51:26.620 --> 00:51:29.230
Vincent ZOU: 我其实还真没有思考过这个问题。

523
00:51:30.740 --> 00:51:31.630
Fan Yunjie: 了解

524
00:51:31.800 --> 00:51:33.790
Fan Yunjie: 可以。Okay，

525
00:51:33.920 --> 00:51:37.390
Fan Yunjie: 我觉得差不多了，我的问题都问完了。

526
00:51:37.390 --> 00:51:38.740
Vincent ZOU: 你这个好厉害

527
00:51:38.930 --> 00:51:40.010
Vincent ZOU: 还是很牛的。

528
00:51:41.180 --> 00:51:43.950
Fan Yunjie: 没有啊，我先stop recording一下啊。

529
00:51:44.490 --> 00:51:46.170
Vincent ZOU: 你们这个非常不错。



受访人31:
WEBVTT

1
00:00:00.000 --> 00:00:02.180
Fan Yunjie: 我们可以把摄像头关掉了，

2
00:00:10.470 --> 00:00:12.090
Fan Yunjie: 可以关掉摄像头

3
00:00:13.050 --> 00:00:14.510
Fan Yunjie: 可以关掉吗？

4
00:00:16.379 --> 00:00:17.050
Fan Yunjie: ok.

5
00:00:17.050 --> 00:00:23.460
xinyan: okay。我觉得我其实是在于你所描述的两类人群之间。

6
00:00:23.600 --> 00:00:39.290
xinyan: 首先我是一线的生产者，然后呢，我是对新技术的乐观乐观者，就是我很去很愿意去体验，或者是去使用一些新的技术。这是我的立场。然后目前所有的ai的，

7
00:00:39.380 --> 00:00:51.420
xinyan: 图声纹的一些工具，因为我是设计师嘛。然后我也是执行者，也是管理者，所以我对ai生成图片以及ai管理系统的一些逻辑，

8
00:00:51.420 --> 00:01:01.170
xinyan: 都有过一些自己的体验。我目前使用下来的结果。结论是，我可以依赖ai去完成一些指定的指令，

9
00:01:01.730 --> 00:01:05.400
xinyan: 然后这个指令一定是under在我大的框架之下的，

10
00:01:05.660 --> 00:01:08.030
xinyan: 比如说我一个大的project

11
00:01:08.170 --> 00:01:17.650
xinyan: 是让所有呈现在屏幕上的观众有更好的质感，那么我会在这个里面去找一些ai

12
00:01:18.280 --> 00:01:35.470
xinyan: 可以直接帮助我的点。我给你发的图里面其实包括一个就是我以前的工作当中会遇到非常多的卖家讲师的视频在网上去传播，所以我我要去处理。很多讲师在屏幕前有更好表现的一个一个。

13
00:01:36.520 --> 00:01:42.910
xinyan: 所以我们当时找到了一个点就是讲师在录评的时候，他其实是要阅读大量的资料或者背下来，

14
00:01:43.040 --> 00:01:45.820
xinyan: 那么这个时候如果我能用ai

15
00:01:45.990 --> 00:01:52.330
xinyan: 去让去矫正他的眼球就是ai同时去矫正他的眼球，让他去读那些，

16
00:01:53.530 --> 00:02:09.900
xinyan: 那么这个指令就可以大幅度的提高，我们的生产效率就是我们可以直接录播了，就是你可以边读你的稿子边去录播。同时我可以把你的眼球矫正对着那个摄像头去看，那么这个指令呢？它是很成功的，

17
00:02:09.900 --> 00:02:20.420
xinyan: 就一下子解决了所有的。你被稿背不下来，或者表达不自然的一个问题，但它是under在我整个就是我要去整体提升我的

18
00:02:20.590 --> 00:02:33.520
xinyan: 卖家讲师的一个表现状态的一个里面，我不会让ai去做一个大的事情，但在这个框架之下，它可以很好的去处理某一个点，然后这个点是在ai出来之前，我是完全没有。

19
00:02:38.600 --> 00:02:39.310
Fan Yunjie: 喂。

20
00:02:40.550 --> 00:02:41.470
xinyan: 诶，听到吗？

21
00:02:41.470 --> 00:02:42.050
Fan Yunjie: 挺好的。

22
00:02:42.050 --> 00:02:42.610
xinyan: 看一下啊。

23
00:02:42.610 --> 00:02:57.650
Fan Yunjie: 可以听到。所以其实，我其实有点好奇，因为你，您其实是在creative这个领域，或者说是在这个这个领域就已经有很多很多年了，对吧？大概的这个。

24
00:02:57.650 --> 00:02:58.180
xinyan: 对对。

25
00:02:58.330 --> 00:03:03.690
Fan Yunjie: 从哪一年开始去involve in到creative这个领域。

26
00:03:05.140 --> 00:03:10.240
xinyan: 我其实从11年前，因为我学的就是广告设计与传播这个专业。

27
00:03:10.600 --> 00:03:19.540
xinyan: 所以我毕业以后就一直在agency里面去工作。大陆这边的agency，我其实刚开始也是在服务系统里面去工作的，比如说bboo。

28
00:03:22.080 --> 00:03:31.380
xinyan: Base这个比较老牌的公司，但他现在已经离开大陆了。从刚开始进入广公司的时候，我们其实做很传统的事情，比如说像我，

29
00:03:31.530 --> 00:03:48.760
xinyan: 我刚开始工作是做必胜客的大菜单的项目。就中国大陆地区所有的大菜单的设计，所以当时我们设计的时候，为了让客户更能体验菜单翻页的感觉。我们用两张A3的纸就是会把做完的layout打印出来，

30
00:03:48.950 --> 00:03:53.010
xinyan: 然后正反贴贴成一本皱皱巴巴的那种。

31
00:03:53.210 --> 00:03:57.070
xinyan: 像像捧在手里的菜单去让客户去翻。

32
00:03:57.720 --> 00:03:58.160
Fan Yunjie: 就是。

33
00:03:58.160 --> 00:04:07.640
xinyan: 然后现在就很不一样，因为我接触过纸媒的一些末端的一些设计。现在过渡到了ai的部分，大概在

34
00:04:07.740 --> 00:04:12.620
xinyan: 我觉得应该在一两年前吧，我就开始使用所有的ai项目就慢慢接触。

35
00:04:13.880 --> 00:04:25.520
Fan Yunjie: 那你觉得就回顾从方A到amazon就整个ai工具在这一两年出现之后对你工作的你觉得思路或者方法会带来很大的改变吗？

36
00:04:26.750 --> 00:04:39.940
xinyan: 会有会有很大的改变，尤其是在效率方面，就是有一些以前可能只是想象无法完成的事情。现在我可以通过一个指令让它快速的去生成，

37
00:04:40.160 --> 00:04:53.860
xinyan: 比如说以前做tvc或者是做dvc的分进kicko的时候，你有很多的很fancy的一些想法，比如说，尤其是做skincare这个部分，它有很多的rt

38
00:04:54.800 --> 00:05:01.720
xinyan: 皮肤组织，然后水泼在上面，然后弹开这种感觉其实你很难去。

39
00:05:02.010 --> 00:05:15.800
xinyan: 想象，或者是再画出来。但现在，如果你把这个指令丢给ai，它能给你一些东西，它这个东西不会是你最终完成的东西，但它会帮你开阔很多思路，提供很多的options。

40
00:05:16.790 --> 00:05:29.980
Fan Yunjie: 但是我想更大程度上去了解ai目前在大部分创意工作者，他的一个使用的第一个要求是效率，还是说创新。

41
00:05:31.480 --> 00:05:40.240
xinyan: 我觉得是效率，因为我，我觉得ai很难去创新ai所有的生成的东西，它都是基于

42
00:05:41.600 --> 00:05:55.420
xinyan: 已有的东西去去给你的。所以我在。以前带团队去做东西的时候，因为我们做art嘛，要找很多的reference，这个reference其实是帮助你去看多看更多不同的可能性的。

43
00:05:55.450 --> 00:06:03.180
xinyan: 就在找reference的过程当中，我会非常严厉的要求所有参与这个项目的人不允许去碰ai，

44
00:06:03.240 --> 00:06:08.460
xinyan: 因为如果你用ai去给你找reference，那么它生成的指令是直线的，

45
00:06:08.700 --> 00:06:19.390
xinyan: 就它是推导出来的，所以你可能只有一个或者两个结果，但是如果你去找，你会碰到一些不一样的东西，就它的link会非常的

46
00:06:20.390 --> 00:06:21.860
xinyan: 不可预测。

47
00:06:22.830 --> 00:06:25.790
Fan Yunjie: 这种不可预测是不好的事情吗？对。

48
00:06:26.730 --> 00:06:32.620
xinyan: 对我们来说是非常好的事情，尤其是在你做的时候，比如说你

49
00:06:33.930 --> 00:06:52.240
xinyan: 个水果爆炸开的感觉，那么如果你让ai去帮你生成它，就会给你一个水煮水果爆炸开的感觉。但如果你自己去，尤其是像pinterest之类的网站，你去找一个，你也不要去直接输入爆炸开的这种这种关键词，

50
00:06:52.280 --> 00:06:56.340
xinyan: 它会给你更多的可能性。比如说给你不同画风的爆炸，

51
00:06:56.550 --> 00:06:59.580
xinyan: 不同可能性的爆炸或者是不同。

52
00:07:00.770 --> 00:07:10.830
xinyan: 它会跳出爆炸这个词去给你一些别的东西，你会看到别人做的更好的东西，然后你再去把爆炸这个这个想要表达的东西，去link它

53
00:07:10.950 --> 00:07:12.980
xinyan: 找到的另外一张图的感觉。

54
00:07:14.690 --> 00:07:29.940
Fan Yunjie: 我想问一下，就目前就专业的，不论甲方还是乙方团队的这种创意类目的从事者来讲，大家使用ai的，或者说是ai去承担多少百分比的工作。

55
00:07:31.100 --> 00:07:41.310
xinyan: 我觉得在大陆这边。你去表达你正在和ai一起工作，是一个非常加分的项目，无论是对个人还是对公司，

56
00:07:41.350 --> 00:07:53.840
xinyan: 但是在实际执行的过程当中，人永远是决定性的因素，就是你有一个框架，你知道该怎么去做，然后你你懂得把一部分的工作去分配给ai，让ai

57
00:07:54.060 --> 00:07:58.220
xinyan: 在你的框架之下完成这个事情，这是关键。

58
00:07:58.340 --> 00:08:00.880
xinyan: 所以ai不是主导人才，是

59
00:08:00.980 --> 00:08:03.670
xinyan: 要描述清楚你要什么才是最重要的。

60
00:08:05.070 --> 00:08:13.420
Fan Yunjie: 我想了解一下，比方说，就一个project过来之后，大部分的一个线性的工作流是怎样分配的？

61
00:08:14.490 --> 00:08:26.810
xinyan: Okay，我是做branding的，所以一个大的brief。来到我这里以后，首先我会想到，作为我的这个品牌，我需要用什么样的方式去表达我的视觉。

62
00:08:27.000 --> 00:08:43.400
xinyan: 比如说我是一个自然主义的，那么我的图片和人的状态一定是natural的，所以我会首先摒弃掉大修图，那种skin贴修到那种毛孔都没有的那种图片，所以这不是我们要的。

63
00:08:43.400 --> 00:08:54.400
xinyan: 所以我会找一些mobile方向去给清楚我们要的感觉。比如说我要一张自然肌肤的脸上有毛孔的，然后有阳光打在脸上的，

64
00:08:54.490 --> 00:08:56.410
xinyan: 然后finance支付的

65
00:08:56.560 --> 00:09:05.980
xinyan: 这样这个指令框选了一个范围之后，然后我会借助ai的一些算力去帮我更快的去生成一个我要的一个感觉，

66
00:09:06.440 --> 00:09:08.650
xinyan: 而不是以ai先先入手。

67
00:09:09.910 --> 00:09:16.830
Fan Yunjie: 明白，所以说更大程度之上，ai在这个过程当中是只起到了一个工具的作用是吗？

68
00:09:17.270 --> 00:09:19.450
xinyan: 只起到工具和辅助的作用。

69
00:09:20.110 --> 00:09:23.030
Fan Yunjie: 您觉得他还会去帮您

70
00:09:23.490 --> 00:09:35.570
Fan Yunjie: 有一些灵感迸发的作用吗？除了你刚刚所说的，可能在一些视觉的呈现之上，如果说就是在一开始的这种initial的这种insights方面，他会有这样的帮助吗？

71
00:09:36.890 --> 00:09:46.510
xinyan: 我觉得我不依赖ai去帮我扩大我的范围，但如果他能给我一些，那是最好的，我觉得他可以更快的去。

72
00:09:46.570 --> 00:09:50.760
xinyan: 呈现我想要表达的像，比如说我有

73
00:09:50.770 --> 00:10:10.450
xinyan: 十个方向想去做test。这个时候如果有ai，我可以很快的去完成这十个test。如果没有ai，我可能花很多时间去做，那么我只能去完成五个test，就我要筛选哪五个最重要，但有ai，它会更快去帮助我，但它不会帮我去扩大。我本来就有的一些范围。

74
00:10:10.940 --> 00:10:20.610
Fan Yunjie: 明白明白。所以说，本身现在做branding也好，还是说就是甲方自己输出一些一些啧

75
00:10:20.650 --> 00:10:35.710
Fan Yunjie: 内容。也好吧，我觉得大家都在去考量，或者说担心一个事情就是同质化的问题。就ai同质化的问题，那在于这个您作为一个creater，您觉得这个东西是完全可以避免的，还是说也会有比较大的这个担忧。

76
00:10:36.920 --> 00:10:52.110
xinyan: 我觉得这个是一个非常严肃的问题，这也是前两年我开始转做branding的一个重要的原因，因为我觉得ai生成的东西，它就是同质化的。那么你给的。

77
00:10:52.320 --> 00:10:53.460
xinyan: prime.

78
00:10:54.980 --> 00:11:02.640
xinyan: 最开始的地方，先让别人知道你想产生的东西和别的品牌有什么不一样，这个才能最大程度上

79
00:11:02.790 --> 00:11:08.610
xinyan: 去呈现你品牌的一些特征。我觉得这个问题真的非常值得研究，因为

80
00:11:09.000 --> 00:11:13.320
xinyan: 我去了解过一些卖家因为在中国，深圳有很多

81
00:11:14.110 --> 00:11:19.090
xinyan: 卖产品去国外的一些小的企业主，比如说

82
00:11:20.030 --> 00:11:23.230
xinyan: 或者是卖一些工具文具类的，

83
00:11:23.610 --> 00:11:30.420
xinyan: 他们有了ai以后，他们第一个想法就是可以fire掉他们的设计师，因为好像

84
00:11:30.550 --> 00:11:33.050
xinyan: 你给chrome，给那个a

85
00:11:33.370 --> 00:11:50.500
xinyan: 就可以为你生成一个图片，这已经就够了，所以他们已经拿到他们的结果了，但他没有想到的。没有想到的是，他们给出去的，所有的企业主都在给，所以他们缺少了的一个部分，就他们branding本身是没有一些特征的，

86
00:11:50.610 --> 00:11:59.220
xinyan: 所以他们给出去的关键词也是都一样的，所以他们都拿到一样的图，这个在比如说一个电商页面，你去搜索一个whats，

87
00:11:59.600 --> 00:12:12.180
xinyan: 那么可能有十家里面有六家都在用同样的去创作一些女生穿著袜子在沙发旁边的这种一个场景。那么？如果有一些branding的设定，

88
00:12:12.220 --> 00:12:30.260
xinyan: 比如说你是在什么场景下去穿这个袜子，比如说你是在舞会的场景中穿蕾丝的袜子，这是你品牌的调性。如果你有这个最开始branding的一个设定，那么ai会非常好的帮助你去设去创造出一些你所需要的东西。

89
00:12:32.000 --> 00:12:32.800
xinyan: 统治化。

90
00:12:34.140 --> 00:12:39.820
Fan Yunjie: 你，你，你有没有去接受过engineering，或者说自己学过这个东西呢？

91
00:12:40.850 --> 00:12:41.530
xinyan: 没有。

92
00:12:42.250 --> 00:12:42.790
Fan Yunjie: 没有。

93
00:12:42.790 --> 00:12:44.780
xinyan: 是一个ai的使用者。

94
00:12:45.160 --> 00:12:48.810
Fan Yunjie: 没有，那您是怎么去真正意义上区分

95
00:12:49.040 --> 00:12:56.700
Fan Yunjie: 品牌化的pron和这种同质化的prone自己在指定的方向如何去给它更好的区分。

96
00:13:01.480 --> 00:13:05.100
xinyan: 我觉得这个可能是branding的思维去帮助我

97
00:13:05.170 --> 00:13:22.230
xinyan: 了解到在ai的时代，你更容易去创作出一个主图片，但同时，你也更容易去创作出一个和别人一样的图片这个问题，因为做branding，它本身就是在顶层给客给自己的一个品牌定位，视觉定位。

98
00:13:24.250 --> 00:13:25.820
Fan Yunjie: 明白，

99
00:13:26.150 --> 00:13:40.980
Fan Yunjie: 但是我有一个concern就是，其实要是说品牌千千万的话，那竞品也一一大堆呀，那会不会有大家同样的竞品，目前因为一致的这种工具型的输出，

100
00:13:40.980 --> 00:13:47.880
Fan Yunjie: 而而导致了在同精品过程当中很难去有这个usp或者ksp这种这种问题。

101
00:13:48.860 --> 00:13:56.400
xinyan: 我觉得有的。所以在品牌创立的初期，你一定要划分好自己的一些品牌定位，人群划分，

102
00:13:57.120 --> 00:14:01.860
xinyan: 有过一个简单的例子就是，有一个卖

103
00:14:02.230 --> 00:14:07.050
xinyan: 微波炉的，还是一个什么品牌的一个客户，

104
00:14:07.250 --> 00:14:14.960
xinyan: 他想描述的一个使用场景是，在美国小镇，妈妈早上起来为孩子

105
00:14:15.160 --> 00:14:20.520
xinyan: 做早饭的那个过程，所以我觉得它的品牌是有一些自己的温度在里面的，

106
00:14:20.550 --> 00:14:39.650
xinyan: 你有自己想表达的故事，在里面的他有了这个定位以后，他去下这个promp给ai，他其实创造出来的东西是很温暖的是符合他们品牌的，会完全的和别的，比如说做其他品牌的微波炉的一些，一个一个品牌有区分开，因为你是有故事在里面的。

107
00:14:42.290 --> 00:14:46.720
Fan Yunjie: 那本身。如果说已经把这个调性定好之后，

108
00:14:46.870 --> 00:14:53.720
Fan Yunjie: ai还是可以更大程度上辅助品牌的。这种一致性产出的是吗？就是在日后的工作。

109
00:14:54.470 --> 00:14:56.900
xinyan: 对，没错，我觉得ai的帮助非常的大。

110
00:14:57.810 --> 00:15:02.760
Fan Yunjie: 在这种一致化产出的这个方向，它的帮助能有多大？

111
00:15:04.260 --> 00:15:16.380
xinyan: 首先更快，这个毋庸置疑吧。现在ai，纹身图或者是纹身，一些短视频的能力和算力都在很快的提升。

112
00:15:17.770 --> 00:15:25.510
xinyan: 其次是当你有了更精准的prompt，更精准的品牌定位以后，它能生成你品牌所

113
00:15:25.950 --> 00:15:32.620
xinyan: 不可被替代的一些部分。如果你有自己的故事，你可以完全去表达出一个你自己的品牌来。

114
00:15:33.850 --> 00:15:47.420
Fan Yunjie: 明白。所以您觉得现在就艺术创作者或说是就视觉创作者来说，需要去接受很专业的ai培训吗？还是说他们应该是带着自己的insights去使用ai

115
00:15:47.530 --> 00:15:49.800
Fan Yunjie: 这种相关的

116
00:15:50.010 --> 00:15:55.110
Fan Yunjie: training你觉得有必要吗？还是说，更大程度上保留创业者自己的想法。

117
00:15:57.310 --> 00:15:59.160
xinyan: 我觉得

118
00:15:59.330 --> 00:16:01.910
xinyan: 对ai使用的培训。

119
00:16:02.400 --> 00:16:12.880
xinyan: 这个其实肯看个人，因为我觉得如果需要想要去了解的人，他自然就会去了解。然后我觉得在这个情况之下，

120
00:16:13.010 --> 00:16:23.550
xinyan: 更多的是要去增加一些对人的洞察，然后你对人的洞察编成故事，这个时候你才能去给aa下chrome，不是吗？

121
00:16:24.890 --> 00:16:31.290
Fan Yunjie: 没错，确实，但是本身针对于一个企业，一个或者说是一个team leader来说的话，

122
00:16:31.290 --> 00:16:47.760
Fan Yunjie: 对于一些比较新的一些candidates，他们刚进入到这个行业里面，你会给到他们的建议是说你要去学ai，或者说是以一个什么样的方式去训练ai，还是说你更大程度上想看他们自己的活星。

123
00:16:49.100 --> 00:17:01.980
xinyan: 我觉得在企业里的一个好处是，你可以享受到别人使用ai的一个结果就是ai的可能性你可以快速的拿到，比如说现在的它已经解决了，

124
00:17:02.330 --> 00:17:03.380
xinyan: 就是

125
00:17:03.530 --> 00:17:12.079
xinyan: 就是单个物体，你要你下多个trun，它的表情或者是主体是不会变的。这个状况，你要知道ai可以做到这个了，

126
00:17:12.150 --> 00:17:24.490
xinyan: 然后你要去接触不同领域的人就是比如说做数据data的。人其实ai可以在另外一个方面给你更多的启发嘛。然后你去接触一些creative的人，他可以告诉你，a，ai现在可以

127
00:17:24.660 --> 00:17:28.710
xinyan: 给指令，什么样的指令，可以生成什么样的视频或者是图文，

128
00:17:28.980 --> 00:17:35.600
xinyan: 我觉得需要有一个分享的机制，让大家都知道ai的能力到哪里了。

129
00:17:37.460 --> 00:17:39.780
Fan Yunjie: 但是其实我觉得

130
00:17:40.010 --> 00:17:53.890
Fan Yunjie: 我其实有一个比较大的concern在于做coercial这个维度的这个creative，它更大程度上是要统一标准，还是说要要有更大程度上的一个本地创意自由行呢？

131
00:17:57.490 --> 00:18:00.080
xinyan: 关于coercial这个部分，我觉得

132
00:18:00.370 --> 00:18:02.510
xinyan: 当然。

133
00:18:02.720 --> 00:18:12.890
xinyan: 需要去更多的去了解某一个品牌的调性，因为现在在agency里面任何一个creative team，他不可能只服务于一个客户的，

134
00:18:13.180 --> 00:18:19.170
xinyan: 比如说我在org的时候，我会同时去做可口可乐的新年campaign，

135
00:18:19.330 --> 00:18:23.080
xinyan: 同时也会去处理alex的国外的一些case。

136
00:18:23.950 --> 00:18:33.240
xinyan: 这个时候，如果你作为一个设计师，创意者，你要去很快的做一些东西，你要给出一些很精准的词，就是这个品牌是什么样子，

137
00:18:33.560 --> 00:18:35.630
xinyan: 另外一个品牌是什么样子？

138
00:18:38.050 --> 00:18:55.730
Fan Yunjie: 那本身针对于。但是这样的话，其实我还是有一个就是针对标准化的，因为我们其实一直我感觉啊。在我认为商业化的这个输出标准，跟一个艺术来讲的话，它还是有差别的，对吧？但是又。

139
00:18:55.730 --> 00:18:56.230
xinyan: 是对。

140
00:18:56.420 --> 00:19:08.730
Fan Yunjie: 艺术本身，或者说是创意本身又是不可缺少的。在这个过程当中。所以如果说ai真的在未来很大程度之上解决，或者说是

141
00:19:08.730 --> 00:19:17.940
Fan Yunjie: 帮助辅助了30%甚至50以上的工作。那如何平衡这个关系呢？就是创意性以及标准度的这个问题，对。

142
00:19:19.320 --> 00:19:28.810
xinyan: 我觉得这个很值得探讨，因为我觉得艺术是设立标准去创造一些你没有见过的体验，比如说，

143
00:19:29.050 --> 00:19:38.070
xinyan: 在镜头记录下鱼打在皮肤上的感觉。之前ai是不会知道怎么去创造一张这样的图的，

144
00:19:38.260 --> 00:19:50.760
xinyan: 所以我觉得每一个art都需要去追求一些很真实的体验。在我刚入行的时候，如果我想知道火烧过纸的痕迹，我是需要去

145
00:19:51.030 --> 00:20:00.060
xinyan: 去点一把火，去烧了纸，然后用相机记录下来，然后去学习火在指头上这种飘渺的感觉。

146
00:20:00.070 --> 00:20:02.910
Fan Yunjie: 然后记录下来以后我才知道，

147
00:20:03.080 --> 00:20:13.710
xinyan: 那我想去创造一个火焰，这样子才是符合自然的。这样子是假的我就会知道。我觉得coercial和所有的部分。

148
00:20:14.590 --> 00:20:20.860
xinyan: 说实话，他在调取很多已经有的元素去重新的拼接和组合，

149
00:20:20.980 --> 00:20:22.510
xinyan: 在广公司

150
00:20:23.150 --> 00:20:35.190
xinyan: 很少需要真正的创意人，很少需要真正的艺术家去创造一些没有的东西。当当然我不否定，比如说，如果你是想念的creative director，

151
00:20:35.260 --> 00:20:45.670
xinyan: 你有你有这个义务去创造一些别人，别人没有见过的事情，比如说你用折纸去创造一个小镇，然后相机穿过这个小镇的感觉

152
00:20:45.910 --> 00:20:52.540
xinyan: 你有这个预算，也有这个时间，也有这个品牌调性去做这个事情。但如果你是一个

153
00:20:52.560 --> 00:21:06.360
xinyan: 比较local或者是评价版的一个一个美妆品牌，你可能就是要学习一些表达方式而已，然后去调取一些已经有的一些东西去创造出你品牌的一些特征就可以了。

154
00:21:07.030 --> 00:21:26.330
Fan Yunjie: 明白了解就是您刚刚有提到一个，就是在最初您去接触到这个行业的时候，您的灵感来源跟现在的，这种发生了很大的同质性的变化，那比方说ai给出的现在视觉的标准，在最一开始的时候

155
00:21:26.480 --> 00:21:32.740
Fan Yunjie: 在跟您的创意直觉，如果说是相冲突的话，这该怎么办？

156
00:21:35.410 --> 00:21:44.490
xinyan: 那就再给一些精准的problem或者是给一些比较精准的reference，让那个ai去重新学习一下。

157
00:21:44.670 --> 00:21:46.900
Fan Yunjie: 我觉得这个可能是要。

158
00:21:46.950 --> 00:21:54.060
xinyan: 经历过那些那些事情的人才会有这个去判断。所以这也是为什么我们后面招了很多新的？

159
00:21:55.270 --> 00:22:02.710
xinyan: 实习生，或者是刚入行的一些做art的同事，我觉得对他们来说有点不公平，因为他们

160
00:22:03.090 --> 00:22:06.910
xinyan: 他们现在可以更快的去完成一个60分的东西。

161
00:22:07.630 --> 00:22:21.810
xinyan: 但对他们未来是一个非常不好的一个状态，因为我们现在已经默认他没有一个，就我们默认现在的at不能做到60分以下，所以我们不给他一个沉默的时间，像我们刚入行的时候，

162
00:22:21.910 --> 00:22:23.280
xinyan: 可能一两年

163
00:22:23.740 --> 00:22:35.130
xinyan: 公司都是默认我需要培养你的，然后一年以后，你才开始真正去做一些东西，但现在所有刚毕业的人，他其实可以很快通过ai去完成一个还ok的东西。

164
00:22:35.130 --> 00:22:38.470
Fan Yunjie: 但这对他们的上限是有巨大的一个challenge。

165
00:22:39.030 --> 00:22:42.790
Fan Yunjie: 我这个challenge，它是

166
00:22:43.170 --> 00:22:45.290
Fan Yunjie: 它都取决于什么呢？

167
00:22:45.500 --> 00:22:46.240
Fan Yunjie: 或者说衡量。

168
00:22:46.240 --> 00:22:47.460
xinyan: 它就是比如说，

169
00:22:47.790 --> 00:22:50.100
xinyan: 比如说，他现在可以可以

170
00:22:50.450 --> 00:22:53.820
xinyan: 三秒钟生成一张火焰燃烧的感觉。

171
00:22:54.280 --> 00:23:06.910
xinyan: 但火焰燃烧的感觉，它是基于以前的，有的图片去拷贝过来的，所以他不知道这个火焰是不是够，真是不是够假，他没有一个别的判断了。

172
00:23:08.140 --> 00:23:10.460
Fan Yunjie: 但是这样的一个问题。

173
00:23:11.350 --> 00:23:16.620
Fan Yunjie: 现在针对于新的候选人来讲，我想知道他涵盖的比率又有多少。

174
00:23:17.890 --> 00:23:34.280
xinyan: 我觉得很多学校也在注重ai和实际体验的一个balance。现在教育也在改革嘛。所以目前这个阶段其实目前，比如说你现在招收一个大学刚毕业的人，他四年前可能没有ai的这个概念，

175
00:23:34.430 --> 00:23:48.200
xinyan: 所以他其实接触过一些理论的教育的，这个时候他要适应一个从没有ai到ai的这个转变有点困难，可能慢慢的，这个教育系统也会注重去培养一些孩子的。

176
00:23:48.360 --> 00:23:52.970
xinyan: 新的年轻人的一些感官上的体验，或者是

177
00:23:54.250 --> 00:23:57.610
xinyan: 一些比较真实的体验嘛，怎么去跟未来这个

178
00:23:57.860 --> 00:23:59.250
xinyan: 去做一个balance。

179
00:24:00.810 --> 00:24:10.300
Fan Yunjie: 但是这个其实是有冲突性的在于，就像您说的，您见过火焰燃烧真正的样子。所以说，您知道如何去

180
00:24:10.440 --> 00:24:27.520
Fan Yunjie: 考量他，是因为你有这个经验，才能去带着这种批判性的思维去思考。但现在如果说已经是ai时代开始进入到行当的孩子了，他们怎么样去，就是有有激发性的可以有自己的这种思维呢？

181
00:24:28.880 --> 00:24:34.210
Fan Yunjie: 除了说在学校，可能现在学校也不太会注重这一方面的培训。如果说

182
00:24:34.570 --> 00:24:44.340
Fan Yunjie: 没有意识到这个问题，因为我觉得现在很大程度上还是在推进ai发展，而不是限制ai发展。虽然我们在做限制ai发展的事情。

183
00:24:44.840 --> 00:24:45.410
xinyan: 对

184
00:24:46.160 --> 00:24:49.370
xinyan: ok，我觉得这个我其实没有很

185
00:24:49.760 --> 00:24:54.100
xinyan: 很很仔细的去考虑过这个问题，而我有想过，比如说

186
00:24:54.250 --> 00:25:03.440
xinyan: 自己未来的子女怎么样去培养他，让他更容易去适应这个ai的时代。我脑子里最最直接的答案就是让孩子去。

187
00:25:04.700 --> 00:25:08.260
xinyan: 去更多的接触真实环境中的世界。

188
00:25:08.810 --> 00:25:12.590
xinyan: 因为我，我跟我的妻子有过做做brainstorming就是

189
00:25:13.820 --> 00:25:17.670
xinyan: 就是记得有以前有本书，他说，以后可能

190
00:25:18.920 --> 00:25:25.420
xinyan: 吃牛肉会变成一个很奢侈的事情，因为现在ai的发展包括

191
00:25:25.670 --> 00:25:37.120
xinyan: 真实的产品可能会会被替代。就是说，像有些蛋白质工厂正在合成一些人工的蛋白质，那么以后可能你能吃到的牛肉，它也不会是真的。

192
00:25:40.570 --> 00:25:43.860
Fan Yunjie: 但是其实这有一个问题在于，

193
00:25:44.070 --> 00:26:04.180
Fan Yunjie: 我们本身的小朋友现在已经是在了这样的一个年代里面了。就是就像我们当初经历了那个搜索引擎刚刚刚刚开始的时候，这也算是一个时代变革，我觉得这个其实只是一个顺应的事情，而不是说是真正意义上要要跟他唱反调的一个事情，也不存在

194
00:26:04.180 --> 00:26:05.480
Fan Yunjie: 这样的现实，

195
00:26:05.480 --> 00:26:10.440
Fan Yunjie: 除非说本身自己有非常强的这种批判性思维的能力，

196
00:26:10.440 --> 00:26:22.450
Fan Yunjie: 说是对于数据处理有很强的主观性，我觉得才会去。对于ai这个事情抱有疑问，但是大部分的受众我相信90%甚至以上的受众都是在对

197
00:26:22.450 --> 00:26:27.230
Fan Yunjie: 默认ai去潜移默化自己的生活。那你觉得如果说

198
00:26:27.230 --> 00:26:42.500
Fan Yunjie: 通过互动来说吧，或者说通过您跟那么多产品的这个接触ai类的产品接触来讲，你觉得如果说从产品层面上去给它一定的迭代或者更新，或者优化，怎么能够去

199
00:26:42.500 --> 00:26:47.560
Fan Yunjie: 让人类在使用它的时候，不要去摒弃掉自己的思考。

200
00:26:51.660 --> 00:26:52.610
xinyan: 哼，

201
00:26:53.590 --> 00:26:55.900
xinyan: 不要去摒弃掉自己的思考。

202
00:26:56.140 --> 00:26:56.520
Fan Yunjie: 对。

203
00:26:56.520 --> 00:26:57.980
xinyan: 是什么意思？

204
00:26:58.640 --> 00:27:04.660
Fan Yunjie: 就是说不要去过分的相信或者依赖，或者说直接上的

205
00:27:04.720 --> 00:27:20.020
Fan Yunjie: 这样说吧，比方说，我想问您一下，您再去让ai给您输出一个结果的时候，您是输出完之后直接会使用，还是说会给它一步或者两步的进一步的优化。这样对吧？

206
00:27:21.390 --> 00:27:23.590
xinyan: 我会验算。同时我会修改，

207
00:27:24.020 --> 00:27:32.290
xinyan: 因为我觉得ai它它是逻辑的，它是系统的，但它是，但它同时也是缺乏人性的。

208
00:27:32.480 --> 00:27:42.230
xinyan: 比如说给你举个例子，以前我经常会用ai来帮我编辑。我老板发给我的邮件就是我回复邮件，我会让ai去帮我编辑，

209
00:27:42.670 --> 00:27:56.290
xinyan: 然后我会在ai编辑完邮件以后，我确认这是因为。邮件里面需要涉及到很多，礼貌，用语，以及是否符合公司的规章制度的一些问题，

210
00:27:56.440 --> 00:28:01.610
xinyan: 在结束的时候会让ai加上5%的语法错误，因为

211
00:28:02.450 --> 00:28:09.370
xinyan: 因为我不是一个英语母语的一个人，所以我不想让这封邮件看起来太完美，

212
00:28:09.770 --> 00:28:12.900
xinyan: 但ai，它其实无法理解这个逻辑，

213
00:28:13.320 --> 00:28:14.470
xinyan: 所以我

214
00:28:14.810 --> 00:28:26.260
xinyan: 我会把这个ai产出的内容变得更人性化。所以我默认ai是它是它是有帮助的，它是逻辑，它是快的，但我需要加入一些人性的部分在里面。

215
00:28:27.040 --> 00:28:33.320
Fan Yunjie: 那这个过程您会去一步一步地调校一下，还是说更大程度上是自己来完成呢？

216
00:28:34.060 --> 00:28:36.490
xinyan: 我，我会去调解而言，因为我，

217
00:28:36.770 --> 00:28:47.810
xinyan: 我觉得和ai的工作是一个配合它，它的，它的点，它厉害的点在哪里，我清楚，然后我需要工作的地方在哪里，我也是清楚的。

218
00:28:48.850 --> 00:28:50.050
Fan Yunjie: 这样子

219
00:28:50.430 --> 00:29:02.130
Fan Yunjie: 的目的是为了更好的磨合，跟ai的合作关系，还是说从从效率上来讲，也确实是会比您自己优化更有效率呢。

220
00:29:03.120 --> 00:29:14.550
xinyan: 首先，效率是一定更多的，更快的。因为有一些制式的或者是流程的东西，ai可以更快，更准确的去帮我处理。

221
00:29:15.400 --> 00:29:15.930
xinyan: 对对。

222
00:29:17.940 --> 00:29:27.580
Fan Yunjie: 然后在同时针对于您跟ai的协作关系，您是希望的未来是变成您的一个assistant，还是说它仍然是一个工具。

223
00:29:29.930 --> 00:29:33.780
xinyan: 我其实希望它可以变成我的一个assistant。

224
00:29:34.570 --> 00:29:41.980
xinyan: 但我觉得可能没那么快，我也在balance我和ai之间的能力的一些上下。

225
00:29:42.180 --> 00:29:46.720
Fan Yunjie: 就因为您刚刚所说的就是比方说写邮件的这个事情，

226
00:29:46.720 --> 00:30:05.260
Fan Yunjie: 为什么我也可以理解呢？因为我，我一开始进入到职场也是没有什么ai写邮件就是自己脑子怎么想的，怎么写，然后可能格式也不对，有过这个过程，那现在的小朋友，他为什么说没有那个跟ai头的能力，我觉得是很大程度上他们直接就去

227
00:30:05.260 --> 00:30:11.380
Fan Yunjie: 使用相关工具了，那这个其实我觉得就是很大程度上造成了他们

228
00:30:11.940 --> 00:30:26.070
Fan Yunjie: 没有一定性的这个批判性思维的一个能力的一个原因。当然，写邮件是一个很小的一个事情写邮件，你用什么写都可以。你写邮件，即使你所有的都是都是ai生成输出，我觉得不会产生任何的

229
00:30:26.070 --> 00:30:41.330
Fan Yunjie: 这个大的影响，不论是人的思维，还是说工作的这个速率都不会有特别大影响，但比方说，要是把这个东西给它拔高，就真正的意义上上升到创意上升到策略上升到这种执行

230
00:30:41.330 --> 00:30:47.070
Fan Yunjie: 仍然还是如此的去依赖ai的这种这种输出，

231
00:30:47.560 --> 00:30:56.630
Fan Yunjie: 你觉得这这个方面会不会是一个比较大的，不论是不论是弊端或者说隐患，对于现在的小孩。

232
00:30:56.630 --> 00:30:57.290
xinyan: 我觉得

233
00:30:57.680 --> 00:31:03.590
xinyan: 对我觉得这是一个很大的一个弊端，尤其是对于很多年轻人，他没有试错过，

234
00:31:03.710 --> 00:31:15.530
xinyan: 所以它几乎无法判断ai给出来的东西是好还是坏，因为ai，我们一直在讨论那个ai幻觉的一个问题，ai，它的能力是在于

235
00:31:15.690 --> 00:31:25.240
xinyan: 帮你把任何事情都梳理得很顺，所以在你没有经历过你不知道对错，但ai又把你理得很顺的情况下，这是很大的一个弊端。

236
00:31:25.350 --> 00:31:30.270
xinyan: 我们曾经。跟朋友一起玩过一个游戏，就是那个

237
00:31:30.640 --> 00:31:34.060
xinyan: 火车两个轨道往左还是往右开的，这个对。

238
00:31:34.540 --> 00:31:41.820
xinyan: 这个游戏卡顿游戏。然后比如说左边是一个律师，右边是两个孩子，

239
00:31:41.940 --> 00:31:56.230
xinyan: 当你去把这一道题丢给ai的时候，它可以给你无限的可能去证明任何一方是更值得被保护的ai给你，可以给你很多很多的理由。但是如果你没有一个

240
00:31:56.530 --> 00:31:59.160
xinyan: 就他给你的理由几乎是无限可能的。

241
00:31:59.610 --> 00:32:07.930
xinyan: 所以你你会随意的去相信。另外任何一方都是ok的。在这个情况下没有自己的判断，没有自己的经验

242
00:32:08.100 --> 00:32:09.350
xinyan: 是很可怕的。

243
00:32:10.880 --> 00:32:30.760
Fan Yunjie: 现在我想了解一下，因为我没有去真正意义上从事过这种比较正式的创意类啊，或者说视觉类的工作。我想知道ai可以帮助一个新的候选人输出他的工作大概百分之多少，他可以输出一个直接完美的结果出来吗？

244
00:32:32.490 --> 00:32:35.190
xinyan: 完全由ai去输出一个完美的结果吗？

245
00:32:37.070 --> 00:32:38.740
xinyan: 我觉得完全不可能。

246
00:32:38.740 --> 00:32:40.010
Fan Yunjie: 完全不可能。

247
00:32:40.210 --> 00:32:44.530
xinyan: 它更大程度上是辅助于这些小朋友的哪些方面？

248
00:32:45.800 --> 00:32:47.520
xinyan: 比如说

249
00:32:47.660 --> 00:32:49.120
xinyan: 你有一个

250
00:32:49.510 --> 00:32:52.830
xinyan: 你知道自己想要的东西，比如说现在的photoshop。

251
00:32:52.830 --> 00:32:53.860
Fan Yunjie: 你想。

252
00:32:53.940 --> 00:32:57.000
xinyan: 把这张图片里面的某一棵树移掉，

253
00:32:57.260 --> 00:32:59.890
xinyan: 你可以用ai很快的去帮你完成这个事情，

254
00:33:00.050 --> 00:33:08.250
xinyan: 换掉这个人手上的手套ai可以很快的帮你完成，所以我觉得他不会超过30%。在你所有的工作里面。

255
00:33:09.450 --> 00:33:10.050
Fan Yunjie: 了解。

256
00:33:10.050 --> 00:33:11.390
xinyan: 如果我现在要注意。

257
00:33:11.390 --> 00:33:11.880
Fan Yunjie: 就是。

258
00:33:11.880 --> 00:33:25.830
xinyan: 去了解一个，比如说我现在要招一个人，我的at，他上来就告诉我他的工作ai可以完成80%，那我完全可以理解为他没有自己的standard。这是个很危险的一个候选人。

259
00:33:26.760 --> 00:33:31.650
Fan Yunjie: 但是你从结果上可以判断出来他的ai参与程度吗？

260
00:33:33.790 --> 00:33:35.290
xinyan: 从结果上

261
00:33:35.880 --> 00:33:41.780
xinyan: 光看结果我是看不出来的，如果我要去问一下他怎么去得出这些结果？

262
00:33:41.880 --> 00:33:47.670
xinyan: 他如果不能告诉我，他为什么得出这些结果，那就是ai参与的部分过多，

263
00:33:47.780 --> 00:33:50.590
xinyan: 因为他都是让aa ai去帮他思考的。

264
00:33:51.150 --> 00:33:58.190
Fan Yunjie: 明白明白，所以您觉得最好的一个团队协作关系。比方说，我们现在就把ai拟人化。

265
00:33:59.650 --> 00:34:08.310
Fan Yunjie: 跟一个创团队的一个协作关系最好的是一个什么样的？您刚刚也说也向往他将来可以成为一个assistant。

266
00:34:09.219 --> 00:34:13.789
xinyan: 我不知道你了，不了解那个传统广公司里面的一个架构。

267
00:34:13.969 --> 00:34:16.859
xinyan: 传统广公司里面的架构有一个

268
00:34:17.239 --> 00:34:19.999
xinyan: 非常重要的主力军叫senior

269
00:34:20.589 --> 00:34:21.719
xinyan: senior, art.

270
00:34:22.830 --> 00:34:32.850
xinyan: 个人的作用是他知道上面的内容，同时有很快的手术，然后同时又能带下面的junior designer。

271
00:34:32.989 --> 00:34:36.810
Fan Yunjie: 我觉得ai它完全可以取代这个senior。

272
00:34:36.810 --> 00:34:38.300
xinyan: Art的位置，

273
00:34:38.489 --> 00:34:39.540
xinyan: 它是

274
00:34:39.699 --> 00:34:45.230
xinyan: 准确的接受到上面的指令，然后很快的去完成给下面的一些启发，

275
00:34:45.820 --> 00:34:55.520
xinyan: 他不能再做更多的事情了。所以我觉得你说一个完整的工作流，他的工作被ait的部分大概就是30%很重要，很快速。

276
00:34:57.580 --> 00:34:59.760
Fan Yunjie: 但是这是一个完全

277
00:34:59.960 --> 00:35:02.780
Fan Yunjie: 很上层的，或者说是

278
00:35:02.970 --> 00:35:08.890
Fan Yunjie: 具备一定话语权的岗位，那那不就变成了ai在反向训话人类了吗？

279
00:35:10.570 --> 00:35:18.600
xinyan: Senior at不是一个很高的位置，他是一个很扎实去干活的一个人就是把上面的想法去实现的。一个人。

280
00:35:19.010 --> 00:35:22.100
Fan Yunjie: Ok。所以在这个过程当中。

281
00:35:22.520 --> 00:35:35.430
Fan Yunjie: 对于就是ai在这个创业行业当中，因为很多大家都在去讨论一个问题，说artist creat，他们会不会失业的问题。对于这个问题，您是乐观的还是不乐观呢？

282
00:35:36.910 --> 00:35:43.030
xinyan: 我非常的乐观，我觉得他们永远都不会失业，因为ai所有需要学习的东西都来自于他们。

283
00:35:45.260 --> 00:35:47.260
Fan Yunjie: 但是很大程度之上，

284
00:35:47.390 --> 00:36:01.380
Fan Yunjie: 现在的我，我以方不说，但甲方公司很多已经不去再雇佣，不论是这个写文案的呀，还是说去做这个视觉简单的poster输出的，这样的人

285
00:36:01.410 --> 00:36:07.630
Fan Yunjie: 很多程度之上已经是靠ait带了这个趋势，你怎么看？

286
00:36:08.300 --> 00:36:15.560
xinyan: 我觉得在任何一个时代都有70%的公司把自己的定位就定在了一个

287
00:36:15.680 --> 00:36:17.590
xinyan: 体力活，或者是

288
00:36:17.710 --> 00:36:36.400
xinyan: 不需要创新的一个位置。Ai时代同样也是这样子的，我觉得一定会有30%的公司，他会坚持去实拍，包括甲方的客户也是这样子的，他愿意去告诉你，我们正在跟ai合作，代表他们已经进入到这个时代了。

289
00:36:36.440 --> 00:36:39.540
xinyan: 但是如果一个高端品牌，

290
00:36:40.640 --> 00:36:49.010
xinyan: 它所有输出的内容都是ai，那么也就意味着他们降低了自己的位置，他们觉得自己不需要再引导这个市场。

291
00:36:49.980 --> 00:36:53.700
Fan Yunjie: 那您觉得多大程度之上呢？确实是帮助企业

292
00:36:53.890 --> 00:36:55.390
Fan Yunjie: 却节省了成本。

293
00:36:57.350 --> 00:37:03.780
xinyan: 对于视觉创作这个部分，我觉得30%，然后对于别的部分，

294
00:37:04.130 --> 00:37:12.760
xinyan: 我觉得可能也会有吧，像我以前在亚马逊的时候，其实ai会帮助很多物流仓储以及

295
00:37:13.310 --> 00:37:14.600
xinyan: 就算。

296
00:37:14.980 --> 00:37:22.630
xinyan: 用户在网上浏览，然后去导入到他们的一些广告，这个其实帮助更大，但我没有接触到那个地方。

297
00:37:22.770 --> 00:37:30.220
Fan Yunjie: 明白，但其实我觉得视觉呈现，它是一个很复杂的，而且它会有很多的。

298
00:37:30.820 --> 00:37:33.500
Fan Yunjie: 相关因素在里面，包括

299
00:37:34.020 --> 00:37:45.200
Fan Yunjie: 不同人的理解可能都不会是统一的，包括针对于设计师或者说最初这个品牌想要呈现出来的这个最直观的看法。

300
00:37:45.200 --> 00:37:45.670
xinyan: 零售点。

301
00:37:45.670 --> 00:38:04.810
Fan Yunjie: 不一定所有人都能get到，就是因为视觉它本身它是一个多维多化去参考的一个我觉得是一个比较软性的一个因素，但是我们再回到copybase，就是如果说就文案输出这个行业，这个赛道上来说的话，会不会有非常非常大的可替代性？

302
00:38:06.100 --> 00:38:14.420
xinyan: 我觉得基础ai，基础的copy一定会很大程度上的被ai替代掉，因为ai可以首先生成的很快嘛，

303
00:38:14.460 --> 00:38:27.340
xinyan: 然后你的是不是你想要的，其实就基于你给他喂了多少东西而已。所以经过几轮的调校。像我现在每次做一个project，我都会去安排一个完整的

304
00:38:27.450 --> 00:38:32.530
xinyan: ai的一个folder去管理这整个project，所以很多时候

305
00:38:32.870 --> 00:38:40.150
xinyan: ai给出来的效率实在是太快了。这是为什么很多现在的copy失去工作的一个原因。然后以前

306
00:38:41.100 --> 00:38:57.770
xinyan: 翻译，这个工作其实很麻烦，因为你要了解语境和你工作的内容以及一些关键的敏感词，但现在ai只要你给他足够的去给他足够的限制，它就会给你很快很精准的东西，所以我觉得对copy based的

307
00:38:59.060 --> 00:39:00.880
xinyan: 打击确实是蛮大的。

308
00:39:01.830 --> 00:39:07.360
Fan Yunjie: 了解，所以说那在这个过程当中本身

309
00:39:07.620 --> 00:39:18.200
Fan Yunjie: 就是copybase，它现在已经变成了这个岗位，您已经变成了一个被稀释掉的一个ai完全可替代的岗位。您觉得这样子判断是对的吗？对。

310
00:39:19.490 --> 00:39:20.330
xinyan: 我觉得

311
00:39:20.540 --> 00:39:26.250
xinyan: 不能同意这个观点，因为copybase。就像我说的，你ai

312
00:39:26.430 --> 00:39:44.430
xinyan: 所产出的copy，基于你给他喂了多少的promise，但是实际的一个人的一个copy base。他在生活当中，他能看到客户的眼神，能知道客户和客户之间的关系以及项目背景等等很多东西你是没有办法去完全的丢给ai的。

313
00:39:44.740 --> 00:39:54.710
xinyan: 我们都说那个ceo写出来的copy是最符合这个品牌的，因为他足够多的信息，他了解，当我们一个

314
00:39:54.940 --> 00:40:03.670
xinyan: 真人的copy base，他如果可以真的进入到客户的系统里面，跟客户有足够多的交流，他一定的能力是胜任ai的。

315
00:40:05.260 --> 00:40:10.860
Fan Yunjie: 但是我我其实之前在采访有一位ai的项目folder，

316
00:40:11.010 --> 00:40:17.250
Fan Yunjie: 它是针对于艺术从业者将来被ai去替代的这个事情，

317
00:40:17.450 --> 00:40:27.900
Fan Yunjie: 他有很不乐观的这样的观点，因为他认为所有的这个问题只不过在样本量的这个基础之上。比方说我们现在看电影。

318
00:40:27.980 --> 00:40:44.530
Fan Yunjie: 贾樟柯跟张艺谋他们的风格肯定不一样，跟姜文的风格也不一样，但是ai通过学习也不过是他最多，你比方说给他80,000个样本，他去学，他也会学会怎么样去呈现出来？贾樟柯怎样去呈现出来？姜文，

319
00:40:44.550 --> 00:40:54.950
Fan Yunjie: 这个您认为是一个一个一个正确的观点吗？或者说这也您认为也是一个将来可能遇到的壁垒吗？

320
00:40:55.870 --> 00:41:00.810
Fan Yunjie: 我觉得我不同意这个观点，而且我非常的不同意这个观点，因为。

321
00:41:01.120 --> 00:41:14.020
xinyan: 因为我觉得贾樟柯他的厉害不在于他贾樟柯呈现的内容有多么复杂，而是他开创了这个表达形式。其实最近在上海有很多的展览嘛，就是。

322
00:41:14.200 --> 00:41:15.590
xinyan: 从

323
00:41:16.540 --> 00:41:34.990
xinyan: 最早的写实派到后来的印象派。其实印象派的画作，它所有的画法和技巧。现在所有的中学高中生，你只要学个两年你都能去画出来，但是从写实派到印象派的那个转变是需要有人提出这个理念的。

324
00:41:35.070 --> 00:41:48.060
xinyan: 提出，这个理念的人是建立在你已经画过写实派的人，然后对社会有一定的思考，对社会背景有一定的反噬。最后，你想表达这个内容，引发了后续

325
00:41:48.420 --> 00:42:00.190
xinyan: 几百年的一些思考，这个才是它有价值的点，所以我觉得ai它不会去创造任何没有的东西，这是它最可悲的地方，它的思考就是零和一之间，对吧？

326
00:42:01.350 --> 00:42:17.270
Fan Yunjie: 对的，他是一个二进制，但是他确实没有这个能力去创造出来一个贾樟柯，但是他可以去学习。贾樟柯，比方说贾樟柯将来的电影他就真的可以按照贾樟柯的调性去拍出来一部，这种你觉得也是不认可的，是吗？对。

327
00:42:18.860 --> 00:42:27.550
xinyan: 首先，他如果按照贾樟柯的电影去拍，那他代表的还是贾樟柯，所以你买单的还是贾樟柯本人，而不是ai。

328
00:42:27.770 --> 00:42:35.310
xinyan: 其次，复制真的太简单，复制那个艺术家把香蕉贴在墙上，这种做法

329
00:42:35.470 --> 00:42:38.970
xinyan: 你在家里都可以完成，但你做了没有任何的意义。

330
00:42:40.880 --> 00:42:46.670
Fan Yunjie: 明白，那您在本身在您的工作过程当中吧，您有没有，

331
00:42:47.210 --> 00:43:06.730
Fan Yunjie: 就是发现ai的哪些弊端，就除了说您刚刚所说，它的创意性有些稀缺，然后本身它就是一个领导1.2进制的，这样的一个存在体有没有可能说ai看似懂了你，但其实在误导你的这种过程对。

332
00:43:07.440 --> 00:43:11.250
xinyan: 有很多ai，首先它的主要工作就是复合你。

333
00:43:11.470 --> 00:43:15.910
xinyan: 我觉得这是最危险的一个点就是，无论你提出一个什么，

334
00:43:16.020 --> 00:43:26.010
xinyan: 它都会想办法让你觉得你提出的东西是ok的，这是最可怕的一个点就是，你活在自己给自己设定的误区里面。

335
00:43:27.570 --> 00:43:30.220
Fan Yunjie: 你们的这种您一般会怎么样去，

336
00:43:30.400 --> 00:43:33.330
Fan Yunjie: 您会去纠正它吗？或者说是您会去

337
00:43:33.570 --> 00:43:38.520
Fan Yunjie: 想办法的，让它批量化的去标准在您自己的程度上吗？

338
00:43:39.190 --> 00:43:51.320
xinyan: 首先在写的时候，我会尽量的告诉他就是，我现在其实有个这样的做法，我在ai给我结论的后面，我想让他帮我加一个，除了这个之外还有别的可能性吗？

339
00:43:51.900 --> 00:44:09.720
xinyan: 就是我会让他跳出这个思维给我一个。当然我知道这个也许会跳，也许也不会跳，但是我想让他给我一个和这个答案不一样的一个答案。另外呢？就是我觉得需要更多的丰富，在真实生活中的一些体验，

340
00:44:10.880 --> 00:44:15.600
xinyan: 就是你有这个经历以后有更多的经历，你就会有更多的判断标准。

341
00:44:16.440 --> 00:44:30.740
Fan Yunjie: 明白，所以在这个过程当中，你是会就比方说，像这种你刚刚提到了一个像abst，我给让他给我个答案，a，再给我个答案，b，然后我去比较一下这个是您经常会使用的方法，是吗？

342
00:44:31.320 --> 00:44:40.200
xinyan: 我现在每次都会这样去做，因为我至少会告诉他，我让你帮我工作的目的不是为了负荷我，而是为了，给我一个，

343
00:44:40.400 --> 00:44:45.180
xinyan: 这个快要看你大的帽子是什么。比如说你这个大的mod是完成这个项目

344
00:44:45.550 --> 00:44:48.960
xinyan: 还是在，比如说，

345
00:44:49.610 --> 00:44:54.430
xinyan: 某一类艺术创作里面有些突破，这你要给它设定一个巨大的帽子。

346
00:44:55.010 --> 00:44:59.870
Fan Yunjie: 明白，我，我之前在采访过一个

347
00:45:00.110 --> 00:45:07.900
Fan Yunjie: 他是在甲方去作业，也不是一个很小的甲方，当然他们本身的工作非常非常的

348
00:45:08.650 --> 00:45:24.300
Fan Yunjie: 轻量化，所以他们没有真正意义上有creator的这样的一个岗位，在那里，他们的market自己去输出相应的post。他本身。在认为ai，ai的这个磨合当中，

349
00:45:24.400 --> 00:45:35.740
Fan Yunjie: 他最好的一个工，一个工作的显现程度就是去帮他去修正。比方说，现在像photoshop一样告诉他从哪改呀？什么这些的

350
00:45:35.910 --> 00:45:53.930
Fan Yunjie: 您认为本身ai在这个还是回到这个协作过程当中，您认为它最好的一个定位还是存在于修正的这种功能性吗？还是说还是一种可视化的一种功能性，还是说帮助大家去

351
00:45:53.930 --> 00:46:06.350
Fan Yunjie: 进行一定的，这个主流的。一定一定一非偏差性。比方说，我们你刚刚说提到branding，我们的一致性，您觉得它最佳的一个定位是什么？

352
00:46:08.070 --> 00:46:10.860
xinyan: 它最佳的定位应该是一个

353
00:46:11.230 --> 00:46:20.920
xinyan: 就是你给出具体指令的一个执行者吧。我觉得在甲方工作其实很繁杂，然后你需要创造和ai工作的一个机会。

354
00:46:21.660 --> 00:46:34.750
xinyan: 比如说数字人这个部分。其实刚开始我们提出这个idea的时候，很多我们叫就是那些培训讲师，他其实是不太买单的，因为我们后来知道就是

355
00:46:34.870 --> 00:46:43.890
xinyan: 个人的ip，最重要的是你个人在这个镜头前，你卖的是自己的一个状态，以后会变成流量。

356
00:46:44.090 --> 00:46:48.510
Fan Yunjie: 所以他们刚开始把自己的形象变成一个digital的一个。

357
00:46:48.520 --> 00:46:54.330
xinyan: 矢量卡通，他们其实很很不愿意去这样去做，因为自己的可替代性就没了。

358
00:46:54.330 --> 00:46:54.960
Fan Yunjie: 明白。

359
00:46:54.960 --> 00:46:58.320
xinyan: 然后后来不是后来川普的那个吗？

360
00:46:59.020 --> 00:46:59.490
Fan Yunjie: 哼。

361
00:46:59.490 --> 00:47:12.200
xinyan: 很多人过来找我，他说，我发现我用真人在网上去讲这个课程。现在因为这个关税的问题让我在网上变成了就是众矢之的，大家都在骂我，

362
00:47:12.200 --> 00:47:23.100
xinyan: 那我能不能重新启用一下这个。数字人的这个部分，因为他这个时候他不想让自己录在镜头前面了，这个你还是要去match一下这个需求的部分。

363
00:47:24.070 --> 00:47:31.430
Fan Yunjie: 但是他是您认为他完全可以替代人的角色，还是他可以放大，或者说是

364
00:47:31.550 --> 00:47:35.200
Fan Yunjie: 缩小人本身的这种这种角色替代性。

365
00:47:35.860 --> 00:47:42.010
xinyan: 我觉得它可以放大人的能力，但一定不能替代人的存在。我们在

366
00:47:42.150 --> 00:47:49.290
xinyan: 创作和ai的合作的时候，我们也会最大可能的避免让ai去代替人的这个作用。

367
00:47:50.030 --> 00:48:02.520
Fan Yunjie: 就数字人的这种这种传播行径来讲的话，我以我的这个受众来说啊，我在思考他如何去获取用户，或者说客户的信任。

368
00:48:05.640 --> 00:48:19.070
xinyan: 是当你在网上去看一些教育类的课程，其实你更希望感受到一个真实的一个人的一个状态。所以这也是为什么？我们想去尝试这个新技术的时候，最后还是被驳回了。

369
00:48:19.900 --> 00:48:27.550
Fan Yunjie: 了解，是因为它更大程度之上不能够有更强的可信度，是吗？

370
00:48:27.800 --> 00:48:28.650
Fan Yunjie: 还是说。

371
00:48:28.650 --> 00:48:43.180
xinyan: 两方面呢？一方面是可信度会降低。另外一方面是sail trainer，他其实更想让自己真实的脸暴露在网上，因为他，他想为自己赢得更多的流量。但是我不排除会有一些别的状态之下，

372
00:48:43.470 --> 00:48:57.170
xinyan: 比如说，如果我现在需要一个模特或者是机场那些导购，我希望他以更标准，更流程化的一个人物呈现的时候，也许数字人可以代替，会呈现得更好。

373
00:48:58.680 --> 00:49:04.290
Fan Yunjie: 了解。所以，在这个维度上来讲的话，我觉得

374
00:49:04.700 --> 00:49:16.600
Fan Yunjie: 本身您刚刚提到数字人，他是我觉得不是一个新的概念了，应该也存在了一段时间了，可能早期的时候，大家对于他的可接受能力还是比较强，

375
00:49:16.690 --> 00:49:30.150
Fan Yunjie: 然后现在伴随着这个一定的，大家对于ai的这种我觉得疲态吧，对，可以用疲态这个词来，然后大家可能对于ai生成，很多事情就开始不买账，

376
00:49:30.260 --> 00:49:37.120
Fan Yunjie: 或者说是对于很多甲方工作者或者甲方的这些轻量化的创业工作者来说，

377
00:49:37.310 --> 00:49:51.810
Fan Yunjie: 他们去如何解决ai的生成内容去获得用户的信任，不光光是从素质人这个角度，包括他们生成出来的，不论是这个视觉上的，视觉上的还是说这个文字上的这种维度，

378
00:49:51.990 --> 00:50:01.150
Fan Yunjie: 因为没有相应的就艺术的判断或创意的判断完完全全依靠ai的话，如果你站在一个

379
00:50:01.360 --> 00:50:13.010
Fan Yunjie: 比较高的一个维度吧。有一个乙方比较比较多的case的参与者来说，您给到这些甲方的一些小的，小的，这些工作者什么的建议。

380
00:50:14.870 --> 00:50:19.610
xinyan: 我觉得在新技术出来的时候，大家都更愿意去尝试，这是一个很好的一个，

381
00:50:19.740 --> 00:50:31.220
xinyan: 因为通过我觉得这是很好的一个结果大家都去尝试嘛。所以有些新的idea，新的A加b加C的方案会被直接导出一个更好的结果出来，

382
00:50:31.320 --> 00:50:33.940
xinyan: 就像以前很流行的h five，

383
00:50:34.330 --> 00:50:40.330
xinyan: 所有人都在做，最后大家都觉得这个其实对自己的流量并没有什么直接的帮助。

384
00:50:40.450 --> 00:50:40.880
Fan Yunjie: 对的。

385
00:50:40.880 --> 00:50:45.060
xinyan: 但是我觉得作为甲方来说，他是一个很好的，

386
00:50:45.360 --> 00:50:50.590
xinyan: 就是说的不太好就是消耗成本的一个消耗预算的方式

387
00:50:51.130 --> 00:51:00.300
xinyan: 花钱的方式，然后大家也愿意去接受一些，比如说，如果我作为一个甲方的

388
00:51:00.610 --> 00:51:01.640
xinyan: leader,

389
00:51:01.750 --> 00:51:05.710
xinyan: 我会更愿意知道我们品牌在尝试一些新的

390
00:51:05.890 --> 00:51:09.280
xinyan: 和消费者去做接触的一些

391
00:51:10.580 --> 00:51:12.380
xinyan: 科技啊，或者是方式。

392
00:51:14.520 --> 00:51:16.980
Fan Yunjie: 但是怎么样去

393
00:51:17.590 --> 00:51:19.670
Fan Yunjie: 评论或者怎么样去

394
00:51:19.740 --> 00:51:37.810
Fan Yunjie: 去balance这样的关系们，或者说是您作为一个团队的，比方说现在您这个就是就是您的项目了，然后您的成本也是有限的，您本身也不会去真正involve in到这么细的工作，您也默认了这样的一种形式存在，但您怎么样去避免？

395
00:51:38.010 --> 00:51:46.700
Fan Yunjie: 不论是用户可信性，还是说这种输出的这种物料一致性等等，这样的这种很油腻的ai产出物的。这样的问题。

396
00:51:48.280 --> 00:51:52.400
xinyan: 我觉得每个品牌它都有一套验算自己。

397
00:51:53.730 --> 00:51:54.650
xinyan: 投入

398
00:51:54.860 --> 00:51:55.780
xinyan: 和

399
00:51:56.070 --> 00:52:01.720
xinyan: 产收笔的这个一个公式，或者是比如说亚马逊，它其实有很多data的部分嘛。

400
00:52:01.800 --> 00:52:19.360
xinyan: 刚开始比如说我们做新的ai项目或者新的H五项目，我们会在最后去收集一些用户的反馈，或者是粉丝量的增减或者是满意度的好坏等等，这个可能会有一定的周期，比如说，一个q，four，或者是两个季度

401
00:52:19.950 --> 00:52:28.250
xinyan: 之类的。最后我们再回来去判断明年我们要不要在这个上面花去更多的钱，或者在这个方面更多的去投入一些东西。

402
00:52:29.500 --> 00:52:37.970
Fan Yunjie: 所以更大程度之上还是要先去试错，然后再去反驳回来，对吗？

403
00:52:38.190 --> 00:52:47.750
xinyan: 对，我觉得所有的品牌都不想失去这个先机。就比如说朋友圈的广告。我记得第一条，第一条，朋友圈的广告是宝马的吧，

404
00:52:48.170 --> 00:52:52.030
xinyan: 大家都记住了，后面呢？其实就没有太多人会去记住，

405
00:52:52.180 --> 00:52:56.460
xinyan: 包括可口可乐前段时间也用ai去创造了一条完全

406
00:52:56.800 --> 00:53:05.310
xinyan: 完全ai的一条片子，我觉得他的出发点一定是ok的，没有问题的，就品牌需要去做这个事情。

407
00:53:06.450 --> 00:53:25.310
Fan Yunjie: 明白。所以我，我看了一下您本身在amazon这个做的这个相关的workshop，您是就主导了这个。这个本身您的这个主导过程，您方便来介绍我一下吗？

408
00:53:26.400 --> 00:53:31.970
xinyan: 可以啊。就像我说的。我的工作呢？是提升整个amazon ads

409
00:53:32.200 --> 00:53:42.260
xinyan: 所有产出视觉的质量，那么人物在镜头前表达其实是一个很重要的部分，因为，我们需要让所有

410
00:53:42.680 --> 00:53:50.590
xinyan: 在镜头前去讲amazon一些产品或者是功能的时候，有更专业或者是

411
00:53:50.990 --> 00:53:52.570
xinyan: 更体面的表达吧。

412
00:53:52.720 --> 00:53:59.500
xinyan: 那么以这个为出发点，我们给自己设定了一个我们amazon应该是什么样子的？应该是国际化的，专业的。

413
00:54:01.900 --> 00:54:17.140
xinyan: 并且是亲切的这样子一个调性，那我们就开始讲。我们的服装我们的表达方式，灯光怎么去用，音乐怎么去用，然后在这个在这个过程当中ai会帮助到我们哪些，比如说ai可以帮助我们眼球矫正

414
00:54:17.350 --> 00:54:24.210
xinyan: 或者是ai可以帮助我们去优化一些，图片的部分，让人更加的精致一点等等，

415
00:54:24.660 --> 00:54:28.380
xinyan: 大概是这样子一个逻辑，所以ai它只是其中的一个部分而已。

416
00:54:28.850 --> 00:54:34.630
Fan Yunjie: 比方说他眼球矫正或者说提提磁器，就这一类的项目当中，大家的专长在哪啊？

417
00:54:35.490 --> 00:54:39.200
xinyan: 爷爷的专场是一个指令解决了

418
00:54:39.440 --> 00:54:58.800
xinyan: 所有类似的问题。比如说我们收卖家讲师的视频的格式都是一样的嘛，都是人对着镜头在讲，当我们给他了一个权限，你可以对着镜头同时眼球可以去读你下面的那些小字，就你小抄的部分。

419
00:54:58.800 --> 00:55:00.050
Fan Yunjie: 这个时候。

420
00:55:00.360 --> 00:55:09.620
xinyan: 提升了很多，就就本来别人录个五分钟就别人录个30秒就得停一下，你现在可以同时录个五分钟就这么简单。

421
00:55:10.760 --> 00:55:14.700
Fan Yunjie: 您觉得他这种矫正他是更大程度上

422
00:55:14.810 --> 00:55:17.400
Fan Yunjie: 算是增强还是操控。

423
00:55:18.900 --> 00:55:20.180
xinyan: 增强优化吧，

424
00:55:20.310 --> 00:55:21.790
xinyan: 他并没有改变什么。

425
00:55:22.970 --> 00:55:30.770
Fan Yunjie: 那有没有相应的用户反馈就是对于这这一个技术，它的一些反馈是更大程度上

426
00:55:30.930 --> 00:55:32.130
Fan Yunjie: 怎么样的。

427
00:55:32.960 --> 00:55:39.820
xinyan: 我们拿到了非常多卖家讲师的反馈，他们的反馈就是非常好，就是我现在可以读稿了，

428
00:55:40.200 --> 00:55:47.520
xinyan: 现在不用再去背那些词了，所以我可以更灵活，更长期更稳定的那些去表达自己的视频。

429
00:55:48.970 --> 00:55:55.470
Fan Yunjie: 从来也没有说过任何的反馈，会思考到ai的边界程度，是吗？

430
00:55:56.580 --> 00:56:00.000
xinyan: 对，我觉得ai它它是一个

431
00:56:00.220 --> 00:56:04.900
xinyan: 无限可能，那个一个产品，你要去match它的功能点。

432
00:56:06.310 --> 00:56:08.030
Fan Yunjie: 了解，了解，

433
00:56:08.380 --> 00:56:11.430
Fan Yunjie: 那比方说在这个过程当中，

434
00:56:11.590 --> 00:56:24.850
Fan Yunjie: 大家再去进行这种ai辅助的情况之下，会不会给？因为您刚刚提到您的这些用户，还是说商家为主，会不会给商家的受众一种信任性降低的感觉呢？

435
00:56:26.480 --> 00:56:45.520
xinyan: 也不会啊，因为我们还是真人在录制这些视频嘛。只不过你在看这个真人讲师的时候，他的眼睛没有在乱动，还是一个真人在那里。当然，我们眼球矫正只是ai帮助我们的其中一个部分，我们后期也有开始研究ai怎么去处理光线，让人物呈现在

436
00:56:45.600 --> 00:56:48.110
xinyan: 因为不是所有人都会打光，

437
00:56:48.800 --> 00:57:02.200
xinyan: 那我们可以去调整他们的光线，这样在呈现的时候，其实作为普通的观众，你会看到一个更专业的讲师，然后可以读五分钟，他眼睛都不会去转一下的那种讲师。

438
00:57:02.730 --> 00:57:12.230
Fan Yunjie: 了解那amazon，它本身在这个项目过程当中，它是目的，是解决了痛点，还是说有更大程度上强调功利性的价值。

439
00:57:13.950 --> 00:57:14.790
xinyan: 那么

440
00:57:16.170 --> 00:57:34.320
xinyan: 对于自己的定位还是一个比较专业的大的品牌，他需要在自己输出视觉或者呈现视频的时候有一个更专业的姿态，他不会去。他有义务去想我的视频怎么样可以专业一点，所以我们的专业一点就是在细节上就是大家都在拍，

441
00:57:34.420 --> 00:57:40.560
xinyan: 但我们可以让我们的讲师，眼睛不要动，光线更加的自然等等。

442
00:57:41.660 --> 00:57:47.520
Fan Yunjie: 明白，所以您本身就就是就是在于您自己的

443
00:57:47.660 --> 00:57:52.480
Fan Yunjie: 内核上，或者说您本身的从事的价值观上，

444
00:57:52.830 --> 00:58:10.530
Fan Yunjie: 您会不会有一些权衡在于，伴随着ai的这种高产率的输出，我有一些功利价值，然后我本身也可以去快速的去去完成很多的项目，而是削弱了真实表达这样的

445
00:58:10.770 --> 00:58:13.540
Fan Yunjie: 一种权衡，会不会有这种struggle。

446
00:58:14.780 --> 00:58:17.850
xinyan: 字儿，刚你刚说削弱了什么？表达。

447
00:58:18.270 --> 00:58:25.540
Fan Yunjie: 就是您本身比较真实的一些创意表达，而更大程度上追求这种快速的，功利型的价值。

448
00:58:27.330 --> 00:58:30.340
xinyan: 我觉得其实还好，因为

449
00:58:30.680 --> 00:58:45.130
xinyan: 我使用ai是基于我知道我自己想要去产出一个什么东西，ai在这过程当中也只是帮助我去做了一些部分，做了一些我的。可能需要花费我更多工作量的一个工作时间的一个部分，

450
00:58:45.340 --> 00:58:50.640
xinyan: 所以我觉得结果还是我想要去呈现的，他没有改变任何东西。

451
00:58:51.670 --> 00:58:56.340
Fan Yunjie: 那您比方说在一到两年之内，你最希望ai有哪些

452
00:58:56.670 --> 00:58:58.330
Fan Yunjie: 维度上的突破？

453
00:59:00.630 --> 00:59:03.220
xinyan: 一到两年之内，

454
00:59:06.670 --> 00:59:09.690
xinyan: 我希望ai可以更加的了解我

455
00:59:09.940 --> 00:59:14.660
xinyan: 就是跨项目的，比如说我现在在很有意思的去

456
00:59:14.860 --> 00:59:22.080
xinyan: 训练我的ai，让他知道我是一个什么样的人，我用什么样的语气去完成一个项目，

457
00:59:22.200 --> 00:59:24.460
xinyan: 我在意的是什么？因为我觉得

458
00:59:24.970 --> 00:59:27.590
xinyan: 每个设计师他其实有一些自己的特点。

459
00:59:27.760 --> 00:59:29.130
Fan Yunjie: 比如说。

460
00:59:29.170 --> 00:59:32.050
xinyan: 我可能会更在意人群洞察，

461
00:59:32.610 --> 00:59:36.240
xinyan: 然后表达出一些真实的感情。

462
00:59:37.320 --> 00:59:48.380
xinyan: 我想让ai在接触到我下个项目的时候，直接已经考虑过跟大家做过这些项目，所以我其实现在这个changept已经能已经可以做到这个了。就是，

463
00:59:49.800 --> 00:59:59.580
xinyan: 那个叫什么全局思考还是什么的，就是他知道我是一个什么样的人，但我希望ai在这个方面可以更精进一点，因为我觉得这个，因为我

464
00:59:59.820 --> 01:00:03.200
xinyan: 跟他做的所有项目都是都是data，

465
01:00:03.320 --> 01:00:06.530
xinyan: 他如果能更好的去利用这些东西，他会更了解我。

466
01:00:07.270 --> 01:00:10.750
Fan Yunjie: 了解，但是您不会有一个

467
01:00:10.970 --> 01:00:19.200
Fan Yunjie: 担忧吗？在于未来您本身输出观点。它的这种怎么说呢？版权性。

468
01:00:21.390 --> 01:00:23.720
xinyan: 啊。我并不担心这个，因为我觉得

469
01:00:23.960 --> 01:00:31.130
xinyan: 在ai的时代，在智能手机的时代本身个人的一些信息和

470
01:00:31.240 --> 01:00:36.800
xinyan: 隐私，他都是没有的，就是只不过是没有人想去了解你而已。

471
01:00:37.130 --> 01:00:45.390
xinyan: 所以我比如说我给你举个很简单的例子。我其实一直在研究怎么去创创作一个数字版的我自己，

472
01:00:45.700 --> 01:00:49.650
xinyan: 我跟我的妻子就是共用一个changept的账号嘛。

473
01:00:49.650 --> 01:00:51.930
Fan Yunjie: 他用的是俄语的部分。

474
01:00:51.930 --> 01:00:56.950
xinyan: 然后我用的是中文交流的部分，所以加gpt其实很容易分辨

475
01:00:57.000 --> 01:01:12.680
xinyan: 他是在跟谁聊天，就是当我用中文的时候他就知道，哎，我在跟你聊天，所以我其实做过一个蛮有意思的实验就是，我想让chatpt基于我跟他所有合作过的项目去创造出一个

476
01:01:12.790 --> 01:01:14.170
xinyan: 我自己出来，

477
01:01:14.500 --> 01:01:16.090
xinyan: 像我的妻子在

478
01:01:16.230 --> 01:01:27.070
xinyan: 同时登录这个账号的时候，他可以去问这个账号一些问题，然后我希望可以模仿我的角色去回答这个问题，就我，我，我其实是

479
01:01:27.400 --> 01:01:37.550
xinyan: 对于自己的一个数字分身比较乐观的一个态度，我不觉得这个会侵犯到我，或者是代替掉我什么之类的，反倒是我觉得

480
01:01:37.660 --> 01:01:45.750
xinyan: 能把自己的一个形象给留下来，这个其实蛮浪漫，其实已经已经有一些国家在做一些蛮有意思的项目，就是，比如说，

481
01:01:46.020 --> 01:02:01.860
xinyan: 比如说某一个诗人，他写过很多的东西。当我去把这个诗人所有产出的内容输入到ai的时候，我希望ai可以模仿他的语境，来跟你做一个面对面的交流，我觉得这个其实蛮浪漫的。

482
01:02:02.550 --> 01:02:22.550
Fan Yunjie: 这个观点非常有趣，但是我在想，其实我在使用gpt的时候，即使是我一个人跟他去讲话，他都会有混乱的记忆。你觉得目前来讲，您两位共同在去使用一个model的时候不会说意识到ai，它会有混乱记忆之类的问题吗？

483
01:02:23.050 --> 01:02:31.060
xinyan: 它会有。然后这也是我觉得ai需要去精进的地方，它可能是算力，也有可能是推导逻辑的问题，

484
01:02:31.170 --> 01:02:32.320
xinyan: 但是我

485
01:02:32.700 --> 01:02:38.980
xinyan: 的做法就是每次当它有一个错误错误理论的时候，当我发现我就会立马纠正他。

486
01:02:41.310 --> 01:02:45.750
Fan Yunjie: 您这边让我比较好奇的就是您在工作以外，都是

487
01:02:46.040 --> 01:02:50.510
Fan Yunjie: 使用ai到哪些程度，或者说是都是会在哪些纬度使用ai。

488
01:02:54.020 --> 01:02:55.260
xinyan: 比如说

489
01:02:56.180 --> 01:03:02.570
xinyan: 搜索一些就特定的信息，比如说机场多少毫安的电池能带进去

490
01:03:02.720 --> 01:03:06.740
xinyan: 某个机场是不是有一些特殊的规定，其实ai会帮你

491
01:03:06.860 --> 01:03:08.750
xinyan: 给出更直接的答案，

492
01:03:09.050 --> 01:03:11.240
xinyan: 省去了很多搜索的过程，

493
01:03:11.420 --> 01:03:16.710
xinyan: 或者是有一些点对点的答案，比如说，

494
01:03:16.840 --> 01:03:18.440
xinyan: 哪里的护照免签，

495
01:03:18.620 --> 01:03:21.180
xinyan: 或者是哪个国家的一些特殊的政策，

496
01:03:21.350 --> 01:03:27.980
xinyan: 还有一些就是翻译，因为我在生活中其实接触到除了英语系国家以外很多别的国家的人，

497
01:03:28.300 --> 01:03:33.840
xinyan: 我觉得现在ai的翻译，它已经完全超越了以前直翻的一个。

498
01:03:33.840 --> 01:03:34.170
Fan Yunjie: 对啊。

499
01:03:34.170 --> 01:03:37.830
xinyan: 逻辑了，就你可以给出一些很具体的chrome，比如说

500
01:03:38.180 --> 01:03:42.780
xinyan: 我需要在他的语境里面，或者是我需要增加一些幽默感

501
01:03:42.890 --> 01:03:47.490
xinyan: 符合当地。比如说你想去模仿一个英国区域那种

502
01:03:47.710 --> 01:03:51.090
xinyan: 小流氓的一个语气，他其实可以帮你模仿出来的。

503
01:03:51.810 --> 01:04:05.240
Fan Yunjie: 没错，但是这样的一些工具型的使用会让ai很具象化的理解您吗？您刚刚提到了想让他去为您创造一个分身，这样您是怎么调教他的？

504
01:04:06.050 --> 01:04:17.380
xinyan: 我觉得从branding角度来看这个问题的话，就是所有人他都有一两个关键词，这是他的brand message，然后在这两个关键词之下，

505
01:04:17.710 --> 01:04:23.170
xinyan: 它会做出各种各样的事情，比如说喜欢穿什么颜色的衣服，什么样式的衣服，

506
01:04:23.380 --> 01:04:29.990
xinyan: 教什么样子的对象，以及用什么样的语气去跟别人讲话等等。

507
01:04:30.280 --> 01:04:36.710
xinyan: 我觉得你给出的信息给ai越多，ai越会总结出更精准的你自己。

508
01:04:38.070 --> 01:04:46.530
Fan Yunjie: 当然，但是你会去为了，这个特地的跟ai有一些互动吗？还是说就完完全全会。

509
01:04:47.210 --> 01:04:56.770
xinyan: 会，我会特别的告诉他，我处理这个事情的原因是什么？我为什么要这样去处理？因为我的目的是让ai更精准的总结出我自己。

510
01:04:57.730 --> 01:05:08.680
Fan Yunjie: 我想了解一下这样的一个过程，比方说大概是一个什么样的内容形式，然后您希望它本身给到您的一个怎么样的结果是您满意的程度。对。

511
01:05:09.840 --> 01:05:17.290
xinyan: 比如说在回复领导英文邮件的时候，我的结论是，一篇符合规矩的

512
01:05:17.400 --> 01:05:20.510
xinyan: 同时，有5%语法错误的一个

513
01:05:21.730 --> 01:05:29.340
xinyan: 给到我的领导，同时我会反过来告诉ai，我处理这个方式的原因是什么？我告诉他

514
01:05:29.870 --> 01:05:40.400
xinyan: 并不是完全准确的。然后在我的工作当中，为什么我需要5%错误的语法，因为我想让他知道，因为我想让我的领导知道这是一个

515
01:05:40.570 --> 01:05:42.770
xinyan: 真人写出来的一个邮件，

516
01:05:42.890 --> 01:05:50.360
xinyan: 我会告诉他为什么我要这样做的一个逻辑，我觉得这个可以帮助ai去更了解我是一个什么样的人。

517
01:05:51.790 --> 01:06:03.940
Fan Yunjie: 您在熟悉的领域肯定会有一定的这种掌控性，但如果说您在不熟悉的工具使用，或者说是调查一些信息的时候，您又有多大程度上相信ai。

518
01:06:08.140 --> 01:06:25.070
xinyan: 其实这个没有问太多，比如说机场的一些规范签证的一些条款，这个我是相信的。我尝试过让ai帮我投诉简历，我意识到他给出来的很多信息是快速及时的，但都是不准确的，比如说。

519
01:06:25.070 --> 01:06:25.450
Fan Yunjie: Git.

520
01:06:25.450 --> 01:06:38.000
xinyan: 我让他去帮我match15家可我简历匹配的公司和企业，并且同时找到这15家公司hr的联系方式或者是直接联系的

521
01:06:38.150 --> 01:06:49.010
xinyan: 的人，我发现我投出去的15封邮件，比如说15封，可能有七封回回来是这个邮件本身就不存在或者是没有回复的那种。

522
01:06:50.000 --> 01:06:57.640
Fan Yunjie: 明白。所以说，如果说真的意义上，现在一个您完全不了解的品类

523
01:06:57.660 --> 01:06:59.710
Fan Yunjie: 过来找您去做branding，

524
01:06:59.710 --> 01:07:22.380
Fan Yunjie: 然后您在一开始需要去学习这个品类的一些，比方说一个制造导弹或制造芯片的行业，您可能也开始要学习一定的信息，你会在这个过程当中去反向的验证ai，它的这个输出内容吗？或者说是想要去看它本身，给到您的内容的引用来源之类的吗？

525
01:07:22.950 --> 01:07:38.050
xinyan: 当然就是ai会成为我的某一个渠道来源，信息渠道来源，其他的，我会。比如说我现在想开一家奶茶店在上海，我一定会问ai，同时我也会去线下的奶茶店去问一问正在开店的人的一些想法。

526
01:07:38.820 --> 01:07:50.980
Fan Yunjie: 了解，了解。所以如果将来ai本身在给到您结果的过程当中，会给到你他思考的过程，你觉得会这样会帮助你去判断它的真实性吗？

527
01:07:54.080 --> 01:08:06.250
xinyan: 我觉得会同时也会帮助我更理解ai为什么这样去做，但同时ai它就还是只是我的信息来源的一个部分而已，还是我会去了解一下别的

528
01:08:06.360 --> 01:08:07.650
xinyan: 信息的渠道。

529
01:08:09.090 --> 01:08:10.270
Fan Yunjie: 了解，

530
01:08:10.420 --> 01:08:18.920
Fan Yunjie: 那回到你们本身工作来讲，您觉得未来的这个创业经理他，更多程度上，你会把它定义成一个

531
01:08:19.200 --> 01:08:25.689
Fan Yunjie: architect，或者还是一个纯粹的这种。的这种creator。

532
01:08:28.420 --> 01:08:38.560
xinyan: 我觉得未来的creative它会更倾向于给出from。所以这也是为什么我在两年前就转到了

533
01:08:38.680 --> 01:08:41.430
xinyan: branding这个这个角色的一个原因，

534
01:08:41.580 --> 01:08:46.170
xinyan: 我觉得未来的prom需要更精准，未来执行的人会

535
01:08:46.270 --> 01:08:58.229
xinyan: 遇到一些麻烦，因为现在ai他的能力提升的很快，以后执行可能会变得越来越简单，最重要的是给出的这个人，他的精准度是不是ok。

536
01:08:59.800 --> 01:09:13.109
Fan Yunjie: 那您如果要给未来这些创意行业的这些manager，一些ai的协作guideline。您觉得如果您现在就提出三条，您认为哪三条？您觉得比较priority一点。

537
01:09:15.060 --> 01:09:22.069
xinyan: 首先要有成没成本就是你一定要去试错，比如说你想去，我觉得现在很多

538
01:09:22.240 --> 01:09:29.460
xinyan: ai的或者是创意人，他跳过了真正执行的阶段，我觉得还是要去执行就是基本功。

539
01:09:29.710 --> 01:09:35.460
xinyan: 第二个，他要去感受真实人类的生活，就是他一定要把自己放在

540
01:09:36.649 --> 01:09:39.560
xinyan: 用户或者是他的

541
01:09:40.490 --> 01:09:45.260
xinyan: 消费人群里面，去感受他们的生活，就自己真的进入到那种生活里面就是。

542
01:09:45.790 --> 01:09:51.300
xinyan: 然后第三个我觉得就是抱有对ai的一些非常

543
01:09:51.740 --> 01:09:54.420
xinyan: positive的一些想法，就多尝试一下。

544
01:09:55.130 --> 01:10:03.980
Fan Yunjie: 明白了解，所以就是如果说将来如果有一天ai真的可以带来一种比较新的一种

545
01:10:04.220 --> 01:10:06.950
Fan Yunjie: 一种创意的范式，甚至

546
01:10:07.370 --> 01:10:10.640
Fan Yunjie: 可能更大程度上去。

547
01:10:10.840 --> 01:10:26.630
Fan Yunjie: 去。怎么说呢，消耗掉了您本身的这种岗位，变成了真正意义上的品牌跟用户之间沟通的一种工具型的桥梁。您对于这样的一种结果的导向，您认为您会怎么去看。

548
01:10:28.080 --> 01:10:29.560
xinyan: 首先我觉得

549
01:10:29.700 --> 01:10:41.420
xinyan: 不太会被消耗，因为我觉得一个工具被创造出来它的最终目的如果是取代人类的话，它一定会在某一个节点会被消灭。

550
01:10:42.250 --> 01:10:45.990
xinyan: 另外呢？我其实对于自己的工作

551
01:10:46.260 --> 01:10:54.320
xinyan: 蛮蛮自信的，就是我相信我自己做的东西，因为我相信我能体验到ai体验不到的东西。

552
01:10:55.430 --> 01:11:02.200
Fan Yunjie: 明白，那您觉得最好的一个创意性的范式。对于ai来说的话，它其实还是去

553
01:11:02.310 --> 01:11:03.250
Fan Yunjie: 拓展，

554
01:11:03.460 --> 01:11:06.400
Fan Yunjie: 而不是去创作，是吗？

555
01:11:08.450 --> 01:11:12.960
xinyan: 我觉得ai很难创作吧。当然我希望他可以去创作，

556
01:11:13.160 --> 01:11:16.700
xinyan: 但创作它一定是基于我。对于

557
01:11:16.970 --> 01:11:21.000
xinyan: 我的我的就是我需要它成为一个什么样子的？

558
01:11:21.300 --> 01:11:22.700
xinyan: 可能性。

559
01:11:22.830 --> 01:11:24.180
xinyan: 然后他给我

560
01:11:24.300 --> 01:11:26.570
xinyan: 超出我的预期也是ok的。

561
01:11:27.230 --> 01:11:35.840
Fan Yunjie: 所以您现在对于所有小视频上输出的这种ai的这种输出视频，您是非常嗤之以鼻的，对吧？

562
01:11:36.600 --> 01:11:39.190
xinyan: 你是说微信视频号或者是抖音吗？

563
01:11:39.190 --> 01:11:40.270
Fan Yunjie: 哎对。

564
01:11:42.890 --> 01:11:45.400
xinyan: 我觉得还好。哎，我觉得蛮

565
01:11:45.970 --> 01:11:48.090
xinyan: 因为我觉得这些

566
01:11:48.220 --> 01:11:54.160
xinyan: 花样被玩出来了，它一定是提升了ai的算力和ai的精准度的。

567
01:11:54.360 --> 01:11:57.310
xinyan: 这个就像ai的自我学习和自我成长一样，

568
01:11:57.430 --> 01:12:02.470
xinyan: 我不会拒绝我的一个助手，他不停的在精进自己的

569
01:12:02.780 --> 01:12:07.880
xinyan: 工作能力，因为他还是被我所用嘛。所以我希望他越优秀越好。

570
01:12:09.140 --> 01:12:09.800
Fan Yunjie: Okay,

571
01:12:10.350 --> 01:12:16.260
Fan Yunjie: okay。我觉得我的问题都差不多了，你有没有什么需要补充的。

572
01:12:17.250 --> 01:12:23.010
xinyan: Ok，我觉得也差不多了，因为我给你基本上坦白了，所以我对ai的想法。

573
01:12:23.010 --> 01:12:26.710
Fan Yunjie: 哎呦，真的感谢我先stop recording。


受访人32:
WEBVTT

1
00:00:04.070 --> 00:00:09.879
Fan Yunjie: 好的，好的，那我这边先介绍一下吧，因为本身就是

2
00:00:09.980 --> 00:00:14.279
Fan Yunjie: 为什么会开始采访您呢？就是因为

3
00:00:14.310 --> 00:00:21.570
Fan Yunjie: 我们一开始去做这个项目，主要的一个意向就是看人机交互对应到tms，

4
00:00:21.570 --> 00:00:45.990
Fan Yunjie: pms，在这里指的是这个transactee memory system。它是一种交互记系统或者说交换型记忆系统，就是说每一个在分在这个行为史里面，它是在团队研究里面的一个概念，就是人不一定会跟这个人合作，它也可能是跟机器合作，那我们每一个人，它都会在三个维度上可能

5
00:00:45.990 --> 00:01:05.340
Fan Yunjie: 专长啊，协同啊，就是specialization credibility，还有这个coordination这一块可能去进行一定的这种矩阵式的分配。所以呢，就是借由我们的这个人机交互，现在目前非常的盛行。但是呢，大家在这个过程当中发现了

6
00:01:05.340 --> 00:01:22.810
Fan Yunjie: 可能因为ai逐渐兴起，然后人类逐渐的去减少了这个thinking的一个问题，当然它也会存在一种在团队或者说是在人跟机之间这种协作的一些一些一些怎么说呢？磨合的问题。

7
00:01:22.830 --> 00:01:27.739
Fan Yunjie: 所以说我们就开始了这个研究主要对标的解决问题就是

8
00:01:27.740 --> 00:01:43.619
Fan Yunjie: 在这个ai area的这个这个范畴里面怎么去强化？大家create thinking，或者说是大家在create thinking这一块，怎么能够通过一些产品优化或者底层的逻辑去给它更好的去激化出来，不至于让它去减退。

9
00:01:43.620 --> 00:01:48.350
Fan Yunjie: 所以一开始这个这个subject出来之后呢？我们瞄准的

10
00:01:48.390 --> 00:01:55.250
Fan Yunjie: 一些受众其实是在就是学术领域就academic这一块就是采访了一些这个

11
00:01:55.310 --> 00:02:09.799
Fan Yunjie: research fellow，然后也去采访了一些这个就是professor啊，或者说是学校内部的一些，一些就是manager，或者说是做招生的一些人，但是总体来说，

12
00:02:09.800 --> 00:02:17.570
Fan Yunjie: 我给我们的感觉不是很好，因为他们在学术圈子里面的人都不是太信ai，就导致他们

13
00:02:17.670 --> 00:02:24.180
Fan Yunjie: 完全具备批判性思维的能力，那就跟我们的这个有点相左。所以说我们觉得

14
00:02:24.430 --> 00:02:27.980
Fan Yunjie: 那从efficiency这个角度来说的话，那可能。

15
00:02:28.150 --> 00:02:34.449
Fan Yunjie: 这个professional，或者说是这种business level可能会更好一些，所以我们就

16
00:02:34.720 --> 00:02:52.269
Fan Yunjie: 拓展一下我们的这个sample，然后也是为什么去找到您，我们来去了解一下，就是在这个人机交互的这个范畴，您主要都是有哪一些见解，或者说是有哪些这个合作的想法呀？什么这些的ok，然后

17
00:02:52.270 --> 00:02:55.299
Fan Yunjie: 就先介绍一下您自己吧，好吗？

18
00:02:56.410 --> 00:03:09.790
Gavin Lee: 行，那我自我介绍吧。我，我名字是李伟杰，然后我是来新加坡的，但是我本人是已经在大陆。工作了

19
00:03:09.790 --> 00:03:25.639
Gavin Lee: 快要十年。然后我最开始的行业是融入了广告行业，所以是那种传统广告跑片啊，跟做一些P图，跟想来的给品营销品牌来做一些宣传跟等等，

20
00:03:25.640 --> 00:03:48.169
Gavin Lee: 后面就转行。然后就是现在感觉咨询的公司，他们的业务是正在扩大，因为他们除了提供一些策略的方案，给他们的客户在跟一些还有一些未来的五年，十年的计划，他们也是同时想要

21
00:03:48.170 --> 00:04:03.300
Gavin Lee: 融入一些Poc就叫proof of concept。在创意跟一些设计上就是比如说做一些customer journey就是customer journey等等。还有一些uiux的画面就是

22
00:04:03.300 --> 00:04:12.789
Gavin Lee: 怎么可以让他们的。客户的专案会看起来有。所以我在这三年

23
00:04:12.810 --> 00:04:28.090
Gavin Lee: 一一直在在混在这个行业里吧。所以就后面也是发现这六个月到九个月。确实我们用的ai的usability。

24
00:04:28.100 --> 00:04:28.890
Fan Yunjie: 哼。

25
00:04:28.890 --> 00:04:46.570
Gavin Lee: 也是会融入一些ai的创意跟设计的工具在我们的workflow里来帮我们提高一些。我们的最后要生成的的效果，跟那个给我们这个咨询的部门。

26
00:04:47.190 --> 00:05:03.209
Fan Yunjie: 了解了解，所以就是您从这个for a转到咨询这比较大的一个就职业上内容的差别是什么呀？或者说是比较大的一个工作上的myteology的一个一个转转变是什么。

27
00:05:04.500 --> 00:05:13.260
Gavin Lee: 我个人，我个人觉得就是最大的。我要适应的从事跳到资讯就是，

28
00:05:13.260 --> 00:05:21.189
Gavin Lee: 我要从一个创意的想法的思路，也是要融入一些business objective的思路，因为

29
00:05:21.190 --> 00:05:45.409
Gavin Lee: 之前在纯做广告，就是可能你的想法就是会让这个比方说这个平面要看起来非常的好看，就是哪怕就是有些字体啊，或者你用一些颜色就比较讲究。但这后面就是跟一些这些咨询的同事合作，因为。毕竟他们都是nba毕业的，所以可能他们的思路跟

30
00:05:45.540 --> 00:05:57.860
Gavin Lee: 他们的想法会跟我有点不一样，因为他们会比较跟。跟他们每次喜欢用一些，所以我的。

31
00:05:57.860 --> 00:06:09.900
Gavin Lee: 挑战那时候就是怎么去用创意也是可以就跟他们一起合作，但是不要太多在他们的，因为最后他们是

32
00:06:09.900 --> 00:06:19.700
Gavin Lee: 看revenue为主嘛，就是我们的做的东西其实是一个辅助给他们。更让他们的有一些更大的。

33
00:06:20.980 --> 00:06:36.770
Fan Yunjie: 啊，那其实就是您在整个咨询的project里面，您其实主导辅助的是creative，这个都面对吗？还是说它是一个like corporate strategy跟这个creative management它的一个一个融合。

34
00:06:36.770 --> 00:06:42.339
Gavin Lee: 你，可能你可能去可以理解，就是我们只是一个一个在里面才融合啦。

35
00:06:42.470 --> 00:06:43.370
Gavin Lee: 对对。

36
00:06:43.550 --> 00:06:44.820
Fan Yunjie: 了解。

37
00:06:45.140 --> 00:06:55.869
Gavin Lee: 可能他们有不同的要卖他们的方案，那可能我们的服务给他们就是去填上，在一个在他们整个里面。

38
00:06:56.460 --> 00:07:10.679
Fan Yunjie: 了解。所以从您哪一个阶段开始，您觉得ai它是逐渐融入到这个工作里面的是从这个咨询行业开始吗？还是说之前在包A就有过。

39
00:07:12.810 --> 00:07:31.419
Gavin Lee: 可能是从这个咨询行业吧，因为那那段时间我还在做广告就是这个ai的话题还没有。爆变火吧，到在这个像这一年的这个热点在这里，在这个在这个阶段吧，

40
00:07:31.520 --> 00:07:39.440
Gavin Lee: 所以我才是真正去在ai不同的工具在，可能是在这个年初到现在。

41
00:07:39.980 --> 00:08:00.709
Fan Yunjie: 了解，所以本身这一个热度过来，它是伴随着您本身在工作过程当中就已经引入了很多ai工具，还是说，只不过是这个甲方，他们更大程度上强调这个热点的热度，然后希望可以去通过这样的热度去进行一波炒作之类的。

42
00:08:01.390 --> 00:08:18.550
Gavin Lee: 有一个我觉得是两个两个不同的在去。去摸索这个ai吧，就是一个是我上面的领导，在自己的创意部门，他们也是在一直在鼓励我们去。

43
00:08:18.620 --> 00:08:43.169
Gavin Lee: utilize的一些工具嘛，就可能他们也是想要看有哪一些工具可以帮我们。进步一下。项目的或者项目的一些质量在，会给客户给我们那个consulting的主看，也是我们能做更好的东西，就是不用

44
00:08:43.169 --> 00:08:47.359
Gavin Lee: 去依赖跟靠著我们每天用这样adobe。

45
00:08:47.360 --> 00:09:00.669
Gavin Lee: 还是去纯粹一个一个去或者一个一个去排版。所以，这个这个是第一个方面吧，然后，然后第二个方面。可能你可以理解是。

46
00:09:00.720 --> 00:09:10.150
Gavin Lee: 我也是不想被拉拉到就是后面的一些

47
00:09:10.170 --> 00:09:23.509
Gavin Lee: 一些情况就是我会用我以前的领导在职业广告公司做一个比例，因为我以前我看到他们，可能他们已经还在卡在那个20年代的广告，他们会以为就是黄黄。

48
00:09:23.510 --> 00:09:36.050
Gavin Lee: 黄金年代的广告圈吗？就是我要拍一个50分五分钟还是三分钟的，一个一个一个一个tpc嘛，但确实

49
00:09:36.050 --> 00:09:46.419
Gavin Lee: 我觉得就是现在很多消费者，他们的监督已经更新都已经变，都是运到在品牌上像抖音啊小红书啊，上。

50
00:09:47.050 --> 00:10:00.879
Gavin Lee: 所以我就会觉得，我不想被嘛，所以我个人还是会比较担心，就是可能我十年后我，我也是还是要希望我还可以继续工作吧。所以你可以。

51
00:10:00.880 --> 00:10:01.369
Fan Yunjie: 这样子。

52
00:10:01.370 --> 00:10:04.279
Gavin Lee: 等于理解，是一个一种吧，给自己。

53
00:10:04.560 --> 00:10:17.869
Fan Yunjie: 明白。所以说，伴随着这种就是ai的disruption，你您觉得就是对于团队，或者说您个人的工作的这个想法还有思路会发生很大的变化吗？

54
00:10:20.380 --> 00:10:27.729
Gavin Lee: 最开始变化蛮严重的，因为可能团队会

55
00:10:27.860 --> 00:10:41.029
Gavin Lee: 对这个新的工具会比较害怕吧，因为我觉得就是，不管是ai，我觉得就是你去学，或者你去尝试一些新的爱好或者新的活动，你肯定在前面的

56
00:10:41.320 --> 00:10:53.340
Gavin Lee: 前前几个课都会比较有点害羞，或者有点犹豫，或者你会怀疑自己。哎，为什么我，我在做这个东西，我可能他会觉得我做不好。

57
00:10:53.340 --> 00:11:02.389
Gavin Lee: 这个我觉得是每个人会遇到的情况，但是从我会用一个

58
00:11:02.390 --> 00:11:15.570
Gavin Lee: 不同的角度来自己吧，就是可能我是有这个想法跟这个思路已经知道我需要跟进步自己的能力跟一些不同的吧，

59
00:11:15.570 --> 00:11:38.690
Gavin Lee: 所以后面就是去慢慢去。影响跟我的团队吧，其实我我知道就是不是每个人都很合适愿意去一些新的吧。但我个人觉得就是，可能我从我的角度就是我，如果我能做到

60
00:11:38.690 --> 00:11:56.469
Gavin Lee: 会希望也是可以。让我的，自己的团队或者身边的人也是有有点小的兴趣去至少知道这个东西其实是一个帮助也，而而且不是一个在给你一个添麻烦的事情。

61
00:11:56.760 --> 00:12:10.739
Fan Yunjie: 了解，所以除了本身的自己思路上的一些转变以外，您觉得现在大家跟ai协作的一个关系，或者说是ai比较擅长的，在这个creative这个领域都是哪些。

62
00:12:12.690 --> 00:12:33.660
Gavin Lee: 是帮我们减少一半的速度，在很多很多作品上就是给你一个很很很比较很容易的例子啦。就以前以前比方说我们要做一些笔稿给笔给客户我们的方案或者我们的一些创意的。

63
00:12:33.660 --> 00:12:35.590
Gavin Lee: 然后有一些图

64
00:12:35.590 --> 00:12:53.469
Gavin Lee: 我们是叫啦，就是你要出，你要跟展现出来，你的创意的是什么？就是不能纯粹靠一些文字在旁边，因为通常会配一个画面去。描述描述这个。所以，

65
00:12:53.920 --> 00:13:13.250
Gavin Lee: ai，我觉得就是它虽然最开始没有很准，但是它能帮你输出一些好的。让你减少在你这个工作的的速度跟一些那个重量上，

66
00:13:13.280 --> 00:13:26.180
Gavin Lee: 就比方说以前会做一个demo，可能要花两个钟或者三个钟甚至三个钟，但是你有ai，你可以省一半的时间，在这个这个这个过程当中。

67
00:13:27.340 --> 00:13:35.980
Fan Yunjie: 了解您方便介绍一下，比方说现在一个新的project过来，您从头至尾跟ai协作的一个workflow，大概是怎么样的吗？

68
00:13:37.320 --> 00:13:50.459
Gavin Lee: 我可以稍微简单描述一下最近我给一个香港的咨询的同事帮他们。做一个

69
00:13:50.640 --> 00:14:01.009
Gavin Lee: 转型化的方案吧，因为他们的目的，他们的客户其实是香港的国际机场，然后他们的目目标很简单，他们想要

70
00:14:01.010 --> 00:14:25.829
Gavin Lee: 给他们的客户看。就是啊，以后的五年，十年，就是对于每个不同的人。经过跟去香港机场，他们要提供一个的线上的就是完全都是有不同的，就是比方说他们先去取一下机票，然后取一下那个登机牌，然后到了一个不同的

71
00:14:25.830 --> 00:14:36.069
Gavin Lee: 他们可以去体验。一些购物在机场里面，然后后面就是继续不同的体验点，所以

72
00:14:36.070 --> 00:14:41.610
Gavin Lee: 他们其实那段时间，给我们这个的时候，

73
00:14:41.610 --> 00:15:06.590
Gavin Lee: 我们他们有自己的想法，他们本来以前的老套路的想法就是画一些插画的，然后展现出来就是很平很平面的东西就是很静态的。但我们后面就跟他们讨论就觉得就是这个东西确实是对，但我们可以帮你提高用视频的方式来展现这个，

74
00:15:06.590 --> 00:15:07.539
Gavin Lee: 那么jenny。

75
00:15:07.620 --> 00:15:26.780
Gavin Lee: 所以我们去。给他说服他。其实我们用视频的方式来展现这个，因为每个最多你能展现三到四个touch point，然后这个会让我们的工作会比较好一点控制嘛，

76
00:15:26.780 --> 00:15:49.309
Gavin Lee: 那我们就开始那个就是当然是跟咨询的。同事先理一下那个每个touch point的一些场景跟等于就是一个脚本一样，就是每个场景，这个conser是在干嘛。然后从那个场景我们会拿它来输入一些prompts，然后用。

77
00:15:49.310 --> 00:16:01.060
Gavin Lee: 我们这个创意跟设计的工具，像还有甚至去生成一些的素材，

78
00:16:01.210 --> 00:16:12.800
Gavin Lee: 然后每一个素材跟每个镜头我们跟他们过一下，如果他们满意的话，后面我们会用。还是要回复用。

79
00:16:12.800 --> 00:16:22.839
Gavin Lee: 比较manual一点啦，就是你要拿这四到五个的蔬菜，然后剪到一个50秒或者40秒的完全的饰品。

80
00:16:22.840 --> 00:16:28.700
Gavin Lee: 所以，这个workflow是有这个大概的流程。

81
00:16:29.530 --> 00:16:34.119
Fan Yunjie: 你学过就是engineer相关的，这个

82
00:16:34.550 --> 00:16:40.389
Fan Yunjie: 就是不论是在公司里面的培训，还是说您个人有学过，就是prompt engineer相关。

83
00:16:40.600 --> 00:16:46.889
Gavin Lee: 都是大部分都是内部的分享，跟个人的自己的解说吧。

84
00:16:47.370 --> 00:17:01.459
Fan Yunjie: 了解，您觉得就是在这个过程当中，您给ai去喂它的提示词是一个比较怎么好去驯化它，能更好的理解或者更好的产出你想要的这个产物。

85
00:17:01.460 --> 00:17:06.420
Gavin Lee: 这个感觉这个好问题，因为最开始我们就

86
00:17:07.160 --> 00:17:09.400
Gavin Lee: 那个项目，其次有三个视频，

87
00:17:09.579 --> 00:17:13.030
Gavin Lee: 所以最肯定你做第一个视频的时候啊。

88
00:17:13.030 --> 00:17:16.640
Fan Yunjie: 就是你肯定会遇到很多不同的。

89
00:17:16.640 --> 00:17:36.310
Gavin Lee: 生成出来一些奇奇怪怪的素材跟一些视频出来，所以就是你的前面两个小时都在解锁。如果你输入那些的时候，是不是这个ai工具能不能符合你的要求，

90
00:17:36.310 --> 00:17:38.289
Gavin Lee: 所以都是

91
00:17:38.290 --> 00:17:56.389
Gavin Lee: 一种测试的的过程当中，在前面的两个小时甚至三个小时。后面就是如果你找到对对的，来喂到你的ai。我们就做了那个视频。二，跟视频上比较顺利多一点。

92
00:17:56.540 --> 00:18:10.590
Gavin Lee: 所以。大部分都是你刚开始做的就是你要去解锁，跟去知道你要用哪一个比较对的字去做一些一些深层的素材，

93
00:18:10.900 --> 00:18:19.739
Gavin Lee: 这样会你才能知道可能有一些关键字，我不能用这样，我毕竟我要用另外一个方面。

94
00:18:20.510 --> 00:18:37.020
Fan Yunjie: 您可以比较细化的告诉我一个例子吗？比方说您目标是产生一个什么样的画面，然后通过什么样的，这个prompt会直接可以比较efficient去达到，然后，通过什么样的这种prompt可能会去引，就是误导他之类的。

95
00:18:38.020 --> 00:18:49.410
Gavin Lee: 你要细一点啦就okay等一下。我想一下，这个可能就需要点时间。

96
00:18:49.410 --> 00:18:51.279
Fan Yunjie: 没关系。

97
00:18:52.400 --> 00:19:09.439
Gavin Lee: 可能就是我刚回到我说，就是每个镜头我们已经跟我们的同事去有一个就是知道，每个是长得什么样子，所以我就给你一个。简单的啦。就是如果是镜头一，

98
00:19:09.530 --> 00:19:21.649
Gavin Lee: 他们需要那个人进来机场的时候去那个台子去。取他们的登机牌跟取他们的飞机票，

99
00:19:21.810 --> 00:19:32.909
Gavin Lee: 然后就是首先就是那个workflow我们先去用。我们跟客户align的一些描述的字输入在里面，所以比方说，

100
00:19:32.910 --> 00:19:57.820
Gavin Lee: 帮我帮我生成两张女生去进来机机场，他们的样子要看起来像零零后。后面就是他们在这个台。那个柜柜台就是需要取一下他们的护照，哎，取一下他们的登机牌跟飞机飞机票，然后我们的环境外面需要看起来是比较阳光比较好，

101
00:19:57.820 --> 00:20:04.079
Gavin Lee: 比较亮，就是不要看起来太暗，然后是一个现代的机场，像香港机场一样，

102
00:20:04.200 --> 00:20:27.399
Gavin Lee: 所以是通过一个第一个的测试吧，然后我们才能生成一些四张或者四个还是三个不同的素材，然后我们才可以开始做那个对比如他给我生成这个。不管是视频还是图片，是不是符合我们的方向

103
00:20:27.400 --> 00:20:40.190
Gavin Lee: 要走，因为我们前面也是有定一些。方方向去follow一下那个关键字嘛，就是客户。就是我们的同事，也是有给我们一些keywords吧，我们要follow。

104
00:20:40.830 --> 00:20:46.309
Fan Yunjie: 了解，所以他其实在这个过程当中，您觉得

105
00:20:46.410 --> 00:20:59.600
Fan Yunjie: 就是您的目的是为了让他不断的通过您prompt，可以让他去更加了解您的这个思路，所以说您也不会去吝啬这个时间成本去去纠正他，去跟他对吗？

106
00:20:59.980 --> 00:21:03.939
Gavin Lee: 对对对，你可以，你可以这么例子。

107
00:21:04.170 --> 00:21:09.000
Fan Yunjie: 一般来说就是大概几轮可能会生出一个比较，就是。

108
00:21:09.880 --> 00:21:10.460
Fan Yunjie: 什么。

109
00:21:12.060 --> 00:21:31.870
Gavin Lee: 大概四轮吧四到五轮，因为你就是就是有一些字，就是你要稍微换一点，就是我就是okay。就比方说，那天我刚好也是跟我那个一起的团队一起也生成一些镜头给。那个视频，然后，它

110
00:21:32.290 --> 00:21:48.489
Gavin Lee: 可能是写一些，就是会让那个ai工具不是很认可他想要的意思。我举一个例子啊，等一下，我先需要找那个聊天记录，因为那天晚上

111
00:21:50.580 --> 00:21:51.840
Gavin Lee: 你给我

112
00:21:57.000 --> 00:22:06.219
Gavin Lee: 等一下，你给我，你给我一分钟来找那个聊天记录。我记得有一个情况非常的可以描述出这个问题，

113
00:22:47.210 --> 00:22:49.750
Gavin Lee: 哎，我突然找不到。

114
00:22:50.780 --> 00:22:57.380
Gavin Lee: 我我们可以回到这个问题等会吗？如果我找得到，还是你要继续下一下的问题。

115
00:22:58.730 --> 00:23:12.789
Fan Yunjie: 可以没关系。okay，那其实在整个您跟这个ai去进行合作过程当中，它更大程度上只是支持支持您这个去生成一些material，是吗？

116
00:23:13.160 --> 00:23:19.519
Gavin Lee: 其实其实从我的工作来说，就是生产一些不同的

117
00:23:19.760 --> 00:23:26.630
Gavin Lee: 创意素材啦，就简单来说，就是都是或者类似多一点。

118
00:23:27.080 --> 00:23:40.560
Fan Yunjie: 所以就是在这个生成之前的这个。这种strategy就是个人的想法，或者说是个人对于这一次素材的把控这一类的东西是不会去依赖ai的，对吗？

119
00:23:41.940 --> 00:23:47.220
Gavin Lee: 是个好问题，确确实。

120
00:23:48.090 --> 00:23:57.309
Gavin Lee: 我觉得如果我理解清楚，就是可能就看每个人的不同的工作，创意的工作习惯吧。

121
00:23:57.750 --> 00:24:03.429
Gavin Lee: 就如果你想依赖ai，那

122
00:24:03.550 --> 00:24:20.259
Gavin Lee: 像我，可能我现在很多东西，甚至啊，就是就是有点，这个是会可能会偏那个话题一点就是，甚至如果我，我，我，我要回复一个邮件给我领导，我会用，或者帮我去那个邮件出来。

123
00:24:20.730 --> 00:24:40.540
Gavin Lee: 对，就是可能我。那天我有点懒，回复这么多邮件，但是我知道我需要去回复这个邮件汇报给领导，那我会简单去跟或者跟他说我的目的是什么，我需要帮我一个邮件就是有几个，然后

124
00:24:40.640 --> 00:24:48.640
Gavin Lee: 这个方式是我可以承认我在依赖ai，在在一些workflow，我在做一些东西。

125
00:24:49.700 --> 00:24:58.819
Fan Yunjie: 在前期的就是这种insert形成到generate这种素材的，这个期间肯定还是需要有一个这种。

126
00:24:59.100 --> 00:25:00.140
Gavin Lee: 在前面。

127
00:25:00.140 --> 00:25:07.870
Fan Yunjie: 对research去lead你，然后或者说是有一个比较完善的一个思路去带着这个ai去跑。

128
00:25:08.710 --> 00:25:13.589
Fan Yunjie: 路的形成，很大程度上是依赖您个人，还是说也会有ai的成分在对。

129
00:25:14.470 --> 00:25:20.649
Gavin Lee: 我觉得现在是占比5000分之50吧。两边因为可能

130
00:25:20.980 --> 00:25:34.229
Gavin Lee: 虽然我知道。客户的要求是什么，但可能我只是我只是在表面上会比较基础上知道okay，比方说他们需要我做一个平面，

131
00:25:34.230 --> 00:25:56.759
Gavin Lee: 但是他只是给我一些。关键字，就是说需要那个平面看起来像。我就打个比方就是需要有零零后有比较现代感，会比较会吸引一些年轻人的的风格，但它用这些关键字就是非常还是很大众就是每个人的。

132
00:25:59.660 --> 00:26:19.619
Gavin Lee: 观点跟审美有点不一样嘛。所以其实在这个方面，我自己也是很难去判断。所以我会。靠著ai去跟我一起吧，其实可能我会打出这个来，但是它会给我多一点的更多的。方向我可以去尝试，

133
00:26:19.710 --> 00:26:38.979
Gavin Lee: 那会导致会会影响我做出来的平面，可能之前我只想做两个不同的方向，但是ai给我一些不同的关键字跟一些比较好的。设计的方向我可以做出来三个甚至四个。

134
00:26:39.480 --> 00:26:45.720
Fan Yunjie: 了解，所以其实在这个过程当中，它更大程度上是帮您去发散您的思维，对吗？

135
00:26:45.720 --> 00:26:55.880
Gavin Lee: 对，你可以说是帮我对啊，发展跟发展我的那个想法吧。然后可能他是一个跟我的伙伴吧。

136
00:26:56.430 --> 00:27:00.679
Fan Yunjie: 那他在这个过程当中输出的这一些

137
00:27:00.830 --> 00:27:06.629
Fan Yunjie: 案例，或者说是输出的这种方法有多大程度上是可行的。

138
00:27:07.830 --> 00:27:16.599
Gavin Lee: 我现在感觉现在是60%可行的，因为可能40%都是。

139
00:27:16.830 --> 00:27:20.799
Gavin Lee: 他可能是在网上去

140
00:27:20.990 --> 00:27:43.929
Gavin Lee: 去搜一下。现在。比方说回到那个话题了，就是零，零后就是他们喜欢的观点跟一些吸引的东西是什么，所以可能不是说百分之百会达到那个的要求，就是他可能只是给你一个大广的方向，然后后面就是你还是要靠人家去

141
00:27:43.930 --> 00:27:46.530
Gavin Lee: 筛选哪一个会比较合适吧。

142
00:27:47.640 --> 00:27:55.959
Fan Yunjie: 那您觉得就是60跟40这个这个percent你是怎样判断出来的呢？是靠您的经验吗？还是。

143
00:27:56.650 --> 00:28:06.689
Gavin Lee: 一个是从工作的经验吧。一，另外一个是，我，我会多了解我自己的客户的要求，

144
00:28:07.320 --> 00:28:09.090
Gavin Lee: 才会做这个判断。

145
00:28:09.090 --> 00:28:09.970
Fan Yunjie: 明白。

146
00:28:10.140 --> 00:28:15.159
Gavin Lee: 那如果说他在这个过程当中输出的这个结果。

147
00:28:15.160 --> 00:28:30.739
Fan Yunjie: 它是可能60%可行，您会去一直的跟他去进行磨合，让他输出一个就是最完整的，最相对来说最符合您要求的这种方法，还是说线下的工作就是由您自己来做了。

148
00:28:32.100 --> 00:28:36.810
Gavin Lee: 哎sorry，你可以再重复吗？我没我没catch到你的那个问题。

149
00:28:37.080 --> 00:28:48.690
Fan Yunjie: 就是因为它本身它生成出来的，这种方法可能只有like sixty percent它是可行的，所以说您可能需要跟它进行better进行磨合，它才能。

150
00:28:48.690 --> 00:28:49.400
Gavin Lee: 不断的。

151
00:28:49.400 --> 00:29:04.859
Fan Yunjie: 优化，那这个过程当中，您会去跟他去better吗？还是说整个他输出60%。您给了自己的这个。这个判断之后，剩下的工作就不会再交给ai去做了，是要交给您自己来做了，您是会选择让他不断地输出到最后的结果。

152
00:29:04.970 --> 00:29:07.169
Fan Yunjie: 比如说你去进行。

153
00:29:07.170 --> 00:29:12.010
Gavin Lee: 08这个可能是我个人的习惯了，我不会跟他去。

154
00:29:12.210 --> 00:29:26.370
Gavin Lee: 跟去让他为我更多的方案吧。可能因为我本身已经知道我大概的。方向跟是什么？就是我需要，他就是帮我补充

155
00:29:26.370 --> 00:29:36.590
Gavin Lee: 一些在我这个想法里面，然后我才个人会自己去拼在一起，所有的我自己想要拟出来的方案吧。

156
00:29:37.110 --> 00:30:00.780
Fan Yunjie: 了解，但是我觉得这个有一个非常大的一个先决条件在于您是从没有ai的时代，就像我们都是嘛，就是一开始，可能我最最早的时候也在方面工作，然后可能在那个时候，大家就连写个邮件都是需要自己去写的，完全没有说什么工具替代这一说，所以大家是需要用特别多的头脑风暴去解决，很多实际的案例

157
00:30:00.790 --> 00:30:04.709
Fan Yunjie: 才会生成这种经验，去跟ai去

158
00:30:04.840 --> 00:30:24.639
Fan Yunjie: 怎么去调校他的这个输出的方向。然而现在很多就是我不知道您现在的就是现在手底下的像internet什么，他们大概都是什么年纪，他们基本上进到这个岗位，或者进到这个行业里来，就已经是一个ai驱动的一个时代了，您觉得他们会不会去，

159
00:30:24.660 --> 00:30:34.290
Fan Yunjie: 就是缺少这一部分判断的实力，或者说是本身过分的去依赖这个ai工具，然后没有自己的就是可以批判的这个能力呢。

160
00:30:35.490 --> 00:30:47.050
Gavin Lee: 哇，这个是好问题喔，我，我，我个人就是我自己研究在身边的团队里面，

161
00:30:47.310 --> 00:30:51.490
Gavin Lee: 可能现在的

162
00:30:51.830 --> 00:31:02.669
Gavin Lee: 比我年轻的一些同事，他们会比较。愿意跟比较，会乐意去尝试这些。不同的ai。

163
00:31:02.670 --> 00:31:03.430
Fan Yunjie: 对啊。

164
00:31:03.430 --> 00:31:07.240
Gavin Lee: 的工具吧，就是他们不会觉得就是

165
00:31:07.350 --> 00:31:14.280
Gavin Lee: 对这个事情会比较害怕啦，他们会比较愿意去尝试。然后如果他们真的失败，

166
00:31:14.380 --> 00:31:17.740
Gavin Lee: 他们会当成这个是一个学习的当中嘛。

167
00:31:18.300 --> 00:31:19.989
Gavin Lee: 但是。

168
00:31:19.990 --> 00:31:28.599
Fan Yunjie: 但是他们的经验是从哪儿得来的呢？就变成了从这种ai失败的经验里面去总结吗？然而不是通过一些实际的案例吗？

169
00:31:28.600 --> 00:31:41.670
Gavin Lee: 就是我觉得他们都是看互联网，就是比方说有一些不同的ip啊，或者有一些大师啊ai的大师在，每天或者每周发一些不同的内容，

170
00:31:41.670 --> 00:31:51.799
Gavin Lee: 可能他们是从这个渠道去解锁，跟去尝试一下这个不同的ai的能力吧。

171
00:31:51.920 --> 00:32:02.949
Gavin Lee: 所以可能他们去消化跟看的内容也是一个也是一个会会帮助他们这个学习啦，

172
00:32:03.230 --> 00:32:13.620
Gavin Lee: 就是他们可能不会有一些比较。真实的专案，可以给他们去有一些练习，但可能他们的另外一个方式是通过这个渠道。

173
00:32:14.120 --> 00:32:19.620
Fan Yunjie: 了解您的意思是就是本身之前的，需要长期的经验

174
00:32:19.660 --> 00:32:38.359
Fan Yunjie: 去磨合出来的这些方法现在可能通过一些比较快速的一些总结就可以达到了什么，或者说是通过这种比较高速率的这个ai的工作，旅游，大家其实也不会是成为现在小朋友去做工作，没有没有自己思维的一个能力，对吗？对。

175
00:32:39.000 --> 00:32:40.060
Gavin Lee: 对对。

176
00:32:40.420 --> 00:32:57.069
Fan Yunjie: 了解，了解。所以在这个过程当中，您觉得现在团队来讲的话有多大程度上有一些工作就是可以被ai去替代掉了，或者说是可能某一些职能就可以被ai替代掉了。您觉得呢？

177
00:32:58.680 --> 00:33:02.730
Gavin Lee: 有什么好的话题欸，这个其实

178
00:33:04.310 --> 00:33:09.510
Gavin Lee: 这个其实我跟我身边的朋友，去讨论过吧。

179
00:33:10.120 --> 00:33:21.929
Gavin Lee: 我现在不是能可以给一个明确的答案就是有多少的比例会被ai，

180
00:33:22.220 --> 00:33:25.029
Gavin Lee: 被替换。因为

181
00:33:25.770 --> 00:33:33.800
Gavin Lee: 我以前有参参加过一个论坛，是跟ai相关的话题，我记得就是有一个

182
00:33:33.810 --> 00:33:53.250
Gavin Lee: 有一个那边。那时候有说一些，他对ai的内容会怎么样。然后我我记得他说一句话就是，人，还是会有他的存在。在这个ai，因为他说就是

183
00:33:53.250 --> 00:34:02.630
Gavin Lee: 我们每天的问题就是人创出来的，所以最后你还是需要人去解决人创出来的问题，对。

184
00:34:03.280 --> 00:34:06.000
Fan Yunjie: 这个世界最大的bug在于人的思想。

185
00:34:06.000 --> 00:34:07.769
Gavin Lee: 最高的棒确实是人不是。

186
00:34:07.770 --> 00:34:08.300
Fan Yunjie: 所以说。

187
00:34:08.300 --> 00:34:23.020
Gavin Lee: 那虽然就是你有ai或者你有别的东西帮你去提高跟解决一些workflow还是一些，或者一些里面帮你缩小还是变小，但是你最大的

188
00:34:23.080 --> 00:34:37.300
Gavin Lee: 你最大的创出来的问题还是存在，就是哪怕就是如果，除非我们的人，人，人类都消失，那那可能都是被被被机器人都被替换

189
00:34:37.560 --> 00:34:39.250
Gavin Lee: 对，所以我记录，

190
00:34:39.489 --> 00:34:53.970
Gavin Lee: 我现在不是很确定还会占比多少啦，但我，但我，还是我会比较自信，就是知道可能是从创意的行业可能会影响我们比较大啦，

191
00:34:54.000 --> 00:35:03.660
Gavin Lee: 就是像很多工程师在做一些。行业，他们的他们的代码的岗位都被ait换嘛。

192
00:35:03.710 --> 00:35:16.709
Gavin Lee: 我觉得创意的创意的行业。不管是你做纯设计还是广告，还是你是那种辅助的部门，来帮助一些甲方里面的

193
00:35:16.710 --> 00:35:26.450
Gavin Lee: 设计或者创意，我觉得可能会四分之一会慢慢被替换啦。就是可能以后你只是需要一个人去

194
00:35:26.490 --> 00:35:38.790
Gavin Lee: 做沟通，做一些方向的。一些一些跟客户或者你内部，然后剩下都是可能是给ai操作。

195
00:35:39.270 --> 00:35:46.029
Fan Yunjie: 明白了解。所以说，但是我觉得这个其实是存在一个问题在于大家

196
00:35:46.060 --> 00:35:51.100
Fan Yunjie: 都如果使用ai的话，就比方说您刚刚提到

197
00:35:51.100 --> 00:36:12.789
Fan Yunjie: 再去跟这个一个客户去沟通的这个情况之下，会先把自己的这些想法消化掉，然后再喂给ai，然后看他能不能去发散，然后再继续的去看能不能去输出一些去把您的思维更好的去产出的一些结果。那如果说

198
00:36:12.790 --> 00:36:15.040
Fan Yunjie: 大家都通过这种方式，

199
00:36:15.350 --> 00:36:31.470
Fan Yunjie: 或者说因为prompt它本身的限制性也会有然后。但是呢，人的思维我觉得还是无限的，那这种情况下会不会导致未来所有的品牌可能输出的产物都比较同质化的。这个问题。

200
00:36:33.090 --> 00:36:43.720
Gavin Lee: 这个这个是我一直说实话，这个很难去，怎么去回复我。

201
00:36:44.000 --> 00:36:45.110
Fan Yunjie: 呵。

202
00:36:47.010 --> 00:36:57.630
Gavin Lee: 可能可能我现在会比较观点去看一下在这个情况上，

203
00:36:58.320 --> 00:36:59.880
Gavin Lee: 等一下。我想一下。

204
00:36:59.880 --> 00:37:00.330
Fan Yunjie: 就是。

205
00:37:00.330 --> 00:37:10.800
Gavin Lee: 其实这个点其实这个点就是看

206
00:37:11.500 --> 00:37:16.040
Gavin Lee: 看那个行业的热点是要走哪一个方向啦。

207
00:37:16.670 --> 00:37:30.419
Gavin Lee: 我不知道，可能我说这个东西会不会吧，就是我现在是预预感的到就是okay。就比方说以前疫情的时候人家不能旅游，人家不能去。

208
00:37:31.240 --> 00:37:50.469
Gavin Lee: 人家不能去旅游，人家不能去去去出国嘛。然后就是他们已经呆在家里，只能在面对电脑去做东西，或者在消化他们的内容，然后就是可能有一个有有一个品牌就是品牌，或者有个创始人有想法说，诶，我们要不做一个元宇宙，

209
00:37:50.470 --> 00:37:56.129
Gavin Lee: 那这样人家还可以有一些互动，虽然是在线上上，但这种

210
00:37:56.390 --> 00:37:59.770
Gavin Lee: 其实最开始是像我们的

211
00:37:59.790 --> 00:38:12.130
Gavin Lee: 线上的沟通的app嘛，但就是你就会让人家去创出来跟定制他们自己想要的在线上，然后

212
00:38:12.130 --> 00:38:26.069
Gavin Lee: covid就就是后面疫情就没了。然后这个需求也是后面就慢慢，这个也是没骗过，就是甚至很多公司投入很多钱在前面，在这个愿意做，但是后面就是这个东西也是凑不齐啊，继续不了。

213
00:38:26.070 --> 00:38:41.769
Gavin Lee: 然后就是我觉得可能是从源语就可能这个我现在可能是在自己猜测吧，就是这个ai想法就慢慢去加入在里面，然后ai就是变成另外一个。

214
00:38:41.800 --> 00:38:44.680
Gavin Lee: 话题从以前

215
00:38:44.880 --> 00:38:53.760
Gavin Lee: 最开始做他们的那个人的那个机器人。然后不同的品牌在深圳也是做一些

216
00:38:53.760 --> 00:39:05.299
Gavin Lee: 机器狗啊，类似等等。然后每个人想要抓住那个热点，因为他们觉得市场上有这个需求，每个人想要往这个方向去。赚钱，

217
00:39:06.510 --> 00:39:12.350
Gavin Lee: 那我现在也是不能去保证，就是ai会

218
00:39:12.860 --> 00:39:20.820
Gavin Lee: 会会在我们的人的当中会会会会存在多久啦？所以只能

219
00:39:20.990 --> 00:39:24.010
Gavin Lee: 我现在做的东西都是看一步走一步吧。

220
00:39:24.490 --> 00:39:28.589
Fan Yunjie: 明白，所以您觉得一个是本身

221
00:39:28.850 --> 00:39:47.790
Fan Yunjie: 阶段性的输出，就像您说的，看，一步走一步，然后不断的去调校，这个是个事情，然后另外一个就是您认为品牌本身，它的like uspsp，它这一个部分要首先定位的很好，才能去保保证这个不会同质化的。这个问题对吗？对。

222
00:39:48.160 --> 00:39:51.000
Gavin Lee: 你可以这么去理解吧。

223
00:39:51.370 --> 00:40:07.250
Fan Yunjie: 但是就比方说像现在，小米，华为，然后或者说是还有什么品牌oppo，他们都同步的再去做这个某一个某一个新的这个手机like fold。这是什么？我觉得。

224
00:40:08.240 --> 00:40:27.670
Fan Yunjie: 但是在这个同时，其实大家的这个key proposition或者是这个unixel proposition，其实我觉得还是会比较难拿捏这个点。如果说在我们说不是一些特殊的这个时代上就是在这种普通的，我们的这种比较大众化的一些输出方向上，

225
00:40:27.820 --> 00:40:46.000
Fan Yunjie: 那如果这种情况之下，会不会比方说现在同样这三家，我找了publicis，我找了这个oj，v，我找了谁，我们三家同时出来比较，大家的感觉差不多，大家同时说出的感觉差不多少，会不会有这个问题，因为现在这种ai工具的这种

226
00:40:46.000 --> 00:40:52.630
Fan Yunjie: 过分的被使用，或者说是太过于就是覆盖了一些workflow。

227
00:40:53.950 --> 00:41:00.930
Gavin Lee: 这个是好问题，因为可能

228
00:41:01.330 --> 00:41:20.699
Gavin Lee: 是你会遇到一种情况，就是比方说有三家公司在比搞一个专案，可能你会看他们的方案出来都是啊，百分之百八十或者70，甚至90，看起来也是差不多一样，但我觉得最

229
00:41:20.920 --> 00:41:36.860
Gavin Lee: 可能是。那些员工，他们是怎么用自己的方式去说服，或者甚至甚至卖他们的方案给他们的，对方的的客户，

230
00:41:37.050 --> 00:41:53.180
Gavin Lee: 可能我觉得这个还是回到人的能力了，就是ai不会是一个工具，可以帮你去有一些。比较能

231
00:41:53.740 --> 00:42:01.589
Gavin Lee: 代表出来一些特别的跟你的竞争对手有点在这个这个方面上吧。

232
00:42:01.880 --> 00:42:04.109
Fan Yunjie: 所以就变成了说

233
00:42:04.140 --> 00:42:23.490
Fan Yunjie: 现在大家的比稿都一样，甚至说其实我觉得这个其实可以进推到一步，到消费者就是消费者，看这三款手机也都差不多，反而在大家谁更会讲故事，谁更能会去就是变成了一个一个key point在seller，在在在在sales的这个层面上了，是吗？

234
00:42:24.150 --> 00:42:34.219
Gavin Lee: 对，我觉得可能是有几个factor啦，一个是storytelling，一个是你的，你的就是会吸引到人家。

235
00:42:34.520 --> 00:42:41.919
Gavin Lee: 另外一个就是那个品牌或者那个方案可以带什么给对方，

236
00:42:42.280 --> 00:42:51.780
Gavin Lee: 可能很多人在看在，就是比方我，我可能会看那个可以带我生活上有什么好的好处吧，

237
00:42:52.150 --> 00:43:00.410
Gavin Lee: 就是如果没有带我的生活上没有什么好处，那可能我会考虑第二个fact就是那个吧。

238
00:43:00.810 --> 00:43:06.439
Gavin Lee: Okay，okay，明白。所以本身其实在后期的比方说。

239
00:43:06.440 --> 00:43:20.820
Fan Yunjie: 趋势性的一些提炼呀，或者说是一些数据整合呀，或者说是。本身输出方案，它的一个feedback的的一个衡量上ai，它仍然也会有一个很好的效果。

240
00:43:22.760 --> 00:43:27.110
Gavin Lee: 哎sorry，你可以再说吗？我没没听清楚，刚才有点断线。

241
00:43:27.170 --> 00:43:36.109
Fan Yunjie: 比方说就是，我们现在输出一个方案，您大概给他的一个，这个打分大概是一个85分，

242
00:43:36.250 --> 00:43:47.820
Fan Yunjie: 然后。然后呢？它是一个完全ai generated的一个方案，那他最后feedback的一个分数能真的可以达到这样的一个85分的这个程度吗？对，

243
00:43:50.030 --> 00:43:51.680
Fan Yunjie: 从法来讲。

244
00:43:52.350 --> 00:43:53.460
Gavin Lee: 可能不会吧，

245
00:43:54.370 --> 00:44:00.960
Gavin Lee: 就是就是那个ai在给人家打打分，在每个人的方案上，对吧？就是举个例子。

246
00:44:01.890 --> 00:44:11.320
Fan Yunjie: 可能更大程度上在于比方，我们现在完完全全用ai出来一个poster，或者说是一个小的campaign，

247
00:44:11.320 --> 00:44:23.680
Fan Yunjie: 然后我们这边去。Evaluate未来的一个一个feedback大概会是一个怎么样的产出，那就想要知道ai在这里到底能够去，如果完全让ai去做的话，它能达到多少？

248
00:44:23.680 --> 00:44:27.330
Fan Yunjie: 如果说是能够跟人相比的话

249
00:44:27.460 --> 00:44:30.719
Fan Yunjie: 是会比纯人类更好一些，这样。

250
00:44:31.800 --> 00:44:40.060
Gavin Lee: 可能他们只能去判断跟打分，在没在没在比较基础上的。

251
00:44:40.110 --> 00:44:56.050
Gavin Lee: 跟一些吧，就是举一个例子，我我前几天也是给一个中东的咨询。同事在做一些，他们的话题也是ai转型，但是他们的

252
00:44:56.050 --> 00:45:09.109
Gavin Lee: 话题会比较特别，因为他们就要展现出来。在一个前后的有ai的，在每个岗位上，所以他们其实举很简单的例子就是

253
00:45:09.110 --> 00:45:20.700
Gavin Lee: 像老师，老师就是他们的工作上就是除了教课，然后培训同学，在他们的学校上，他们其实有很多不同的

254
00:45:20.700 --> 00:45:43.059
Gavin Lee: 任务，每天要做，就是比方说还要发邮件啊，还还要做，还要去准备一些课程啊，然后还要去去。达标给他们的学生的分数在一些考试上，所以他们就是也是展现出来一个前后就是后面，如果有ai，那可能有一些比较小的啊，

255
00:45:43.510 --> 00:45:52.700
Gavin Lee: ai可以帮你提款，就比方说，你需要去写邮件，给学生的父母去做一个报告

256
00:45:52.700 --> 00:46:06.659
Gavin Lee: 这种东西，或者你要去帮去。去去。你的学生的考试卷，你要去去看一下，那ai是可以帮你去做这个东西。

257
00:46:06.860 --> 00:46:17.389
Gavin Lee: 但是有一些比较需要人需要操作的东西，像教课这种，就可能老师还是需要这个辅助在这里，

258
00:46:17.390 --> 00:46:32.249
Gavin Lee: 所以为什么我会说就是ai可能只是目前只能符合你最basic打分的criteria跟，但如果你要去更多，就有一些更多的解释

259
00:46:32.540 --> 00:46:51.689
Gavin Lee: 从人的。感情上啊，或者一些还是啊，或者一些人的一些，一些一些，我觉得他们目前，我个人觉得我他们还不能做到这个层面上吧。

260
00:46:52.690 --> 00:46:58.269
Fan Yunjie: 了解，但是我像我们大部分的受众来说的话，

261
00:46:58.390 --> 00:47:11.599
Fan Yunjie: 再去想creativity这个事情，还是会跟他和art去去给它对标。您觉得目前来说的话就是如果从这个层面上来说，ai是

262
00:47:11.870 --> 00:47:17.039
Fan Yunjie: 可替代的，还是说它是完全不可替代的。如果从艺术层面。

263
00:47:22.790 --> 00:47:38.039
Gavin Lee: 我觉得还是不能替代吧，因为我觉得设计跟不管是哪一个设计还是创意啦，就是每个人有自己的审美跟自己的喜欢的点

264
00:47:38.260 --> 00:47:39.760
Gavin Lee: 就是可能。

265
00:47:40.110 --> 00:47:45.210
Gavin Lee: 比方说，你看一个，你看一个设计，还是你看一个方案

266
00:47:45.340 --> 00:47:50.779
Gavin Lee: 对我来说，可能我会觉得非常的好，但对另外一个人说

267
00:47:50.890 --> 00:47:56.860
Gavin Lee: 不是很好，所以就是每个人的品味跟他们的审美就是

268
00:47:57.570 --> 00:48:00.080
Gavin Lee: 有点差别蛮大的，

269
00:48:00.360 --> 00:48:12.669
Gavin Lee: 除非你是问专专专属专业的人，那可能他们会给你一个更更详细跟跟细节的的方案，那你要ai去判断这个事情

270
00:48:12.690 --> 00:48:21.340
Gavin Lee: 诶，我觉得现在目前他们就不能去做，得到非常的非常的好，

271
00:48:22.880 --> 00:48:26.030
Gavin Lee: 这个会解释你的问题嘛。

272
00:48:26.520 --> 00:48:33.579
Fan Yunjie: 可以可以，但是我觉得我想更deep一点，就是我觉得ai它是有学习能力的不是吗？如果说它。

273
00:48:33.580 --> 00:48:34.240
Gavin Lee: 对对。

274
00:48:34.240 --> 00:48:51.699
Fan Yunjie: 那这个带领下不断的去学习，那他不会产出的，就跟您本身的创意思维很像吗？不会有这个问题吗？就是。或者说是。我，我，我在想，比方说一个导演，我们现在看，每个导演都都会有不同自己的风格，

275
00:48:51.700 --> 00:48:57.180
Fan Yunjie: 像这个他肯定有自己的风格，或者说是

276
00:48:57.180 --> 00:49:11.130
Fan Yunjie: 张艺谋，他肯定有自己的风格，那我们如果就不断的让大家去学习，即使他有8000个样本让他去学，他总会学会的，然后他也许就是会自己输出一个，这个，这个相对来说很

277
00:49:11.130 --> 00:49:22.100
Fan Yunjie: 安迪，我很张艺谋的这样的一个作品出来，当然他也会输出一个很galen lee的这样的一个作品出来，这个您觉得是不可能的吗？对。

278
00:49:25.080 --> 00:49:28.179
Gavin Lee: 这个事其实是可能啦。在

279
00:49:28.400 --> 00:49:46.580
Gavin Lee: 我觉得就是要看，虽然他也是可以学习，也甚至他也是可以去禁锢他这个的理解跟消化。但也是要看你的目标来用ai在每个方面上是什么？先

280
00:49:47.100 --> 00:49:48.050
Gavin Lee: 对

281
00:49:48.320 --> 00:50:07.300
Gavin Lee: 这个可能就是，如果像你说如果是专门是为了，就是如果你想要让ai去评判一个方案，那可能在前期这个。这个公司就是比方说公司，哎，他们买这个工具品牌，他们为了去，

282
00:50:07.300 --> 00:50:12.170
Gavin Lee: 叫他去评判一些方案，他们的供应商给他们，

283
00:50:12.170 --> 00:50:21.810
Gavin Lee: 那可能他们在前期要去喂跟去，去咨询这个ai的工具，或甚至就是教他

284
00:50:21.810 --> 00:50:30.229
Gavin Lee: 用哪一个，哪一个方案。从公司的不同的政策上，你才可以给我一个比较公平的。达标出来，

285
00:50:30.490 --> 00:50:38.399
Gavin Lee: 那我觉得都是一些很多都是，前期的都也是要去有一些固定的目标，跟alignment。

286
00:50:38.940 --> 00:50:44.850
Fan Yunjie: 明白。所以说您觉得他是有能力学习的，但是

287
00:50:45.350 --> 00:50:49.990
Fan Yunjie: 说到底他是没有能力去是错，容易的，会。

288
00:50:49.990 --> 00:50:51.490
Gavin Lee: 对创造的。

289
00:50:51.870 --> 00:50:58.389
Gavin Lee: 我觉得现在我可能是从设计跟创意的方面，我觉得我还没看到。

290
00:50:58.930 --> 00:51:00.710
Fan Yunjie: 明白明白

291
00:51:01.000 --> 00:51:11.920
Fan Yunjie: 会不会存在一些。用ai产出的一些内容有一些合规的问题，或者说是有一些版权的问题之类的。

292
00:51:14.800 --> 00:51:25.539
Gavin Lee: 版权啊，就这个这个话题有点敏感喽，我怎么描述出来？

293
00:51:26.910 --> 00:51:36.240
Gavin Lee: 等一下啊版权这个我觉得是两个方面，就是版权，就是

294
00:51:37.240 --> 00:51:46.840
Gavin Lee: 我觉得这个版这个，这个制，版权就是一个就是一个corporate的政策，是来保护一个个人的ip。

295
00:51:47.270 --> 00:51:50.860
Gavin Lee: 首先这个是我的逻辑性啦，因为就是

296
00:51:51.550 --> 00:52:00.319
Gavin Lee: 可能很多东西你做，或者可能东很多东西不管你写还是创造，还是你要做音乐，还是你要做饭，

297
00:52:00.600 --> 00:52:06.460
Gavin Lee: 就你看的东西都是很重复吧？你也是可以当成他像一个。

298
00:52:07.220 --> 00:52:20.549
Gavin Lee: 你在我在偷你的想法，我在偷你的执行，我在偷你的作品，但每个人做的东西就就很简单，就是你吃吃一个饭，然后其实有很多面店都是做一样的面，那这个

299
00:52:20.590 --> 00:52:32.229
Gavin Lee: 对我来说是最简单的思路我去。simplify这个想法。后面回到就是ai这个版权的问题就是，

300
00:52:32.450 --> 00:52:52.059
Gavin Lee: 可能这个是个人的品牌或者公司想要保护他们自己的ip啦，就是你也是很难去控制。用户的想要生成的一些东西出来，就是要看他们的目的是什么吧。首先。

301
00:52:54.200 --> 00:53:02.260
Fan Yunjie: 明白，所以说它目前也是会呈现出来一些直接的核位问题吗？就在您的案例当中，对。

302
00:53:03.290 --> 00:53:07.560
Gavin Lee: 对对，我觉得还是会道出出来一些问题啦。

303
00:53:08.370 --> 00:53:10.149
Fan Yunjie: 您方便就是

304
00:53:10.350 --> 00:53:19.579
Fan Yunjie: 描述一下嘛，就是相关的，可能说目前已经存在了，还没有办法解决，或者说有已经有相关解决方案的一些问题。

305
00:53:21.270 --> 00:53:27.479
Gavin Lee: 让我想一下这个这个还没想好，

306
00:53:29.460 --> 00:53:33.899
Gavin Lee: 我们要不去下一个问题，然后可以，这个可以回回回来吗？

307
00:53:34.070 --> 00:53:39.560
Fan Yunjie: 可以，可以，没问题，没关系，所以本身在现在的团队当中，

308
00:53:39.860 --> 00:53:47.240
Fan Yunjie: 您觉得更大程度上大家是仍然把ai当成工具，还是说觉得它已经变成了一个

309
00:53:47.360 --> 00:53:53.480
Fan Yunjie: like coworker，或者说是一种a personal assistant这样的一个职能在了。

310
00:53:54.720 --> 00:53:58.420
Gavin Lee: 我个人觉得还是一个吧？

311
00:54:00.020 --> 00:54:04.730
Fan Yunjie: 那其实其实在您这儿，他的谅解还是挺重的，真的。

312
00:54:04.730 --> 00:54:09.870
Gavin Lee: 对对，对我还是我还是个人觉得就是我现在用的

313
00:54:10.070 --> 00:54:16.969
Gavin Lee: 百分之占比在ai上会比我去年跟年初用的比较多，

314
00:54:17.610 --> 00:54:31.299
Gavin Lee: 就是我可以看到我用的用户在ai，不管是解锁或者不管在学习一些新的东西一些工具，或者一些，或者甚至最简单就是写一个邮件。

315
00:54:31.300 --> 00:54:42.040
Gavin Lee: 我已经可以看到我有一个蛮大的。进步，在这个这个方面上吧，就

316
00:54:42.440 --> 00:55:00.530
Gavin Lee: 就打个比方很简单，有有时候我也是无聊，我会去问你把一些一些事情就是跟以前律师有相关，或者一些跟一些公司的背景跟公司的业务，等等，这些

317
00:55:00.630 --> 00:55:08.679
Gavin Lee: 就是可能会让我解锁我这个好奇性的一些，一些，一些一些内容上吧。

318
00:55:09.630 --> 00:55:15.339
Fan Yunjie: 这个能方便拓展一下吗？就是您在工作以外，都会跟ni有些哪些互动。

319
00:55:16.110 --> 00:55:27.829
Gavin Lee: 最简单就是check db咯就是最简单是checkpt。然后就是

320
00:55:28.640 --> 00:55:36.180
Gavin Lee: 哎sorry，我的team，我的我的team里面的人还在跟我聊，在以后一分钟回复他们。

321
00:55:36.180 --> 00:55:37.540
Fan Yunjie: 没关系。

322
00:55:37.960 --> 00:55:40.160
Gavin Lee: 结果我发现不是下班的时间。

323
00:55:40.980 --> 00:55:52.380
Gavin Lee: 发给我，发给我，我帮我现在帮你喔。本来问

324
00:55:57.630 --> 00:56:02.999
Gavin Lee: 对，其实其实就是。

325
00:56:03.340 --> 00:56:12.229
Gavin Lee: 打个比方很简单的比方，之前我在去看了那个电影，电影，那个嘛，就是那个演的，然后

326
00:56:12.330 --> 00:56:21.579
Gavin Lee: 他们是故事，是我，现在我就是我发现就是每次我消化或者看到一个新的内容，就是比方说，不管在电影上还是在电视上

327
00:56:21.600 --> 00:56:44.409
Gavin Lee: 就是有一些剧，我后面看完我会敢对这个剧有一些后续有一些兴趣嘛，所以我就可能会用ai去跟他问他，诶，ai就是诶，我，我帮我跟我说一下，多一点这个F1的电影的历史，或者本身的这个这个比赛的历史，

328
00:56:44.410 --> 00:56:59.430
Gavin Lee: 你去请跟我描述跟我给我多点的内容来。扩展，知道就是本身除了我看这个电影的以外的角度，其实我也是想要去理解更透更多的

329
00:56:59.430 --> 00:57:16.820
Gavin Lee: 它的本身的，比方说车是他们的车是怎么创出来啦为什么他们的F1比赛就是必须要有。二20个车为什么不能是30个车等等这些，所以我觉得就是会让我。展开我这个好奇心吧。

330
00:57:17.610 --> 00:57:24.820
Fan Yunjie: 了解，所以更大程度上就是也把它当做一个简易化的搜索引擎去做去去用，是吗？更大差。

331
00:57:24.820 --> 00:57:27.829
Gavin Lee: 你可以理解是这样。

332
00:57:28.100 --> 00:57:31.539
Fan Yunjie: 有没有说一些跟他进行一些比较，

333
00:57:32.070 --> 00:57:35.470
Fan Yunjie: 私人或者说是比较怎么说呢？

334
00:57:35.950 --> 00:57:48.280
Fan Yunjie: 个性化的沟通，这种有没有？因为有一些。我在去采访的时候会有会有一些这个受众呢？他们把ai当成自己的心理咨询师。

335
00:57:48.930 --> 00:58:05.099
Gavin Lee: 是吗？我这个我没有，就是我没有跟他比方说，我没有叫他算我的八字，跟帮我看什么塔罗牌这种等等，这些东西我，我，我没有问ai过这个这个方案这个方面上

336
00:58:06.110 --> 00:58:19.969
Fan Yunjie: 这个是没有的，这个主要的原因是因为你也害怕，就是就即使它只是一个一个算法型的工具，但是还是会觉得自己的隐私要更大程度上被保护的感觉吗？还是说就是。

337
00:58:20.390 --> 00:58:31.320
Gavin Lee: 不是自己的饮食啦。饮食啦，可能自己的我不会，我不会对这种东西比较比较有兴趣吧，可能我不是比较迷信，在这个方面上吧。

338
00:58:32.410 --> 00:58:41.489
Fan Yunjie: 那你会不会用？就比方说跟他去聊一些自己的困惑，像有一些我也有聊到了一些这个

339
00:58:41.830 --> 00:58:53.760
Fan Yunjie: cannabis，他们可能比方工作当中遇到了一些困惑，或者说是自己的一些利益，一些stakolders，他们会把这个描述出来，让ai去给他一些解决方案。什么？这种问题有没有。

340
00:58:55.190 --> 00:59:04.860
Gavin Lee: 没有。哎，其实我觉得我用ai是纯粹是从一个跟一些教育base的方面上啦。

341
00:59:05.400 --> 00:59:11.319
Fan Yunjie: 了解它，各大城市还在您这儿是充当一个工具，在这个生活维度上。

342
00:59:11.740 --> 00:59:13.110
Gavin Lee: 对对，对。

343
00:59:13.110 --> 00:59:17.440
Fan Yunjie: 了解，了解，了解，明白，所以本身的话，

344
00:59:17.950 --> 00:59:25.090
Fan Yunjie: 我们回到您的工作吧。然后。我看您其实有做过一些esg相关的是吗？

345
00:59:25.580 --> 00:59:26.479
Gavin Lee: 做啥。

346
00:59:26.920 --> 00:59:28.030
Fan Yunjie: Yesterday.

347
00:59:28.260 --> 00:59:31.580
Gavin Lee: 对，那个是我上一家公司吧。

348
00:59:32.430 --> 00:59:43.489
Fan Yunjie: 所以如果存在一些比较高风险的，一些客户的这些项目就不论不论是esg相关，还是说您现在的您会去去

349
00:59:43.820 --> 00:59:47.500
Fan Yunjie: 用ai去输出去去，更大程度上，

350
00:59:48.160 --> 00:59:55.469
Fan Yunjie: 哎，还是还是同样的workflow？还是说为了这credibility，您就是舍弃ai，更大程度上使用您自己。

351
00:59:58.580 --> 01:00:06.359
Gavin Lee: 以前那时候有这个esg的话题。确实ai那段时间没有那么火，所以，

352
01:00:06.720 --> 01:00:22.440
Gavin Lee: 可能那段时间做的方案就是，如果你要得出跟拿下一些credibility的一些report啦，还是一些。简单来说，采访我们还是会通过

353
01:00:22.640 --> 01:00:28.159
Gavin Lee: 普通的方式去拿到这些报告，跟一些。

354
01:00:28.160 --> 01:00:45.929
Gavin Lee: 一些数据吧，甚至就是请一个公司帮我们去采访人家，或者去邀请一些的领导对这个话题会比较有一些。更深度的理解。

355
01:00:45.930 --> 01:00:55.719
Gavin Lee: 但啊，目前我据我所知就我觉得我没有用过ai在那段时间在这个话题上啊。

356
01:00:56.000 --> 01:01:11.449
Fan Yunjie: 了解，所以就是在您现在比较高频使用ai的一些project当中，你有没有，就是某些程度上一开始会被ai带走ai跑偏方，然然后在一个错误的行径上

357
01:01:11.650 --> 01:01:14.739
Fan Yunjie: 去跑了一段时间的这种情况呢？

358
01:01:16.220 --> 01:01:22.520
Fan Yunjie: 你说的意思就是我告诉你对我会误导我，

359
01:01:25.970 --> 01:01:39.460
Gavin Lee: 没有。哎，我觉得现在，可能我现在还是会比较乐观。看到ai是一个比较一个辅助的工具，跟合合作的方式吧，就是甚至没有会给我感觉就是。

360
01:01:39.720 --> 01:01:45.590
Gavin Lee: 有这个误导，在这个方面上。

361
01:01:46.770 --> 01:01:51.749
Fan Yunjie: 了解，所以就是您在去输出方案的时候，会特意的

362
01:01:51.890 --> 01:02:02.520
Fan Yunjie: 让他去输出like plan a plan b这样这种a，b test这样的感觉吗？然后去帮您判断一下会有这种这种cogniz这个flow吗？

363
01:02:03.260 --> 01:02:10.809
Gavin Lee: 我会比方说叫它去输入更深层一些，比方说我需要

364
01:02:10.860 --> 01:02:27.069
Gavin Lee: 三个还是两个不同的方向，或者我会特意去。打出这个比较比较清楚一点，因为可能就是可能我觉得我学到一个事情，就是那个就是

365
01:02:27.150 --> 01:02:41.829
Gavin Lee: 你要对他们的，你要跟他们说的东西越清楚，是越好，就是不能当成他像一个谷歌或者一个百度，就是不能纯粹打111行字，然后你会会会他给你一个100，

366
01:02:41.990 --> 01:02:45.019
Gavin Lee: 100个页的还是20个页的方案，

367
01:02:45.020 --> 01:03:06.089
Gavin Lee: 其实很多东西我觉得前面还是需要人去做自己的，跟有自己的方向，自己的然后甚至你也是要有自己的。比较明确跟清楚的目标，你要得到什么，你才会可以去。有一些更好的

368
01:03:06.090 --> 01:03:08.689
Gavin Lee: 生产的，深层的东西会出来。

369
01:03:09.430 --> 01:03:26.890
Fan Yunjie: 明白，那您现在团队当中的这些小朋友，或者说是些刚入职场的一些。intern吧。他们如果说输出工作90%都是用ai输出这个你会觉得介意吗？

370
01:03:28.560 --> 01:03:41.170
Gavin Lee: 我确我其实不是很在意我，其实我很鼓励他们去用，因为可能他们去理解，跟他们去解锁的方式会比我更快，

371
01:03:41.370 --> 01:03:49.609
Gavin Lee: 因为我觉得就是可能在他们对这个话题会会比较大的兴趣吧。

372
01:03:49.610 --> 01:03:53.070
Fan Yunjie: 而且他们就像我说前面他们不是很。

373
01:03:53.070 --> 01:04:04.590
Gavin Lee: 他们不会很，他们不会怕，如果他们失败，我觉得如果他们对这个失败的东西不会像年纪比较大的人会比较反抗。

374
01:04:05.420 --> 01:04:13.440
Fan Yunjie: 但是就在这个过程当中，其实您是否能更更去care to efficiency的这个问题，但是有一些。

375
01:04:13.440 --> 01:04:14.040
Gavin Lee: 今天。

376
01:04:14.040 --> 01:04:30.499
Fan Yunjie: 导，他们可能会在意的是，我可以让你甚至百分之百都用ai输出，但是你有没有真正意义上的了解你的输出的内容，这个你会去介意吗？就是您的，就是手下他有没有很多。

377
01:04:31.790 --> 01:04:40.559
Gavin Lee: 我觉得这个是跟你的，你的，你的上社的老板有一些你们两位要有一些expectation吧？

378
01:04:40.690 --> 01:04:43.970
Gavin Lee: 就是。比方说

379
01:04:44.030 --> 01:04:54.959
Gavin Lee: 我举一个很简单的例子就是有一个专案，最近我也是去接给大陆一个咨询的同事，然后，

380
01:04:54.960 --> 01:05:08.200
Gavin Lee: 确实就是我是希望我的小朋友可以去用ai。在这个案例上吧，跟这个proposal上就是一个很简单的做一个平面的的图，

381
01:05:08.290 --> 01:05:12.790
Gavin Lee: 但是我的方式就是也是要manage。

382
01:05:13.090 --> 01:05:24.890
Gavin Lee: 这个。工作的那个质量跟这个就是我也是不能拖太久。我会这么操作吧。就是可能我会安排一个

383
01:05:24.890 --> 01:05:40.859
Gavin Lee: 设计师或者创意去做一个客户想要的功课就是教教，教辅功就是教教个功课，就是我们的咨询。同事需要的，然后我就会作为投入跟，然后拉

384
01:05:40.860 --> 01:05:48.199
Gavin Lee: 一个同同小朋友的同事对ai比较有兴趣，但是我要把控他时间，我只能给你

385
01:05:48.200 --> 01:06:02.589
Gavin Lee: 把。比比举个例子，两个小时，三个小时，你只能把控在这个时间，但是你要去。理出一个平面，就是能达到每一个的要求，但是只是纯纯粹用ai，

386
01:06:02.600 --> 01:06:11.470
Gavin Lee: 那这样我会用这个是一个策略的方式吧，就是我会给咨询的部门，或者我自己的老板看我有一个对比，

387
01:06:11.550 --> 01:06:30.780
Gavin Lee: 就是，如果你要完全用以前的方式，那你能看到是这种样子，但是可能你用ai就是会比方案A有点稍微好一点，然后甚至可能时间用也是减了一半或者减了四分之一。

388
01:06:31.280 --> 01:06:41.080
Gavin Lee: 所以这个是我，我通常会用这个这个方法吧，来去引导跟影响我这个小朋友的方式吧。

389
01:06:41.540 --> 01:06:57.110
Fan Yunjie: 对，您觉得在您的这个工作维度之上，如果把它们分成一个一个block的话，哪一些block您觉得是可以完全依赖ni就是不会care它哪一些您觉得是一定是需要人类参与或者说人类去把关的。这种。

390
01:06:57.110 --> 01:07:05.390
Gavin Lee: 去帮我去帮我填报报销的时候填我的报销。

391
01:07:05.390 --> 01:07:05.810
Fan Yunjie: 我们。

392
01:07:05.810 --> 01:07:16.439
Gavin Lee: 需要肯定需要和财务帮我填的，因为他们非常的会看每个细节比较比较比较比较详细，

393
01:07:17.930 --> 01:07:18.510
Gavin Lee: 呵。

394
01:07:18.510 --> 01:07:24.580
Fan Yunjie: 那就从这整个这个project来说，从project的这个动力来说就是，哼，

395
01:07:24.810 --> 01:07:25.910
Fan Yunjie: 您觉得哪一种。

396
01:07:25.910 --> 01:07:26.550
Gavin Lee: 然后。

397
01:07:26.550 --> 01:07:38.220
Fan Yunjie: 然后从creativity这个方向来说，您觉得哪些您可以完全依赖ai，哪些是一定还是人类需要在里面起到一定决定性作用。

398
01:07:39.210 --> 01:07:56.530
Gavin Lee: 我觉得可以靠这ai给你出出可以可以倒出来跟生存出来一些不同的创意的方向跟一些啊，探索不同的吧，就是让你去继续发散跟延展你的，你的最初步的想法，

399
01:07:56.600 --> 01:08:06.379
Gavin Lee: 这个我是认同啦就是也可以真的帮你。然后第二个就是它可以帮你在执行上

400
01:08:06.480 --> 01:08:17.910
Gavin Lee: 省，哪怕是减少或者去帮你省一半的时间还是四分之一，我觉得执行上跟想，

401
01:08:18.040 --> 01:08:30.499
Gavin Lee: Idea的方面上可能是肯定可以，需要，可以，也没有说非常的依赖，但我觉得是可以靠它去帮你。提高你的效率啦

402
01:08:31.000 --> 01:08:32.580
Gavin Lee: 对，但

403
01:08:32.840 --> 01:08:49.009
Gavin Lee: 不能依赖，不能靠ai，我觉得就是还是很多，都是沟通的方方式上，就是比方说你需要沟通你的方案，你的，给你的领导或者你的同事，不同的部门，

404
01:08:49.010 --> 01:09:02.319
Gavin Lee: 这个你肯定不能依赖ai，就是你肯定还是。靠自己怎么去说服你们的internal，不管是internal同事还是你的客户，这个你完全不能

405
01:09:02.319 --> 01:09:08.620
Gavin Lee: 去去问ai去帮你去去帮你去进去那个会议室去提案啊。

406
01:09:09.250 --> 01:09:23.479
Fan Yunjie: 那有没有可能就是因为您本身。下面的小朋友非常信任ai，然后您上面的一些比较senior的一些一些这个一些fellow，他们可能就怀疑，因为

407
01:09:23.490 --> 01:09:33.720
Fan Yunjie: 因为这个年代的问题会怀疑ai产出这种事情。如果出现这种分歧，您也刚刚提到，沟通非常重要，您一般会怎么调节这种问题。

408
01:09:34.750 --> 01:09:35.630
Gavin Lee: Okay.

409
01:09:36.229 --> 01:09:48.939
Gavin Lee: 我觉得这个好的，好的问题，因为，等一下，我先回复同事啊

410
01:09:59.170 --> 01:10:06.490
Gavin Lee: 对，我觉得这个是跟他们的思路跟想法有相关吧。就

411
01:10:06.590 --> 01:10:11.590
Gavin Lee: 我那时候，今年1月份刚入职这家公司嘛。然后我发现

412
01:10:11.730 --> 01:10:18.469
Gavin Lee: 我们有几个？全职的同事，我们会叫他们legacy的同事就是可能

413
01:10:18.970 --> 01:10:24.880
Gavin Lee: 为什么这个字叫legacy，因为可能他们已经在这家公司十年，17年，15年了。

414
01:10:25.610 --> 01:10:28.149
Gavin Lee: 所以导致就是

415
01:10:28.340 --> 01:10:40.999
Gavin Lee: 很多的新的政策跟新的东西，我们希望他们可以去尝试。确实他们就我觉得有两个原因吧。一个可能他们会觉得

416
01:10:41.420 --> 01:10:57.999
Gavin Lee: 添麻烦，他们不想去学习，因为他们觉得他们会比较已经很很很很满意，在做他们每天做的东西就是就是中文，是叫老油条了，他们只是觉得没必要吧，因为公司

417
01:10:58.000 --> 01:11:09.389
Gavin Lee: 公司也是不会踩我吧，因为如果我没有做什么大犯什么大错吧，就是每天我能跟按付跟教我的东西，我觉得已经足够了。

418
01:11:10.340 --> 01:11:29.669
Gavin Lee: 对，然后另外一个原因，可能第二个原因，我是我是有尝试一下去，影响他们去去学吧。但在这个学习当中，我发现我要投入很多时间，就像一个小朋友，我要拿那个东西直接喂到他们的嘴巴，

419
01:11:29.800 --> 01:11:38.510
Gavin Lee: 我不能去说，okay，我丢你一个工具，你去帮我解锁，你帮我去看啊，这个工具好不好用，

420
01:11:38.750 --> 01:11:48.159
Gavin Lee: 他们需要一个一个一个人去引导他们，去指导他们怎么去用这个东西，所以后面为什么我是

421
01:11:48.230 --> 01:11:50.540
Gavin Lee: 有点放弃。

422
01:11:50.540 --> 01:12:08.499
Gavin Lee: 其实我没有说放，完全放弃，他们，就是他们还是可以做到最基本的任务啦。但就是我要去额外投入我的时间去。拿ai的东西叫他们去学。我现在比较不会去望他们的，

423
01:12:08.500 --> 01:12:11.450
Gavin Lee: 叫他们去去做做这个ai的东西了吧。

424
01:12:12.660 --> 01:12:19.340
Fan Yunjie: 明白，我觉得很大程度上，您在这个跟ai协作的过程当中

425
01:12:19.610 --> 01:12:38.629
Fan Yunjie: 有一个很很重要的点在于您有自己的判断能力，您是具备着很多这个批判性的思维，然后去判断它的输出。但是其实ai它是一个很自信的一个物种，我把它把它定义成一个物种，就是它给出你的你的这个

426
01:12:39.040 --> 01:12:54.910
Fan Yunjie: 答案的时候，他有很大程度上他自己一本正经的在胡说八道，就是他很自信。他说出一个东西听得很自信，如果说就是ai很明确的告诉你，就是说我不确定，或者说这只是一个假设，然后我

427
01:12:54.910 --> 01:13:00.980
Fan Yunjie: 就是我说的结果我告诉你，我不确定这个你还会更加的信任他的，或者说是

428
01:13:01.020 --> 01:13:05.180
Fan Yunjie: 会议更另一个维度，少年会质疑他了。

429
01:13:06.220 --> 01:13:12.920
Gavin Lee: 你说如果ai直接跟他跟我说，我不是很确定我给你那个方案是百分之百可以work on，是吧？

430
01:13:12.920 --> 01:13:13.629
Fan Yunjie: 对对。

431
01:13:14.430 --> 01:13:17.170
Gavin Lee: 有发生过吗？这件事情。

432
01:13:17.170 --> 01:13:36.539
Fan Yunjie: 我的意思是，现在的维度上，ai他太自信，在我看来，它就是盲目的自信。他有的时候就是一本正经，胡说八道。但是如果说他对于我来讲的话，他，如果今天他说出一个内容，他告诉我，我大概80%的内容是有依据的，20%内容是我自己

433
01:13:36.550 --> 01:13:46.040
Fan Yunjie: 瞎说的，我想天的我可能会更加相信他。但是如果对于您来说的话，您是会更想以这种方式输出的话，你会更相信他还是会

434
01:13:46.460 --> 01:13:51.080
Fan Yunjie: 减少它的使用，因为存在觉得很直观的不确定性了。

435
01:13:52.450 --> 01:13:58.490
Gavin Lee: 可能可能我也是要看

436
01:13:58.590 --> 01:14:05.250
Gavin Lee: 最开始那个要要给他喂的东西跟方案跟问题是什么吧，然后

437
01:14:05.530 --> 01:14:09.280
Gavin Lee: 甚至如果他真的回复我这种答案，就

438
01:14:10.040 --> 01:14:28.050
Gavin Lee: 我会用另外一个的方式再问他吧，可能我会不会对那个答案，他的答案，这种答案会满意，因为我的目的已经很清楚嘛，我就是需要你给我一个一些别的思路，或者别的发发散的东西给我去扩大我这个想法。

439
01:14:28.440 --> 01:14:30.840
Fan Yunjie: 那可能是一个人的。

440
01:14:30.890 --> 01:14:31.859
Gavin Lee: 对啊，我，

441
01:14:31.960 --> 01:14:48.249
Gavin Lee: 我不知道我说的是对啦，但可能就是我的观点就是，可能是人，那时候他输入那些东西，就是要明确知道，回到我，像我说，你要明确知道你要问这个，这个，这个ai工具到底是什么东西？

442
01:14:49.940 --> 01:14:59.229
Fan Yunjie: 那如果在您本身团队里面您就给ai他一个岗位呢？您觉得就是给他定位成一个什么肉比较好。

443
01:15:02.760 --> 01:15:10.090
Gavin Lee: 这个没想，哎，但我，但我没想过这个问题，因为

444
01:15:10.250 --> 01:15:22.619
Gavin Lee: 但我知道有一些公司，他们已经开始请专门请一些岗位，就像ai的领导或者head of ai，或者是他们的岗位里面有一个ai字里面，

445
01:15:23.020 --> 01:15:25.160
Gavin Lee: 所以我

446
01:15:25.270 --> 01:15:39.539
Gavin Lee: 我会有点打一个问号在这里吧？就是，可能ai不会纯粹变成一个员工在里面吧，可能甚至就是它只是一个辅助的

447
01:15:39.620 --> 01:15:48.160
Gavin Lee: 辅助一个工具吧，或者辅助一个system来帮助人家来。给他们一些

448
01:15:48.520 --> 01:15:54.240
Gavin Lee: 更好的去解决他们的生意上或者生活上的问题。

449
01:15:54.910 --> 01:15:55.750
Fan Yunjie: 明白，

450
01:15:55.800 --> 01:16:12.070
Fan Yunjie: 我，我记得在我之前我那个大概是11920年，那时候，在fall的时候还还是有一个非常大权重的一个岗位就是做copy writer的，或者说copy base这些creater，我不知道现在这个岗位是否还存在，

451
01:16:12.070 --> 01:16:16.830
Fan Yunjie: 但是我在我的现在的认知里面，我觉得这个搞得其实是很大程度上是

452
01:16:16.910 --> 01:16:22.689
Fan Yunjie: 可以被aip来画的，您觉得我这个想法您同意吗？

453
01:16:23.750 --> 01:16:35.419
Gavin Lee: 其实我有跟我视野广告朋友，他们那些几个还生还在做广告，其实我有跟他跟他们聊过了就是首先

454
01:16:35.530 --> 01:16:42.160
Gavin Lee: 他们会觉得不会替换这种创意的文案的创意，

455
01:16:42.310 --> 01:16:52.279
Gavin Lee: 因为可能他们会觉得就是有些东西像我说，人还是会能理解

456
01:16:52.440 --> 01:16:58.800
Gavin Lee: 跟去知道他们的他们的客户，他们的品牌的客户想要什么。

457
01:16:58.800 --> 01:17:00.300
Fan Yunjie: 就。

458
01:17:00.460 --> 01:17:06.879
Gavin Lee: 可能很多的大层面上？Ai现在可以帮你去实现，但是

459
01:17:07.110 --> 01:17:26.399
Gavin Lee: 有一些小东西去达到。如如果是比方说你要去写一个脚本，给一个两分钟还是一分钟30秒的品牌故事，可能ai可以帮你，虽然你可以为他，你可以教他怎么去写，但后面

460
01:17:26.400 --> 01:17:35.190
Gavin Lee: 还是需要靠这人去有一些。比较感情上跟一些story telling。

461
01:17:35.250 --> 01:17:45.600
Gavin Lee: 甚至就是也是要。make sure就是你给那个品牌写那个脚本在一个

462
01:17:45.680 --> 01:17:57.909
Gavin Lee: 在一个上，你要你要，你能抓到这个品牌的一些dna，跟一些一些一些一些比较的小细节吧。

463
01:17:57.910 --> 01:18:06.760
Gavin Lee: 所以我还没看到一个非常成功的案例，就是纯粹靠ai去从文案上，脚本上，

464
01:18:06.760 --> 01:18:20.330
Gavin Lee: 创意上，然后拍出来的东西还没看到非常成功的案例啦，可能只是有一些像我说就是ai只是帮你补充一些你需要去帮你填上，

465
01:18:20.840 --> 01:18:33.440
Gavin Lee: 但整个还是会比较，现在可能是还是人，跟ai是合作的比较比较大，是多一点。

466
01:18:33.960 --> 01:18:39.200
Fan Yunjie: 就是您觉得就是肯定要存在一个人类在这儿去调教它，然后去

467
01:18:39.360 --> 01:18:42.230
Fan Yunjie: 把自己的理解让他去消化。

468
01:18:44.030 --> 01:18:45.710
Gavin Lee: 对，对，对。

469
01:18:45.860 --> 01:18:58.350
Fan Yunjie: 那如果说一个纯粹的这种用ai输出的这些脚本，或者说是用ai输出的文案是可以看出来的吗？对于你来讲的话是直接能看出来的吗？

470
01:19:00.840 --> 01:19:06.749
Gavin Lee: 这个好问题。哎，这个我感觉就是如果执行出来，你肯定能看到

471
01:19:06.820 --> 01:19:13.699
Gavin Lee: 就是你，你对比一下，如果你拍一个片子跟拍一个真人跟

472
01:19:13.700 --> 01:19:27.690
Gavin Lee: 生成一些视频的素材，你肯定能看出来是哪一个是ai做的哪一个是导演，或者是后期公司拍的，这个是很明显，但是我觉得

473
01:19:27.690 --> 01:19:42.630
Gavin Lee: 前期很多看不到的东西，就是会对一个普通的。消费者就是很难去。明确去分别出来，所以可能很多前期的工作就是。

474
01:19:42.700 --> 01:19:49.510
Gavin Lee: 确实你没有跟我说是，如果我是ai做的，我也是不会猜是ai做的。

475
01:19:49.850 --> 01:19:58.780
Fan Yunjie: 那从realization之后，您觉得它跟人类的这个调性，或者说是风格有最大的就区别在哪里啊。

476
01:20:01.220 --> 01:20:05.660
Gavin Lee: 你说最重要的区别就是它们创出来的东西，是吗？

477
01:20:05.660 --> 01:20:12.270
Fan Yunjie: 对对，就是已经可以直接分化开了，这个一看就是ai做的，那那怎么它的点在哪儿？就是。

478
01:20:12.590 --> 01:20:22.610
Gavin Lee: 可能有一些画面就是你会知道。有一些。镜头的角度

479
01:20:22.730 --> 01:20:30.640
Gavin Lee: 就是用一个普通的导演，跟一些普通的导演的来去拍，就是肯定实现不了，

480
01:20:31.210 --> 01:20:40.789
Gavin Lee: 就是你，要么你的预算非常的高，要么就是你的技术的非常的的好，但

481
01:20:41.170 --> 01:20:52.189
Gavin Lee: 可能是从创业的，专业的人，在这个行业，他们能看得出吧，但普通的消费者我看我到现在感觉就是

482
01:20:52.310 --> 01:20:58.629
Gavin Lee: 他们看他们也是会觉得他们也是看不出分别是在哪里，

483
01:20:58.910 --> 01:21:05.209
Gavin Lee: 所以大部分就是那个目标，你要卖给哪一位的不同的消费者了。

484
01:21:05.210 --> 01:21:06.070
Fan Yunjie: 明白，

485
01:21:06.310 --> 01:21:12.989
Fan Yunjie: 所以就是就因为我现在看到其实很多公司很小的公司啊，小的特别初创，他们

486
01:21:13.300 --> 01:21:31.279
Fan Yunjie: 可能团队都都不配了，就是完完全全稀释掉了被ai，然后可能输出一些poster啊，输出一些copy就直接靠ai去输出，那就导致我觉得很大程度上就是就很油腻，就是很这种东西突然油油腻腻的这种感觉。所以

487
01:21:31.510 --> 01:21:39.270
Fan Yunjie: 但但是就是从未来的这个角度上来说，我觉得他肯定还是会更加的personalize一点就伴随着

488
01:21:39.410 --> 01:21:55.190
Fan Yunjie: 它。可能一些底层的变化，一些分布式存储的一些变化可能会变得更加的个性化一点，所以您会不会有这个担忧，就是未来自己的工作，或者说是未来自己的这个行业会不会逐步稀释掉的？这个问题。

489
01:21:55.900 --> 01:21:59.680
Gavin Lee: 对对，我当然会有啦，我当然会很担心，就

490
01:22:00.570 --> 01:22:06.600
Gavin Lee: 可能我这个岗位以后是不是会被ai代替？

491
01:22:06.790 --> 01:22:08.940
Gavin Lee: 就是我还是会有这个。

492
01:22:09.560 --> 01:22:14.170
Gavin Lee: 这个害怕点在我心里啦。

493
01:22:14.610 --> 01:22:21.779
Gavin Lee: 所以。但我知道也不不是我能控制在这个方面上吧。

494
01:22:22.070 --> 01:22:29.300
Gavin Lee: 所以要么我现在只是跟著这个热点，然后去一直去学习，然后

495
01:22:29.440 --> 01:22:32.000
Gavin Lee: 一起去跟ai成长，

496
01:22:32.220 --> 01:22:47.699
Gavin Lee: 这个是这个是我怎么看啦，要么就是我完全不管。就是我就不想学它，不想sorry，我不想学它，我不想去理解它，甚至我也是不想用它。

497
01:22:47.880 --> 01:22:54.119
Gavin Lee: 这个是每个取决于每个人的自己的判断性吧，可能

498
01:22:54.530 --> 01:23:13.310
Gavin Lee: 但我会比较担心啦。就如果以后如果不做这个行，那我自己也是会有点担心，就是我能不能在比方说四十四十岁还是45，45还是快要50，如果50，我还有机会转行，

499
01:23:14.120 --> 01:23:17.379
Gavin Lee: 这个也是会给我一些比较担心点吧。

500
01:23:18.620 --> 01:23:27.620
Fan Yunjie: 那您觉得就是比方说像vcg啊，pwc啊，这些公司，其实我觉得乙方公司很大程度上都面临这种威胁。

501
01:23:29.220 --> 01:23:40.890
Fan Yunjie: 那他们在面对这种情况之下，有没有什么，就是战略上的一些调整，或者说是针对员工的一些培训啊之类的。

502
01:23:42.350 --> 01:23:49.260
Gavin Lee: 我觉得有，我觉得就就。虽然我知道大公司，

503
01:23:50.100 --> 01:23:58.530
Gavin Lee: 虽然大公司就是他们就是一直会强调这个ai的东西，然后他们会就比方说跟以前。

504
01:23:58.530 --> 01:24:04.550
Fan Yunjie: 普华涌到他们有一个专门的部门去卖ai的平台嘛，因为他们也是。

505
01:24:04.550 --> 01:24:08.730
Gavin Lee: 想要创出自己的open ais给卖给他们的客户。

506
01:24:10.530 --> 01:24:21.740
Gavin Lee: 甚至就是他们也是想要用自己的ai的open ai source，然后去介绍在我们的公司workflow里面。这个是一个，

507
01:24:22.390 --> 01:24:36.880
Gavin Lee: 这个是一个可怕的事情，因为可能就是有一些岗位就是以后不没必要了，在这个方面上，但我觉得你也是可以从另外一个角度去看这个事情，因为

508
01:24:37.280 --> 01:24:56.260
Gavin Lee: 公司已经这么大，他们钱也是比小公司预算稍微多一点，但是你可以趁这个机会去去消化，跟去学他们手上一些资讯，跟哎，一些资源嘛，然后你就可以在背后上提高自己

509
01:24:56.380 --> 01:25:03.480
Gavin Lee: 就是你不要等这等着事情发生，然后你才后悔，我觉得就是可能

510
01:25:03.620 --> 01:25:09.179
Gavin Lee: 先提前去准备好自己啦对你说。

511
01:25:09.180 --> 01:25:17.480
Fan Yunjie: 刚刚所说的就是那种。ai involve in之后，最直接的可能受到威胁的岗位会是哪一些？

512
01:25:18.930 --> 01:25:27.299
Gavin Lee: 可能最危险就我，我，我个人觉得就是肯定都是那种辅助的部门啦，像啊。

513
01:25:27.970 --> 01:25:38.400
Gavin Lee: 像，甚至有点财务跟hr等等，我觉得我觉得他们影响可能会比较大。

514
01:25:39.230 --> 01:25:47.670
Gavin Lee: 在前期，如果一个公司真的想要百分之百ai的跟一些东西在里面。

515
01:25:48.790 --> 01:25:54.459
Fan Yunjie: 那您觉得像哪一些岗位，它是更大程度上还是有绝对不对。

516
01:25:54.460 --> 01:25:55.319
Gavin Lee: 对啊什么的。

517
01:25:55.320 --> 01:25:56.040
Fan Yunjie: 对。

518
01:25:56.320 --> 01:25:59.190
Gavin Lee: 就是说还是需要它们的存在还是不需要它们的存在。

519
01:25:59.190 --> 01:26:01.790
Fan Yunjie: 就是有绝对的替换难度的。对于ai。

520
01:26:01.790 --> 01:26:18.139
Gavin Lee: 销售喔，我觉得销售还是必须的吧，因为就是你还是要靠你能怎么卖那个方案，或者你要卖那个，甚至你是一个产品，还是一个方案，你需要靠一个很好的销售去说服一个客户。

521
01:26:18.530 --> 01:26:19.800
Fan Yunjie: 这个。

522
01:26:19.800 --> 01:26:22.719
Gavin Lee: 很难有ai去做到吧。

523
01:26:22.900 --> 01:26:23.309
Fan Yunjie: 就是。

524
01:26:23.310 --> 01:26:34.030
Gavin Lee: 我，我个人现在觉得，所以所以我我会，我会认为现在还是服务的行业还是现在比较安全，

525
01:26:34.480 --> 01:26:44.430
Gavin Lee: 就是在可能在这个五到十年吧，就是送外卖，你还是需要人人力，人力的，然后滴滴

526
01:26:44.580 --> 01:26:54.160
Gavin Lee: 可能十年后会有我不知道，或者或者可能会，但我觉得在他们还没找到一个

527
01:26:54.160 --> 01:27:05.329
Gavin Lee: 合适的方式跟他们还要过很多不同的层面拿到很多审批，所以我觉得这个十年他们还是比较。

528
01:27:05.490 --> 01:27:13.810
Gavin Lee: 安全的，但我是承认这种苦工不是每个人会愿意去去去做的啦。

529
01:27:15.070 --> 01:27:22.290
Fan Yunjie: 所以就是您觉得更大程度上人的价值还在。Counication，对不对？

530
01:27:22.470 --> 01:27:29.900
Gavin Lee: 对啊对啊，就是沟通就是需要沟通，我觉得就是ai目前看不出可以替换它。

531
01:27:30.720 --> 01:27:41.120
Fan Yunjie: 那本身对于creativity这个人来说就是这个，这个domin来说，您如果就是十年之后，您觉得它会变成

532
01:27:41.280 --> 01:27:42.090
Fan Yunjie: 最牛，

533
01:27:42.230 --> 01:27:53.469
Fan Yunjie: 变成一个like prompt architect，还是说仍然仍然是一个like crucial这样的，这您觉得会是怎么定位呢？

534
01:27:53.920 --> 01:27:56.949
Gavin Lee: 你说给创意跟设计什么。

535
01:27:56.950 --> 01:27:57.710
Fan Yunjie: 对对。

536
01:27:59.440 --> 01:28:08.000
Gavin Lee: 我会。我现在有一个大预感会觉得会被替换啦。说实话就是。

537
01:28:11.160 --> 01:28:19.300
Gavin Lee: 就是可能他们未来ai也是会学习这个在这个每个跟每个学习会越来越高吧，

538
01:28:19.520 --> 01:28:33.319
Gavin Lee: 所以就是就是打个比方，很简单，就是你作为一个ppt去美化，可能以后ai会有一些功能跟方式里面可以去帮你描述去同意

539
01:28:33.320 --> 01:28:45.159
Gavin Lee: 里面的。还是ppt一些字体啊，一些啊，一些，等等，这些东西我觉得一，哎，我觉得ai以后

540
01:28:45.170 --> 01:28:55.539
Gavin Lee: 甚至帮你去加一些元素，让大家看起来是符合你要去提案的。品牌方的一些调性，

541
01:28:55.620 --> 01:28:56.839
Gavin Lee: 我觉得他们

542
01:28:56.950 --> 01:29:16.769
Gavin Lee: 慢慢可以做得到，就是就是像像一些像一些像一些ppt跟的ai工具像就是虽然在大陆用不了。但如果你没有在大陆就是很多的别的公司，他们有投入这笔钱去买，来做一些ppt跟

543
01:29:16.770 --> 01:29:20.330
Gavin Lee: 帮他们去设计出来。这些模板。

544
01:29:22.460 --> 01:29:23.989
Fan Yunjie: 了解。所以

545
01:29:24.200 --> 01:29:40.230
Fan Yunjie: 你如果说现在就是十年之后给自己一个跟ai，如果说很大家很顺利的在协作过程当中各自找准了各自的位置，你会给自己一个什么title呢？

546
01:29:41.000 --> 01:29:42.889
Gavin Lee: 啊，也有自己一个title。

547
01:29:42.890 --> 01:29:43.640
Fan Yunjie: 对对。

548
01:29:46.420 --> 01:29:47.359
Gavin Lee: 可能。

549
01:29:47.360 --> 01:29:47.880
Fan Yunjie: 来个。

550
01:29:47.880 --> 01:29:48.860
Gavin Lee: 那如果。

551
01:29:48.860 --> 01:29:54.909
Fan Yunjie: A a i pro architect, let's really.

552
01:29:54.910 --> 01:29:58.539
Gavin Lee: 不会吧，我可能会用ai，ai counication的

553
01:29:59.030 --> 01:30:08.029
Gavin Lee: 还是这种吧，因为你要还要跟ai counicate你你的需求啊。

554
01:30:08.030 --> 01:30:13.589
Fan Yunjie: 了解，所以就是您觉得人在这个过程当中，

555
01:30:13.770 --> 01:30:27.440
Fan Yunjie: 就是不论它是不是对产出结果有一定的把控性，它但凡存在了。跟ai去沟通的这个层面，它其实就是没有丧失掉自己cryer thinking，或者说没有被

556
01:30:27.580 --> 01:30:32.360
Fan Yunjie: 所啊时代，或者说是行业所替代掉的。这个问题是吗？

557
01:30:32.790 --> 01:30:34.429
Gavin Lee: 对，对，没错。

558
01:30:34.790 --> 01:30:35.640
Fan Yunjie: 好的。

559
01:30:35.910 --> 01:30:41.710
Fan Yunjie: Ok，我觉得我的问题都差不多了，哈哈，你有没有什么补充啊？

560
01:30:42.080 --> 01:30:55.880
Gavin Lee: 补充的话，其实其实我觉得如果是在

561
01:30:56.360 --> 01:31:00.720
Gavin Lee: 在还是回到那句话吧，就是可能

562
01:31:00.930 --> 01:31:06.789
Gavin Lee: 还是会需要ai的存在在我们的生活上吧，因为

563
01:31:06.990 --> 01:31:13.450
Gavin Lee: 可能就是未来，谁也不懂，就是谁也不懂，以后是不是会有另外一个？

564
01:31:14.000 --> 01:31:15.980
Fan Yunjie: 对新的饭吃新的。

565
01:31:15.980 --> 01:31:16.649
Gavin Lee: 好的订单。

566
01:31:16.650 --> 01:31:17.240
Fan Yunjie: 对对。

567
01:31:17.240 --> 01:31:22.919
Gavin Lee: 对啊，新的问题出来，甚至就是打战。这个问题就是

568
01:31:23.170 --> 01:31:26.140
Gavin Lee: 问题会一直出现，但就是，

569
01:31:26.790 --> 01:31:36.900
Gavin Lee: 可能ai我觉得就是还是会存在帮人家去稍微解决他们的一些能解决的问题吧。

570
01:31:37.430 --> 01:31:43.310
Fan Yunjie: 对我，我觉得是这样，就是ai，它虽然现在大家看着很把它当一回事儿，

571
01:31:43.360 --> 01:31:57.900
Fan Yunjie: 但是因为您也比我大一些，所以你也是更了解，就是这个时代的一个变革。就当初出现搜索引擎social engine的时候，大家也是觉得很厉害，很牛逼，这个东西。

572
01:31:58.100 --> 01:32:01.499
Gavin Lee: 大家都不用脑子了，但是也没有怎样。

573
01:32:01.520 --> 01:32:10.079
Fan Yunjie: 在将来的情况之下，就是我那天去采访了一个ai的一个founder，然后他做一些ai tools，

574
01:32:10.150 --> 01:32:24.930
Fan Yunjie: 他的，他的想法就是，人能不能有批判性思维？这是人的问题，跟任何的时代没有关系。如果说因为一些时代的问题人就被替代掉，那是这个人的问题

575
01:32:24.930 --> 01:32:31.029
Fan Yunjie: 是，这个人没有在这个时代的洪流中，逝者生存。

576
01:32:31.030 --> 01:32:31.610
Gavin Lee: 对啊，我。

577
01:32:31.610 --> 01:32:49.210
Fan Yunjie: 他们使用的任何工具都没有问题就都无关。所以就是就是我觉得这也是一个点吧，就是其实是人在推动所有的这种工具去迭代去研发。然后就像您说的，我觉得特别好一点，就是其实人其实才是这个世界里最大的bug。

578
01:32:49.560 --> 01:32:50.630
Gavin Lee: 对啊。

579
01:32:50.630 --> 01:32:52.339
Fan Yunjie: 是在消化，解决这个。

580
01:32:52.340 --> 01:32:53.000
Gavin Lee: 就是。

581
01:32:53.510 --> 01:33:01.850
Fan Yunjie: 所以ye没错，ok，我觉得差不多了，基本上。

582
01:33:02.910 --> 01:33:07.419
Gavin Lee: 不行哇，你这个问题挺多的。


受访人33:

10
00:00:39.290 --> 00:00:46.480
Fan Yunjie: Ok，我先跟你讲一下为什么会存在这样的一个一个课题吧。

11
00:00:46.860 --> 00:00:48.120
Ivan: 好呀，为什么呢？

12
00:00:48.300 --> 00:00:53.849
Fan Yunjie: 本身aitms这个事情呢，是我们vp，他自己很感兴趣的事情，

13
00:00:53.970 --> 00:01:00.319
Fan Yunjie: 然后他就针对于这个方向，他就有一个portfolio，有一些涵盖像是一些

14
00:01:00.350 --> 00:01:14.290
Fan Yunjie: 论文的产出啊，学术方面的，然后一些key study啊，然后对标到那个hbs的就是have a business school的，然后所以就会涉及到一大堆的样本的积累啊。但是tms

15
00:01:14.490 --> 00:01:23.229
Fan Yunjie: 本身人机交互的这一块，我们聚焦的是人的这个thinking，所以说很大程度之上，

16
00:01:23.510 --> 00:01:29.750
Fan Yunjie: 我们在一开始选样本的时候，选的都是什么人呢？选的都是你这类的，就是academic level的这些。

17
00:01:29.750 --> 00:01:32.040
Ivan: 所以你是在做training吗？

18
00:01:32.510 --> 00:01:33.300
Ivan: 咳。

19
00:01:33.660 --> 00:01:35.160
Fan Yunjie: 到出入那个阶段，是不是。

20
00:01:35.160 --> 00:01:37.090
Ivan: 收集数，收集数据啊。

21
00:01:37.220 --> 00:01:42.989
Fan Yunjie: 那还在收集，但是每一个数，这个数据它最终的目的都是不一样的，

22
00:01:43.180 --> 00:01:55.279
Fan Yunjie: 如果比较好的呢？我们可能会放在case里面，就是回头放到business school里面就那个就比较另外一条路径了，那个就相对来说没有那么深层，但是会

23
00:01:56.000 --> 00:02:05.519
Fan Yunjie: 怎么说呢？价值含量会短平快一点，对就不一样的目的，但是我也不是走什么路径的，我都无所谓了。

24
00:02:05.680 --> 00:02:06.710
Fan Yunjie: 然后啊。

25
00:02:06.710 --> 00:02:07.290
Ivan: 是吗？

26
00:02:08.360 --> 00:02:17.620
Fan Yunjie: 然后呢？一开始都是找你这种人的就是的，这些phd们后来还有些professor，

27
00:02:18.020 --> 00:02:24.540
Fan Yunjie: 然后后来呢，发现这个问题出在哪里，就是因为你们这些人都不是很信任。哎，

28
00:02:24.970 --> 00:02:26.910
Fan Yunjie: 你应该不信任吧？对吧？

29
00:02:26.910 --> 00:02:28.090
Ivan: 我信任啊。

30
00:02:28.740 --> 00:02:30.369
Fan Yunjie: 你，你信任啊。

31
00:02:30.660 --> 00:02:32.180
Ivan: 啊。我很显然。

32
00:02:32.670 --> 00:02:36.390
Fan Yunjie: 那你不是被乒乓球算了。

33
00:02:36.720 --> 00:02:38.140
Ivan: Put in a bus the way.

34
00:02:38.580 --> 00:02:40.480
Fan Yunjie: 批判性思维。

35
00:02:41.930 --> 00:02:50.169
Fan Yunjie: 然后呢，我就是突然觉得这个跟我们的主题有点相悖，因为本身就是想要去优化这个事情，

36
00:02:50.320 --> 00:02:54.789
Fan Yunjie: 大家都不信任，那我们就是有点对觉得

37
00:02:55.000 --> 00:03:06.599
Fan Yunjie: 跟我们一开始的主旨有点脱离，所以说觉得还是要找一些可能的人，所以开始就把这个这个东西给扩大了，把这个样本量给扩大，

38
00:03:06.950 --> 00:03:26.600
Fan Yunjie: 后来之后发现真的是非常的不一样，大家都开始追求efficiency了，真棒我根本不管那个话说的真假，也不去论证他说的假话就跟着他走，ok，非常的苦。然后对于这个就有一些可以痛点的嘛，就是可以去研究的，所以目前来说的话

39
00:03:26.760 --> 00:03:44.009
Fan Yunjie: 我也不懂为什么样本量要加我不懂，然后因为原本的已经很多了，现在还要加，然后我的工作呢？很大程度上已经把我的就是我的pool给透支掉了，

40
00:03:44.200 --> 00:03:49.560
Fan Yunjie: 因为我旁边的朋友们不论什么level的都被我踩了一遍。

41
00:03:50.290 --> 00:03:53.070
Fan Yunjie: 所以呢，现在

42
00:03:53.370 --> 00:04:03.220
Fan Yunjie: 我的那个不是还有embar那边的嘛，然后就说要给我那边的一些学生之类的。我说，哎呀，我，我讲实话，我又觉得他们的

43
00:04:03.640 --> 00:04:08.130
Fan Yunjie: 不太好，他们真的很low，所以说

44
00:04:08.300 --> 00:04:16.309
Fan Yunjie: 我觉得还是去找一些其他的，我可能要需要自己去network一下，到linking上面。Okay，我现在去稍微的

45
00:04:16.519 --> 00:04:19.809
Fan Yunjie: 看了一下你的link我查到你了。

46
00:04:20.440 --> 00:04:31.889
Ivan: 那这个这个我要说什么来着，对它用途是用research purpose嘛。

47
00:04:31.890 --> 00:04:36.230
Fan Yunjie: 你这个应该是更多程度上的research，

48
00:04:36.370 --> 00:04:45.310
Fan Yunjie: 因为就是本身对于矿training这一块来讲的话，我觉得你们也不是一个很很深度的，

49
00:04:45.450 --> 00:04:49.660
Fan Yunjie: 或者说很高品的使用的一个受众，对吧？

50
00:04:49.810 --> 00:04:52.060
Ivan: 怎么理解，对。

51
00:04:52.620 --> 00:04:53.300
Fan Yunjie: 是吧？

52
00:04:53.300 --> 00:04:57.839
Ivan: 比较小众，而且有很多我都不能讲的事情。

53
00:04:58.050 --> 00:05:02.279
Fan Yunjie: Right。所以说我看到你的link，我稍微过一下，

54
00:05:02.880 --> 00:05:09.519
Fan Yunjie: 我给你sign了connection。哎我的天呀，你你的这个linkedin真的是你一点都没有运营啊。

55
00:05:10.240 --> 00:05:12.050
Ivan: 我要运营，可以干什么。

56
00:05:12.540 --> 00:05:17.950
Fan Yunjie: 当然你不用运营，但是你这个也也

57
00:05:18.130 --> 00:05:24.990
Fan Yunjie: 好奇怪啊，你为什么挂一个wto在上面啊，跟你上面的不符合。

58
00:05:26.430 --> 00:05:28.019
Ivan: 对啊，这有用。

59
00:05:28.960 --> 00:05:30.280
Fan Yunjie: 有什么用啊。

60
00:05:30.470 --> 00:05:35.100
Ivan: 然后我之前去面试，他们看到就觉得

61
00:05:35.770 --> 00:05:41.780
Ivan: 就是在圈子里大家都喝酒啊，所以就是

62
00:05:43.270 --> 00:05:49.440
Ivan: 对吧，就是在这个有有这个找找工作的时候，他愿意多看你两眼。

63
00:05:51.080 --> 00:05:56.270
Fan Yunjie: might be，或许吧，但是你又不是前端的岗位呀。

64
00:05:58.100 --> 00:06:05.730
Ivan: 对喝酒也不是为了去谈生意，喝酒只是为了

65
00:06:06.150 --> 00:06:15.629
Ivan: 有这种让让我感觉比较费劲，这个他们大家的就

66
00:06:15.740 --> 00:06:21.549
Ivan: 就是让他们看到我的多元性。我并不是一个只会工作的人。

67
00:06:22.420 --> 00:06:24.000
Fan Yunjie: Okay, okay.

68
00:06:24.130 --> 00:06:27.179
Fan Yunjie: Okay。我很多元。我非常的多元，

69
00:06:27.820 --> 00:06:34.300
Fan Yunjie: 我是一个什么都略懂的人，我没有办法像你那么垂直。

70
00:06:34.800 --> 00:06:50.839
Fan Yunjie: 所以ye我们现在开始这个back to our research吧。所以说我，我我想要去先要了解一下那个ai本身在在这一块应用的一个现状是怎么样子的，你方便介绍一下吗？

71
00:06:52.710 --> 00:06:55.109
Ivan: 就在公司里面。

72
00:06:55.540 --> 00:06:59.199
Ivan: 一般都会有一个，

73
00:06:59.450 --> 00:07:06.690
Ivan: 也不是所有公司，但是有一些公司会有它自己building的这种

74
00:07:07.340 --> 00:07:14.860
Ivan: 大语言模型，然后当然他不是，完全不是不是自己从头做的就是

75
00:07:15.380 --> 00:07:19.159
Ivan: 都是买的changept啊。Cloud cloud这些。

76
00:07:20.120 --> 00:07:23.479
Ivan: 然后它会有一些限制就是你有一些。

77
00:07:24.000 --> 00:07:26.149
Ivan: 可以

78
00:07:27.160 --> 00:07:35.350
Ivan: 有一些东西是不可以在在这个ai上面询问，有些问题是不可以在上面询问不可以输入的，

79
00:07:35.780 --> 00:07:40.200
Ivan: 然后有些监控，但是

80
00:07:40.440 --> 00:07:45.260
Ivan: 但是就比如说你有时候有coding，你想不起来了就感觉。

81
00:07:45.460 --> 00:07:51.869
Ivan: 然后你可以问一下，但是感觉更多就是像在像google的替代品么？

82
00:07:53.580 --> 00:08:02.320
Fan Yunjie: 它更大成分哪个阶段呢？是在strategy阶段吗？还是说一些一些data processing的阶段。

83
00:08:04.520 --> 00:08:08.730
Ivan: 就可能还是比较底层一点的。

84
00:08:09.450 --> 00:08:16.290
Ivan: 对啊，就是data processing，你就只是你要写那个script的时候，你可能可以问一问，

85
00:08:16.480 --> 00:08:22.149
Ivan: 你也不能把数据输进去啊，就这些都是不行的，因为

86
00:08:22.590 --> 00:08:27.630
Ivan: 因为他学，我们比较害怕的是我们的策略被他学了。

87
00:08:28.400 --> 00:08:29.699
Fan Yunjie: 然后。

88
00:08:29.800 --> 00:08:38.499
Ivan: 然后。然后虽然公司在公司级别。他会有一些数据的隐私的保方面的

89
00:08:38.799 --> 00:08:45.369
Ivan: 考量，然后就咳，这些数据都是归公司所有了，但是

90
00:08:45.750 --> 00:08:52.010
Ivan: 在公司内部，trading team之间互相也是竞争对手，所以你也大家。

91
00:08:52.740 --> 00:09:00.279
Ivan: 也不想对吧，就是把这个自己的信息提交到公司级别，然后

92
00:09:00.420 --> 00:09:10.000
Ivan: 让别的training team受益，所以就大家还是比较谨慎，而且在这个行业里面

93
00:09:11.190 --> 00:09:14.300
Ivan: 比ai聪明的人还是比较还是更多一些？

94
00:09:14.690 --> 00:09:21.390
Ivan: 对，就是你不需要，就是你可能有一些想不起来的。你问问他，但是

95
00:09:21.680 --> 00:09:24.509
Ivan: 你并不并不需要完全依赖它，

96
00:09:25.390 --> 00:09:29.290
Ivan: 这是对大大语言模型是这样，然后

97
00:09:30.110 --> 00:09:33.959
Ivan: 对神经网络会有一些具体的使用，

98
00:09:34.440 --> 00:09:38.719
Ivan: 就是用来做一些交易策略的啊这个

99
00:09:39.780 --> 00:09:43.669
Ivan: 现在算是一个新的趋势吧，

100
00:09:43.820 --> 00:09:53.530
Ivan: 但是就是发展比较滞后，因为主要原因还是因为这个东西。

101
00:09:54.420 --> 00:10:00.949
Ivan: 以前大家觉得他就是我们要做成策略的。

102
00:10:02.570 --> 00:10:09.940
Ivan: 做要做的策略必须得你做策略的人必须得非常了解他，你才敢试盘。

103
00:10:10.510 --> 00:10:11.359
Fan Yunjie: 对啊。

104
00:10:11.360 --> 00:10:15.660
Ivan: 然后ai做的，以前大家不是很信任它，因为

105
00:10:16.260 --> 00:10:25.460
Ivan: 就是也不懂，就是不懂为什么他会要求你去这么做交易，所以

106
00:10:26.210 --> 00:10:31.979
Ivan: 大多数公司都比较传统，都不太想要使用这种新的技术，

107
00:10:32.690 --> 00:10:39.039
Ivan: 但是后来慢慢有有一些公司开始尝试之后获得了巨大的成功。

108
00:10:39.230 --> 00:10:46.410
Ivan: 所以就最近这两年，大家也都各个公司也都开始消防都开始使用，

109
00:10:47.490 --> 00:10:56.160
Ivan: 对，但就整体来讲还处于一个不是特别popular的一个阶段。

110
00:10:56.880 --> 00:10:59.490
Fan Yunjie: 那那他为什么生成策略呢？

111
00:10:59.830 --> 00:11:08.600
Fan Yunjie: 大家会不信任它，因为本身其实它只是一个一个学习相关的模型，然后去进行概率铲除的一个问题。

112
00:11:08.600 --> 00:11:11.389
Ivan: 对，那你要你要了解你的模型

113
00:11:11.510 --> 00:11:20.369
Ivan: 就是，如果是传统的数学模型，我们是完全了解这个模型，他能做什么，他不能做什么。他这个模型之所以

114
00:11:20.470 --> 00:11:28.890
Ivan: 可以这样是因为在什么样的假设的前提下是这样，但是你用new network，它是一个black box，你可能

115
00:11:29.090 --> 00:11:35.129
Ivan: 知道他的神经网络的结构，然后你可可以根据结构大概知道他在做什么，

116
00:11:35.500 --> 00:11:44.929
Ivan: 但是它训练出来的那些高维的feature，我们很难解释他为什么这些feature是是这样，

117
00:11:45.560 --> 00:11:50.849
Ivan: 这个，这个，这个是主要原因。然后就因为你很难解释它，所以

118
00:11:51.100 --> 00:11:54.040
Ivan: 你在使用的时候你也不敢用，因为

119
00:11:54.180 --> 00:11:59.789
Ivan: 它赚钱的时候当然是好的了，但是它就就也也会有赔钱的时候，

120
00:12:00.130 --> 00:12:03.180
Ivan: 那它就就是一个variance，

121
00:12:03.360 --> 00:12:09.379
Ivan: 不一定只是正向的，不一定只是赚钱的variance，他也可以是不赚钱的。

122
00:12:09.710 --> 00:12:16.630
Ivan: 所以如果variance太大的话，那我们可能

123
00:12:17.120 --> 00:12:22.610
Ivan: 赚钱的时候可能会赚一些，但赔钱的时候有可能会赔更多，那这种情况

124
00:12:22.820 --> 00:12:28.120
Ivan: 我们不理解它就就不敢不敢用啊，就不知道它什么时候赚，什么时候赔。

125
00:12:28.760 --> 00:12:36.340
Fan Yunjie: 可是ai是可以训练的呀，你可以通过你的prompt去递进的，让它有一个方法论出来呀对吧？

126
00:12:43.320 --> 00:12:47.409
Ivan: 这问题不在训练上，问题在于，我们不理解他。

127
00:12:49.240 --> 00:12:55.669
Fan Yunjie: 是不理解它本身产出的这个策略，他的依据在哪里？

128
00:12:55.670 --> 00:12:57.239
Ivan: 是不理解过程。

129
00:12:58.240 --> 00:13:02.490
Fan Yunjie: 就是如果说这个过程，让你们很清晰的看到，你们或许会信任他。

130
00:13:03.200 --> 00:13:03.870
Ivan: 对啊。

131
00:13:05.460 --> 00:13:08.899
Fan Yunjie: 那你觉得它本身产出的三个策略。

132
00:13:08.900 --> 00:13:23.869
Ivan: 举个简单的例子吧，就是你用，那你我们平时也会做一些预测预测主要都是线性模型，然后这两年开始用一些树模型，但就是这无论是线性模型还是数模型相对来讲

133
00:13:24.300 --> 00:13:26.550
Ivan: 都是比较容易解释的，因为

134
00:13:26.720 --> 00:13:32.990
Ivan: 你的input都是你做的那些feature。Hang，

135
00:13:33.130 --> 00:13:40.910
Ivan: 那这些feature你在做的时候，你已经很了解这个feature，它你为什么要做成这样？

136
00:13:41.090 --> 00:13:46.499
Ivan: 当然你在new network的话，你也可以输入同样的feature。但是，

137
00:13:46.790 --> 00:13:50.129
Ivan: 最理想的状况是你直接输入

138
00:13:50.230 --> 00:14:02.000
Ivan: 原始的数据，然后它一层一层，因为因为new network在之前的几层都是其实本质，它做那些embedding啊，那些其实本质都是在，

139
00:14:02.460 --> 00:14:07.069
Ivan: 就是做feature encoding，然后

140
00:14:08.180 --> 00:14:14.180
Ivan: 然后，但是他他自己训练出来的这些feature我们不太理解他为什么是这样。

141
00:14:15.070 --> 00:14:17.929
Ivan: 然后。

142
00:14:18.530 --> 00:14:22.190
Ivan: 所以因为这个原因，所以就

143
00:14:22.910 --> 00:14:26.900
Ivan: 我们只我们只只能看到。

144
00:14:27.200 --> 00:14:33.189
Ivan: 原始的数据输入了，但是我们不了解它。在输入之后，

145
00:14:33.300 --> 00:14:40.070
Ivan: 他做了什么，让让这个数据变成变成了一个可以赚钱的信号。

146
00:14:42.130 --> 00:14:48.560
Fan Yunjie: 你觉得他现在连一个junior的，他，他的都太替代替代不了，是吗？

147
00:14:56.070 --> 00:15:06.379
Ivan: 对，就是我们谈论的还是有点不太一样。我，我刚才说的是把它当成一个工具，一个用来做交易信号的工具，

148
00:15:07.150 --> 00:15:09.669
Ivan: 然后本质就是还是把它

149
00:15:10.510 --> 00:15:16.510
Ivan: 就是一种模型，然后一种可以产出交易信号的模型。但在实际确定的过程中，

150
00:15:21.610 --> 00:15:30.740
Ivan: 实际确定的过程中，你可能会需要一种模型叫做一种ai模型，叫做reinforcement learning，就是强化学习，

151
00:15:31.130 --> 00:15:32.910
Ivan: 那强化学习。

152
00:15:33.030 --> 00:15:43.260
Ivan: 目前看来，我们还没有看到它非常就是非常有效。在trading行业。

153
00:15:44.830 --> 00:15:52.650
Fan Yunjie: 那比方说就是进行分工的话，有没有可能将来就是某一个模块的工作是交给哪儿来做啊？

154
00:15:54.100 --> 00:16:01.959
Ivan: 这个可以啊对，就像像现在成功的有几家新起的，成功的公司，他们

155
00:16:02.190 --> 00:16:11.500
Ivan: 基本上就完全用ai去替代了这个人工去做feature，这些过程就

156
00:16:11.840 --> 00:16:17.779
Ivan: 就就是原始数据输入，然后他们也不需要解释，就因为

157
00:16:18.350 --> 00:16:25.610
Ivan: 因为他们之前做过很多测试，就是你用人手工的feature就是

158
00:16:26.120 --> 00:16:37.610
Ivan: 大家能想到的那些feature genius，feature其实就只有那么几个，然后现在行业都卷的行业比较卷，所以大家都已经都差不多，用的东西都差不多，

159
00:16:38.230 --> 00:16:41.539
Ivan: 所以就没有更多的提升了。

160
00:16:42.410 --> 00:16:48.510
Ivan: 然后他们用这些他们已知的这些feature去和ai做比较，最后发现

161
00:16:48.990 --> 00:16:52.840
Ivan: 就比不过它了，所以就。然后再加上。

162
00:16:53.380 --> 00:17:03.819
Ivan: 就是交交易也是1.1点来的嘛，就是他也不是说我一上来就全部都用ai，然后，它就是一部分ai一部分。以前的这种传统模式，后来

163
00:17:04.119 --> 00:17:07.469
Ivan: 发现ai就也慢慢就稳定了，

164
00:17:08.520 --> 00:17:12.859
Ivan: 然后就他们就后来就完全用new network。

165
00:17:13.740 --> 00:17:21.479
Fan Yunjie: 那这种替代会大概多大程度上的增加你们的efficiency大概从比百分比上来说的话。

166
00:17:21.480 --> 00:17:30.450
Ivan: 这个也efficiency我不知道，但是赚钱是真的，他们是赚的更多了。

167
00:17:32.310 --> 00:17:41.169
Fan Yunjie: 原因是因为它比较稳定性的产出，还有比较像你们像你们来讲的话，可信任的产出了吗？

168
00:17:42.570 --> 00:17:47.759
Ivan: 原因是因为他们找到了我们没有找到的交易信号。

169
00:17:49.950 --> 00:17:57.050
Fan Yunjie: 那这种他跟人的这个weakness相比的话，它它的为什么可识别呢？

170
00:18:00.060 --> 00:18:06.129
Ivan: 是是什么意思？我不太理解什么叫跟人比weakness可识别。

171
00:18:06.500 --> 00:18:15.769
Fan Yunjie: 就是人为什么没有办法去获得到那么多的交易信号，它的弱点跟ip相差在哪里？在这种。

172
00:18:16.040 --> 00:18:24.489
Ivan: 人也可以做，交但是人都差不多，所以公司各个，这是一个零和游戏就是你要比别人

173
00:18:24.800 --> 00:18:28.009
Ivan: 你牛逼不行，你要比别人更牛逼，你才能赚钱，

174
00:18:29.340 --> 00:18:37.269
Ivan: 他不是对，就是然后人，这这些公司人都差不多，所以大家这个

175
00:18:38.070 --> 00:18:44.619
Ivan: 智商都挺高的，然后也发展很多年了，然后这些现有的这些

176
00:18:44.780 --> 00:18:50.560
Ivan: 人能想出来的，大家都想的差不多了，所以才会出现，

177
00:18:51.500 --> 00:18:58.950
Ivan: 有的公司用这个神经网络去找到了一些大家想出来的feature，

178
00:18:59.460 --> 00:19:02.080
Ivan: 然后就比别人多了一个edge。

179
00:19:03.700 --> 00:19:11.590
Fan Yunjie: 那这一些feature也是会经历一个团队去重新验证的一个情况是吗？就它输出之后，团队要去。

180
00:19:11.800 --> 00:19:12.920
Fan Yunjie: 对。

181
00:19:13.350 --> 00:19:14.219
Ivan: 对啊，

182
00:19:14.350 --> 00:19:27.349
Ivan: 就是它输出之后你要看它其实就是做prediction嘛，就是大主流就是做做预测嘛，然后那他预测的更准了，那就牛逼啊，那ai就厉害啊。

183
00:19:28.860 --> 00:19:36.979
Fan Yunjie: Ok，所以这一个这一个比重，在你们工作当中，它大概会占多少，就是针对你们。

184
00:19:36.980 --> 00:19:43.490
Ivan: 在我们工作中暂时还没有，但是在行业里已经开始慢慢

185
00:19:44.180 --> 00:19:45.799
Ivan: 慢慢越来越多了。

186
00:19:47.110 --> 00:19:54.460
Fan Yunjie: 如果说他的这跟你们本身的一些sense正好相左，会怎么办？会不会有这种情况？

187
00:19:56.370 --> 00:19:58.070
Ivan: 向左是什么意思？

188
00:19:58.430 --> 00:20:01.890
Fan Yunjie: 就是跟你们自己的经验的预测不太一样。

189
00:20:02.850 --> 00:20:08.600
Ivan: 会啊，会有这种情况呀，但是这是可以测试的呀，是可以回测的呀。

190
00:20:09.420 --> 00:20:13.199
Fan Yunjie: 就是根据历史数据，历史数据来做回测。

191
00:20:14.110 --> 00:20:17.609
Ivan: 他可能并是反直觉的，但是他work。

192
00:20:19.530 --> 00:20:26.349
Fan Yunjie: Okay，那是会有一个怎怎么说，你们是有回测之后人工复合之后

193
00:20:26.890 --> 00:20:34.109
Fan Yunjie: 大概多少的几率才能会建立起来。你们是比较流程化的，去使用ai。

194
00:20:41.810 --> 00:20:46.209
Ivan: En这个使用a，使不使用ai的

195
00:20:46.400 --> 00:20:52.190
Ivan: 决定因素有很多，那并不是并不是一个

196
00:20:52.300 --> 00:20:55.629
Ivan: 他厉害就用它的过程，因为

197
00:20:56.240 --> 00:21:02.440
Ivan: 使用ai，你不是说我想用就能用。首先我们还需要有，

198
00:21:02.890 --> 00:21:13.059
Ivan: 首先从公司级别它这个就是决策者需要愿意使用ai。

199
00:21:14.260 --> 00:21:21.040
Ivan: 其次，使用ai做这些训练需要投入很多钱去基础设施，

200
00:21:21.690 --> 00:21:28.100
Ivan: 就比如说需要很多显卡勾了很多显卡，那这些显卡购入它

201
00:21:28.850 --> 00:21:31.920
Ivan: 一个trading team去购入是很困难的。

202
00:21:32.470 --> 00:21:34.800
Fan Yunjie: 一般都是以公司。

203
00:21:34.810 --> 00:21:42.980
Ivan: 来，以公司，这个整个公司，他会去以公司整个公司的这个。

204
00:21:43.570 --> 00:21:53.310
Ivan: 决策去去去购购入这个这些比较昂贵的，对昂贵的基础设施。

205
00:21:54.670 --> 00:21:55.760
Ivan: 然后，

206
00:21:55.930 --> 00:22:05.829
Ivan: 然后。然后有了这些基础设施之后，还就是你，它并不是一个outbox的东西，它这些模型需要有人去训练，

207
00:22:05.940 --> 00:22:16.020
Ivan: 那作为传统做training行业的人，他又不是做ai的，他也他不知道怎么训练，那怎么办，你就得去招聘相应的人。

208
00:22:16.680 --> 00:22:19.950
Fan Yunjie: 然后在这个行业是一个非常。

209
00:22:20.040 --> 00:22:30.840
Ivan: 就是他要求你能尽量赚赚快钱，就是你不可以等很多时间

210
00:22:31.100 --> 00:22:35.459
Ivan: 才赚到钱，整体来讲是这样。当然

211
00:22:35.580 --> 00:22:39.380
Ivan: 有一些公司，他愿意等，可能他最后就等成功了，但是

212
00:22:39.670 --> 00:22:48.699
Ivan: 有些公司大多数公司他不能等它，你给你投了钱，他需要看到有很快的效果，

213
00:22:49.840 --> 00:22:56.380
Ivan: 那这个但是你去做出这么一个work work。这种模型

214
00:22:57.830 --> 00:23:07.580
Ivan: 是是很是很是一个是一件很困难的事情，所以他需要投很多钱去

215
00:23:08.180 --> 00:23:11.219
Ivan: 这方面的资深的专家身上，

216
00:23:12.230 --> 00:23:17.760
Ivan: 然后还得work，如果不work的话呢，他就他就不想，他就不会再往这上面投钱了，

217
00:23:19.040 --> 00:23:27.390
Ivan: 所以就是它取决于这个东西能不能被使用，并不仅仅是因为这个模型，它

218
00:23:28.030 --> 00:23:34.269
Ivan: 这这个，这个不不一定是模型，就是这个方法。Ai这个方法它好，

219
00:23:34.930 --> 00:23:39.260
Ivan: 并不完全是由他决定，他可能很好，但如果他目前还没有

220
00:23:39.430 --> 00:23:45.540
Ivan: 比公司的这些传统方法赚更多的钱，那很多公司他就不愿意往这里面投钱。

221
00:23:47.110 --> 00:23:50.610
Fan Yunjie: 我理解就是可能在管理层的领域，

222
00:23:50.690 --> 00:24:10.029
Fan Yunjie: 他们更大程度上会去杜绝一个事情就是一些资源臃肿，资源，荣誉的事情，我花很多钱雇你们这些很贵的trend，然后我要花很多钱去training这些program training。这些infra，所以说会存在一个可能很大程度上的一个时间段里，面对。

223
00:24:10.030 --> 00:24:15.559
Ivan: 这就和公司的整体战略有很大的关系，就他

224
00:24:15.950 --> 00:24:20.709
Ivan: 对吧，就是你很厉害，但公司不愿意投钱在这个上面那也没有用。

225
00:24:21.170 --> 00:24:26.859
Ivan: 就这就跟在这个行业找工作一样，就是你水平很高，但是

226
00:24:27.020 --> 00:24:33.149
Ivan: 你比如说你是一个高频的，你是一个中频做，但是你水平很高，但是

227
00:24:33.310 --> 00:24:37.649
Ivan: 你水平再高，你没有锋利的保健，你也没法，

228
00:24:38.450 --> 00:24:42.770
Ivan: 就是没法没法预敌，没法杀敌，所以

229
00:24:43.550 --> 00:24:49.089
Ivan: 那这个forty保建就看这个公司愿不愿意给你去造这把宝剑。

230
00:24:49.760 --> 00:25:03.720
Fan Yunjie: 我理解，但是我的问题在于，本身这种情况之下，那那ai或者说是相对来说其他的一些工具介入，它会不会就是一个必然的趋势，而不是在于说你要不要去经历这种disruption。

231
00:25:07.170 --> 00:25:07.800
Ivan: En

232
00:25:12.560 --> 00:25:22.360
Ivan: 这个我觉得每个人观点是不一样的。在我看来，我对ai是比较乐观的就是我觉得它能取代一部分工作，就比如说

233
00:25:22.570 --> 00:25:31.679
Ivan: 有一些developer的工作可能就不太需要了，然后就只需要像cone develop developer这个position可能就

234
00:25:31.970 --> 00:25:39.790
Ivan: 不再那么重要了，就你只需要非常资深的low latency的这种developer和

235
00:25:40.070 --> 00:25:42.809
Ivan: 比较比较好的trend。

236
00:25:43.200 --> 00:25:52.520
Ivan: 对，就是有有一些就你只需要最好的，你不需要中间水平一般的了，其实这个水平一般都可以被ait的。

237
00:25:53.220 --> 00:25:53.810
Fan Yunjie: 但是我。

238
00:25:53.810 --> 00:25:55.050
Ivan: 这个我觉得会有些影响。

239
00:25:55.840 --> 00:26:03.570
Fan Yunjie: 但是如果这样子的话，okay ai，我承担了很多这些某些专业职能，然后

240
00:26:03.820 --> 00:26:10.519
Fan Yunjie: 就是团队成员，这里面有是承担一个的，然后有一个比较senior的这两样，但是他们

241
00:26:10.520 --> 00:26:24.489
Fan Yunjie: 的经历，你们当然是这一代，有有这个经验，你们从没有ai时代过渡过来，你们有这个经验可以，但如果针对于这些比较新人的话，他们没有这个经验，他们会不会准入这个

242
00:26:24.580 --> 00:26:28.199
Fan Yunjie: 这个市场，或者说将来求职的这个比例会更高。

243
00:26:29.090 --> 00:26:33.060
Ivan: 这是肯定的呀，现在就求职会越来越难啊。

244
00:26:34.210 --> 00:26:41.090
Ivan: 本来本来本来进入这个行业就非常困难。然后现在bar越来越高了。

245
00:26:42.590 --> 00:26:48.649
Ivan: 对，就是对吧？你就像我们如果是招fresh graduate，

246
00:26:48.880 --> 00:26:52.880
Ivan: 那只会挑那么几个学校的某几个专业，

247
00:26:53.950 --> 00:27:00.430
Ivan: 然后你没进到那几个学校就已经out，你只能去别的公司，

248
00:27:00.800 --> 00:27:09.830
Ivan: 像去投行，可能你可以去投行，或者去一些小小小小一点不太有名气的公司去打磨你的工作经历，

249
00:27:11.130 --> 00:27:16.229
Ivan: 那对吧？那就就就是。然后这这种情况会

250
00:27:16.360 --> 00:27:21.900
Ivan: 随着ai的广泛使用，从人力上讲会

251
00:27:22.190 --> 00:27:27.039
Ivan: 需要的人越来越少，那很多人就越来越难进入到这个行业了，

252
00:27:27.820 --> 00:27:30.110
Ivan: 因为他不需要了这个，

253
00:27:30.640 --> 00:27:40.390
Ivan: 这个投资人也不能说投资人，就这些合伙人或者是投资人，投资人其实也不是很care就合伙人吧。然后他们

254
00:27:40.530 --> 00:27:47.310
Ivan: 最主要还是希望能够花小钱办大事，那就

255
00:27:47.790 --> 00:27:51.409
Ivan: 尤其是在人力上，因为人力是最主要的一个

256
00:27:51.930 --> 00:27:54.250
Ivan: 这个行业里最主要的一个开支。

257
00:27:55.600 --> 00:27:59.590
Fan Yunjie: Ok，稍等一下那个酒店服务过来一下，稍等五分钟啊。

258
00:28:00.300 --> 00:28:01.130
Ivan: 然后。

259
00:29:21.670 --> 00:29:27.139
Fan Yunjie: 啊sorry，我就屋的那个卫生间在漏水，要放他们进来关一下那个水房。

260
00:29:28.090 --> 00:29:36.629
Fan Yunjie: Okay，那okay。我们继续刚刚那个问题。所以所以怎么能够保证就是一些关键领域的掌控，

261
00:29:36.840 --> 00:29:43.409
Fan Yunjie: 人类的这些交易员可以持续的去进入呢？如果说对于新人的壁垒这么高的话，

262
00:29:46.590 --> 00:29:48.380
Fan Yunjie: hello, dear.

263
00:29:48.380 --> 00:29:55.310
Ivan: 哈喽，斗斗稍等稍等。

264
00:29:56.300 --> 00:29:57.980
Ivan: 拿一下咖啡。

265
00:30:03.320 --> 00:30:05.499
Fan Yunjie: 天到底要喝多少克啊。

266
00:30:05.960 --> 00:30:11.800
Ivan: 睡太少了，昨天所以喝点咖啡，然后保持清醒。

267
00:30:12.970 --> 00:30:17.299
Fan Yunjie: 我这几天都睡很少，我这几天每天都是四五个小时。

268
00:30:20.410 --> 00:30:21.390
Fan Yunjie: 哇。

269
00:30:21.390 --> 00:30:22.560
Ivan: 四五个小时左右。

270
00:30:22.970 --> 00:30:25.189
Ivan: 我昨天三点才睡的。

271
00:30:28.000 --> 00:30:31.379
Fan Yunjie: 今天9.9点钟的话也六个小时了。

272
00:30:33.150 --> 00:30:34.990
Ivan: 对有道理。

273
00:30:35.420 --> 00:30:36.270
Fan Yunjie: 对啊。

274
00:30:37.360 --> 00:30:38.760
Ivan: 那你继续问吧。

275
00:30:39.760 --> 00:30:52.799
Fan Yunjie: 对啊，就是刚刚的问题嘛，又你的准入并没有那么高，然后呢？Ai又可以替代一些很那本，那怎么能够可持续的，让人类对一些关键的领域有一些掌控啊。

276
00:30:59.680 --> 00:31:01.290
Ivan: 这个

277
00:31:02.700 --> 00:31:09.940
Ivan: 就是我们现在在使用的阶段，还没有达到这个人类没法掌控的阶段

278
00:31:10.880 --> 00:31:22.250
Ivan: 就是尤其是在在在量化行业就是大家都是最主要就是还是要掌控它，

279
00:31:22.750 --> 00:31:27.829
Ivan: 所以不能掌控的东西其实并没有被广泛的使用，

280
00:31:28.030 --> 00:31:37.660
Ivan: 就并没有被使用吧。甚至当然，所谓的掌控也是分就是

281
00:31:38.260 --> 00:31:44.600
Ivan: 就是就是看是有，就是就是to what extent。

282
00:31:47.370 --> 00:31:49.309
Fan Yunjie: 但你你刚刚提到了一个问题。

283
00:31:49.310 --> 00:31:49.810
Ivan: 对啊。

284
00:31:49.810 --> 00:31:59.690
Fan Yunjie: 就是说，ai的介入会导致一些新人进来的时候，他们本身的职能会被稀释掉很多，所以。

285
00:31:59.690 --> 00:32:00.590
Ivan: 对啊。

286
00:32:00.900 --> 00:32:03.780
Fan Yunjie: 所以就是可能针对于他们的。

287
00:32:05.650 --> 00:32:17.450
Fan Yunjie: 怎么说呢？他们本身的这些需求也变少了，那他们变少之后，他们在网上的的这个资源就会就会慢慢的截流掉啊对吧，

288
00:32:17.450 --> 00:32:27.040
Fan Yunjie: 那谁过来去掌控本身，你们你们现在确实是在掌控，但是未来的这个workflow怎么去保证，就是就是去避免

289
00:32:27.090 --> 00:32:32.329
Fan Yunjie: 在培养他们的时候就是ai介入来保证他们的历练吗？还怎么样？

290
00:32:39.110 --> 00:32:47.330
Ivan: 不太懂，最后就我们只是用ai作为作为一个工具，但是

291
00:32:47.630 --> 00:32:53.079
Ivan: 工具就两种类型的工具，一种是去做交易信号的这种是

292
00:32:54.320 --> 00:33:05.470
Ivan: 目前不太成熟，但是有成功的公司，有几家比较成功的公司，所以大家都开始打算要做，那他，你把它当成是一个

293
00:33:05.730 --> 00:33:07.930
Ivan: 做交易信号的工具的话，

294
00:33:08.650 --> 00:33:17.300
Ivan: 那你那那对，就是可能这些junior，他从这个角度上讲，这些junior可能不太需要

295
00:33:17.600 --> 00:33:23.890
Ivan: 去从头开始研究怎么去做feature啊，怎么去做prediction啊？

296
00:33:24.250 --> 00:33:30.369
Ivan: 他可能更多的是需要对这new network怎么去训练？New network有一些了解。

297
00:33:30.750 --> 00:33:33.639
Fan Yunjie: 然后去理解为什么要这么做。

298
00:33:34.240 --> 00:33:45.930
Ivan: 然后，但是这个on top of this，它还有一些其他的工作，比如说你

299
00:33:46.390 --> 00:33:55.850
Ivan: 有些工作是没有办法跟用ai去做的，比如说，就是你跟broker的那个。

300
00:33:58.250 --> 00:34:04.280
Ivan: 的，这个谈费率啊什么的，还有一些otc的交易，这些就是。

301
00:34:04.280 --> 00:34:05.430
Fan Yunjie: 的问题。

302
00:34:06.290 --> 00:34:08.819
Ivan: 啊对，就就就就

303
00:34:09.260 --> 00:34:16.530
Ivan: 也不是撮合，就比如说broker，他可能给不同公司不同，team的费费率可能不太一样，

304
00:34:16.790 --> 00:34:19.709
Ivan: 那怎么去谈这个费率，那这种事情

305
00:34:20.350 --> 00:34:26.110
Ivan: 就是当然也不会找junior了，但是可能会就是会

306
00:34:27.139 --> 00:34:35.090
Ivan: ai不能做的事情，可能人会去多做一些，还有就是做一些咳，

307
00:34:36.130 --> 00:34:40.810
Ivan: 就是你用ai可以，比如说你可以赚钱，但是

308
00:34:40.920 --> 00:34:47.700
Ivan: 无论你赚钱还是赔钱，你我们，我们对我们来说都很重要的一件事情就是要做复盘，

309
00:34:48.760 --> 00:35:01.279
Ivan: 做复盘这件事情需要人去做，因为你需要理解为什么你赚钱，为什么你赔钱，你哪做对了，你去赚，你才赚钱，你哪做做的不对，你才赔钱。

310
00:35:01.490 --> 00:35:03.919
Fan Yunjie: 或者是你赚了谁的钱。

311
00:35:04.210 --> 00:35:07.110
Ivan: 然后然后你

312
00:35:07.970 --> 00:35:15.350
Ivan: 是吧，你就是这点是很重要，你要非常清楚你能赚这个钱是赚了谁的钱，

313
00:35:15.710 --> 00:35:23.340
Ivan: 因为它是零和所以对吧，你是赚了散户的钱，还是你是赚了机构的钱。机构是什么类型的机构是你的竞争对手

314
00:35:23.590 --> 00:35:27.429
Ivan: 还是就是比如说投行

315
00:35:27.710 --> 00:35:39.940
Ivan: 还是对啊，就是这个需要需要你人工自己去思考，因为这些信息你可以还还可以用ai工具去做一些辅助分析，但是

316
00:35:40.210 --> 00:35:46.240
Ivan: 最终把它变成你自己的知识，这一步是需要人工去做的。

317
00:35:47.770 --> 00:35:51.049
Fan Yunjie: 明白。所以说整个在你们的这个。

318
00:35:51.050 --> 00:35:51.550
Ivan: 是。

319
00:35:51.550 --> 00:35:52.060
Fan Yunjie: Workflow.

320
00:35:52.060 --> 00:35:58.609
Ivan: 然后就这些工作还是需要有人去做的，然后但是就是junior来讲的话，他可能

321
00:35:59.740 --> 00:36:05.940
Ivan: 刚开始不不太容易做，那我们就肯定会去找那些最

322
00:36:06.620 --> 00:36:08.890
Ivan: 就白逛最好的猪女儿

323
00:36:09.170 --> 00:36:15.170
Ivan: 是就是就是学的比较快的，然后你可以去训练他。但是

324
00:36:15.640 --> 00:36:20.030
Ivan: 就是。而且这个我觉得并不是一个问题，因为

325
00:36:20.410 --> 00:36:30.949
Ivan: 从公司来讲，招人会一直是会有的，但是只是招人变少了，然后简单讲就是更加精简，

326
00:36:31.190 --> 00:36:37.659
Ivan: 只招最优秀的人，然后，但是不招很多人。

327
00:36:39.990 --> 00:36:51.480
Ivan: 就是这些。以前以前因为有一些比如说你是一个中评公司，那你这个中评公司需要写很多很多的feature，写很多很多的alpha把它堆起来，

328
00:36:51.990 --> 00:36:55.700
Ivan: 那这个那很多公司他就走，这种

329
00:36:56.250 --> 00:37:05.440
Ivan: 叫什么？就是我们也不能说他是血汗工厂吧，就是他会以比较低于市场价格的薪水去招聘一些

330
00:37:05.740 --> 00:37:14.480
Ivan: 大量的就背景还不错，但不一定是最好的这些人去给他们去写alpha。

331
00:37:15.730 --> 00:37:25.199
Ivan: 写写策略，然后再把这些，因为因为中频的话它不确定性比较高，比较比较certain valence比较高，你要

332
00:37:25.380 --> 00:37:28.799
Ivan: 持刀时间越久，预测的时间越久，

333
00:37:29.310 --> 00:37:37.710
Ivan: 你的这个不确定性就越多，所以你就需要用大量的这些信号去堆积起来，

334
00:37:37.820 --> 00:37:44.120
Ivan: 每一个单个信号就是预测能力是比较有限，那那

335
00:37:44.250 --> 00:37:49.429
Ivan: 就是需要很多人力啊，所以就就会去大量招人。

336
00:37:49.810 --> 00:37:55.679
Ivan: 但是以后他们现在很多招聘公司也都开始用new network，

337
00:37:55.920 --> 00:38:00.249
Ivan: 以后可能就不需要招这么多人就是他就直接就是

338
00:38:01.250 --> 00:38:06.119
Ivan: 就是用这种模式的方法去替代大量的人。

339
00:38:07.620 --> 00:38:16.510
Ivan: 所以对吧，就是在我就是就是不会，人不会还会糟，只是会变少，

340
00:38:17.230 --> 00:38:22.499
Ivan: 然后长期来讲和任何这些工具

341
00:38:22.780 --> 00:38:31.800
Ivan: 就是首先人会主导这些工具，其次是也在不断从工具产生的

342
00:38:32.550 --> 00:38:37.509
Ivan: 这个结果不断从中学习。

343
00:38:39.210 --> 00:38:44.879
Fan Yunjie: 那对于未来的这些junior，他的这个training更大程度上

344
00:38:44.910 --> 00:38:59.570
Fan Yunjie: 是要确定他们对于ai模型给出来的分析跟建议的一个评估的，这个能力，还是说本身要给他一个我长期以来自己做策略的这样的，一个

345
00:38:59.590 --> 00:39:13.749
Fan Yunjie: 独行性的，一个一个一个一个thinking的能力，让他去判断我对于这个事情有点疑惑在于你们的经验是在于你们伴随时间得来的，那未来的他们如果说很对

346
00:39:13.960 --> 00:39:25.600
Fan Yunjie: 很依赖ai给出的这个建议的话，那他们也没有相关的经验，那他们更大程度上的这个分析的可靠性，他们要从哪里去找抓手呢？

347
00:39:28.730 --> 00:39:36.310
Ivan: 就是从方法论上讲，会有了这些工具之后会有一些变化，

348
00:39:36.740 --> 00:39:45.799
Ivan: 但是你训练这些人的时候，他对market的感觉，对market的了解，这个是肯定是会一直训练对，

349
00:39:45.950 --> 00:39:49.120
Ivan: 而且训这个训练也不是完全靠

350
00:39:49.900 --> 00:39:56.019
Ivan: 什么training program。这些去训练的主要还是靠你自己。在实盘上看

351
00:39:56.420 --> 00:40:01.359
Ivan: 看市场的各种数据，看市场的各种

352
00:40:02.460 --> 00:40:07.249
Ivan: 就是各种event，各种各种表现。

353
00:40:08.270 --> 00:40:17.559
Ivan: 然后逐渐日积月累形成的经验，然后这一部分还是需要人工去做的，因为ai只是

354
00:40:18.250 --> 00:40:22.690
Ivan: 就是目前被使用的ai，

355
00:40:23.090 --> 00:40:29.660
Ivan: 就像我刚才说只有两种，一种就是辅助工具就是问一些问题，到时候google在用

356
00:40:29.920 --> 00:40:33.640
Ivan: 啊。就像那些大语言模型，还有一种就是

357
00:40:34.240 --> 00:40:42.450
Ivan: 额，会用用network去做一些prediction啊，或者去做一些数据方面，数据分析。

358
00:40:43.650 --> 00:40:48.489
Ivan: 然后去形成一些交易信号，那这这这一部分

359
00:40:48.880 --> 00:40:52.939
Ivan: 就是新招的人，他其实本质上和

360
00:40:53.320 --> 00:40:59.400
Ivan: 以前的人做的东西本质上都是一样的，只是它换了一个

361
00:40:59.890 --> 00:41:04.160
Ivan: 就是步枪换换大炮，然后

362
00:41:04.990 --> 00:41:16.330
Ivan: 对，然后他可能他专注的点稍微有一些区别，就以前我们可能会花很多时间去做feature engineering，去设计一个feature。但是现在可能。

363
00:41:16.600 --> 00:41:25.110
Ivan: 他们ai会自己训练出一些就是embedding，然后那这些

364
00:41:25.360 --> 00:41:32.369
Ivan: 他们可能重点更多在于他们怎么去理解这些embedding，然后，他们甚至可以去用一些

365
00:41:32.480 --> 00:41:36.919
Ivan: 现有的feature去做这些的近似来

366
00:41:37.110 --> 00:41:44.859
Ivan: 更好的理解，所以也就就就是他这个focus可能会有些变化，但是本质还是

367
00:41:45.090 --> 00:41:48.459
Ivan: 用它作为一种数据分析的工具。

368
00:41:49.830 --> 00:42:05.430
Fan Yunjie: 理解。所以说从比方说，从到这个贝塔到这个model training整个这个框架下来的话。是这不太可能说人机都无缝衔接，无缝协同了，或者说理想化的话，有可能这样实现吗？

369
00:42:06.450 --> 00:42:12.669
Ivan: 现在已经有公司是这样做的了，就直接outbound，只要输入原始数据，

370
00:42:13.010 --> 00:42:23.609
Ivan: 数据会经过清洗的pipeline，然后就pipeline做得很好，然后就人工干预的比较比较少，人，就你只需要去monitor它就可以了。

371
00:42:25.340 --> 00:42:36.849
Fan Yunjie: 那未来的话会不会有这样的一个角色去，这样就在团队内部就固定有这样的一个人去做这个事情。

372
00:42:40.690 --> 00:42:47.459
Ivan: 这个不是很清楚，因为理论上讲，我们会要求所有的人都

373
00:42:48.430 --> 00:42:56.459
Ivan: 都理解都会使用之上，你可能不完全理解所有的底层，但是你会使用，然后你会

374
00:42:57.450 --> 00:43:07.520
Ivan: 对它，它它会就是谁用谁就en

375
00:43:10.840 --> 00:43:19.339
Ivan: 对他。可能会有，有的人会比较资深，有的人不资深，那资深的人就可能会去

376
00:43:19.780 --> 00:43:22.970
Ivan: get的这些不资深的人对。

377
00:43:24.330 --> 00:43:25.630
Fan Yunjie: 但是。

378
00:43:26.040 --> 00:43:28.030
Ivan: 然后可能会有一些。

379
00:43:28.530 --> 00:43:36.750
Fan Yunjie: 但我觉得这个其实跟跟这个公司或者跟团队的这个共识有一定的相关性吧，对吧？

380
00:43:36.810 --> 00:43:41.730
Fan Yunjie: 如果说这公司的共识或者团队共识。就是啊，ai是一个很

381
00:43:41.730 --> 00:43:58.239
Fan Yunjie: 很强大的一个一个执行单元，然后可能每一个人都要有一些master能力，有一些这个模型假设的。这些判断性有一些如何去调用配合ai的这种协作能力的话，那可能被动性的他就要去去

382
00:43:58.460 --> 00:44:01.520
Fan Yunjie: 驱使他去去进行这个事情吧。

383
00:44:11.220 --> 00:44:24.350
Ivan: 对对也是吧。就是在最开始的决策者是需要对ai有一个相信

384
00:44:25.130 --> 00:44:28.979
Ivan: 那对ai工具有一个相信，那

385
00:44:31.990 --> 00:44:38.720
Ivan: 为什么他在这个行业里发展比较滞后，就是因为大家不相信这些。

386
00:44:39.740 --> 00:44:43.500
Ivan: 这些决策者都是比较老派的决策者，他们不相信。

387
00:44:44.820 --> 00:44:49.280
Ivan: 所以就发展比较滞后，但是后来有一些比较新派的人，

388
00:44:49.840 --> 00:44:56.110
Ivan: 他们开始做了一些尝试和努力，投了一些钱在里面，然后并且做做成了，

389
00:44:56.710 --> 00:45:02.440
Ivan: 然后并且非常成功，所以其他公司看到之后也开始效仿

390
00:45:02.750 --> 00:45:04.470
Ivan: 就是它是有一个过程。

391
00:45:05.790 --> 00:45:13.079
Ivan: 对，然后至于这些program什么的就是在这个行业里面，这些

392
00:45:13.370 --> 00:45:19.940
Ivan: 其实都不是twitter关心的事情。Trader只关心一件事情就是怎么能赚到钱。

393
00:45:21.670 --> 00:45:31.199
Ivan: 然后。然后而这些其实都是从公司级别，他需要公司需要比较有比较比较有远见，

394
00:45:31.510 --> 00:45:39.380
Ivan: 然后他会去做各种各样的program会给你介绍各种各种各样的工具，然后让你去尝试使用。

395
00:45:39.570 --> 00:45:44.319
Ivan: 那如果有一些team，他不愿意尝试，他还比较老派，他就

396
00:45:44.430 --> 00:45:49.430
Ivan: 他也还在赚钱，只有他开始不赚钱的时候，他可能才会想

397
00:45:49.540 --> 00:45:54.400
Ivan: 要尝试一些新的东西，当它还在赚钱的时候，它可能就不会想。

398
00:45:55.870 --> 00:46:08.679
Ivan: 然后还有就是就是你刚刚说，即便是用这些new network非常牛逼的，非常高端的做出来预测非常准的这些交易信号。

399
00:46:09.240 --> 00:46:10.850
Ivan: 交易策略，

400
00:46:10.980 --> 00:46:17.829
Ivan: 即便是这有这些，但是在金融市场，它都有一个自适应性，就是它会

401
00:46:18.580 --> 00:46:24.209
Ivan: 你所有产生的marketing都会被市场自己适应和修复。所以

402
00:46:24.550 --> 00:46:36.570
Ivan: 你所有的这些交易信号都会都不会一直有效，它会有效一段时间就不再有效了。所以你要一直在寻找新的交易信号，

403
00:46:36.980 --> 00:46:38.729
Ivan: 然后当这个。

404
00:46:39.770 --> 00:46:44.949
Ivan: 所以对于ai来说，它也是要在不断学习，不断更新的一个过程，

405
00:46:46.790 --> 00:46:57.310
Ivan: 所以然后，然后。然后在这个过程中，然后人也在不断的从ai学习，然后也让ai从自己身上学习，就给ai一些guidance。

406
00:46:57.880 --> 00:47:07.409
Fan Yunjie: 了解，所以本身其实ai模型，它你越训练它可能预测性越高，然后可能就越在我理解，在black box这样的。

407
00:47:07.840 --> 00:47:09.549
Ivan: 不一定，预算限更高。

408
00:47:09.840 --> 00:47:10.359
Fan Yunjie: 不一定。

409
00:47:10.360 --> 00:47:11.270
Ivan: 规量。

410
00:47:11.570 --> 00:47:21.959
Ivan: 对啊，因为它本身这个金融市场不是一个非常可预测的。

411
00:47:21.960 --> 00:47:23.380
Fan Yunjie: 我们看一下，

412
00:47:23.380 --> 00:47:31.649
Ivan: 不是特别可预测你能捕捉到一点点可预测的东西，可能就可以赚钱。然后当你捕捉到之后，

413
00:47:31.930 --> 00:47:35.869
Ivan: 你开始赚钱之后，你捕捉到的其实是

414
00:47:36.070 --> 00:47:47.670
Ivan: 你。理论上讲市场如果是非常有效的话，你是没有这些机会的，它完全是一个随机的过程，那随机的话，你就看病，你用什么模型都没有用，就是看病，

415
00:47:48.050 --> 00:47:57.080
Ivan: 但是实际上市场不是有效，不是完全有效的，会有一些只有一些，有一些市场会比较有效，那他可能

416
00:47:57.800 --> 00:48:05.770
Ivan: 你只有找到这些不有效的点，这才能赚钱，你才能去。

417
00:48:06.410 --> 00:48:13.219
Ivan: 比如说套利套利，就是因为没有市场出现了价格的偏差，你才能套利，

418
00:48:13.820 --> 00:48:16.410
Ivan: 那到底价格本来是不应该会偏差的，

419
00:48:17.820 --> 00:48:28.250
Ivan: 对吧。然后你每做一次这样的交易，市场就会就会修复一下市场，就让市场变得更有效了。

420
00:48:28.660 --> 00:48:34.080
Ivan: 所以你即使用ai做，你现在做出一个很厉害的预测，

421
00:48:34.740 --> 00:48:42.649
Ivan: 然后你使用了赚了很多钱，但是市场很聪明，他根据这个

422
00:48:43.130 --> 00:48:49.030
Ivan: 你的这些交易产生的market impact，然后他就自我修复了，

423
00:48:49.280 --> 00:48:54.910
Ivan: 然后你之后可能用这个就不赚钱了，除非这个东西它是一个

424
00:48:55.230 --> 00:49:02.899
Ivan: 注定一定永远都会存在的，就像套利永远都会存在，所以

425
00:49:03.850 --> 00:49:07.860
Ivan: 有一些套利永远都会存在，所以那你可能

426
00:49:08.010 --> 00:49:10.279
Ivan: 用这种方法永远都可以赚钱。

427
00:49:12.460 --> 00:49:13.390
Fan Yunjie: 那。

428
00:49:13.570 --> 00:49:15.870
Ivan: 当然这样的机会也会越来越少。

429
00:49:17.350 --> 00:49:22.929
Fan Yunjie: 那针对这种工具型的，这种使用你们更大程度上是追求它的效能还是它的

430
00:49:23.150 --> 00:49:26.150
Fan Yunjie: like。It's plan abandoned。这样子。

431
00:49:32.370 --> 00:49:38.299
Ivan: 这两个我觉得都特别重要。然后，但是目前看来。

432
00:49:40.350 --> 00:49:41.180
Fan Yunjie: Status.

433
00:49:43.240 --> 00:49:53.160
Ivan: 目前看来，可能大家还是已经被ai convinced了，所以就会更注重效能对，因为

434
00:49:53.320 --> 00:49:58.449
Ivan: 最开始使不使用它的原因就是因为它可解释性太差了。但是

435
00:49:59.030 --> 00:50:05.420
Ivan: 现在大家一开始使用，是因为虽然可解释性差，但是能赚很多钱啊，所以

436
00:50:05.770 --> 00:50:13.969
Ivan: 对吧，那有钱之后为什么不不尝试呢？所以大家就就又都开始对他保持这个开放的态度了。

437
00:50:15.000 --> 00:50:17.660
Ivan: 所以，对啊。

438
00:50:18.010 --> 00:50:19.750
Fan Yunjie: 如果一种比像

439
00:50:19.920 --> 00:50:29.830
Fan Yunjie: 更复杂的，或者说更加成本高的一种模型出现，但是它会它会明确的提升效益，但却很难解释的时候，但也是会

440
00:50:30.200 --> 00:50:34.389
Fan Yunjie: 就是大，你们会，你们会优先考虑哪去使用吗？

441
00:50:34.390 --> 00:50:44.590
Ivan: 这个永远是赚钱第一位啊，这个行业就是这样啊，就是你因为对吧？大家

442
00:50:45.040 --> 00:50:48.210
Ivan: 都是有压赚钱的压力的，这个。

443
00:50:49.670 --> 00:50:52.680
Ivan: 你肯定是先赚了钱再说嘛。

444
00:50:54.650 --> 00:51:00.390
Fan Yunjie: 那现在最除了本身的解释性以外，最担忧的点是什么？安全性。

445
00:51:03.300 --> 00:51:11.480
Ivan: 可解释性，可解释性差的可解释性是一个问题，最主要的原因是因为

446
00:51:12.240 --> 00:51:17.900
Ivan: 风险，你没法解释它，你就不懂它的风险在哪里，

447
00:51:19.460 --> 00:51:32.499
Ivan: 而风险导致的结果就是你有很有你，有可能会赔很多钱。如果你清楚你在什么时候会赔很多钱，这也不算是一种风险，因为你已经知道了。

448
00:51:32.800 --> 00:51:42.010
Ivan: 但是现在问题是你不知道你什么时候会赔钱，那这种就就很可怕，

449
00:51:42.430 --> 00:51:50.449
Ivan: 就你可能今天你每天都转，就像导演公司的经典经典模型一样，就是你

450
00:51:50.720 --> 00:51:57.219
Ivan: 每天都在赚一点点小钱，然后突然你就赔一个大的，把之前都赔进去了

451
00:51:57.880 --> 00:52:07.499
Ivan: 或者赔更多，那你就你肯定是不希望这种事情发生，所以在这个方面

452
00:52:08.520 --> 00:52:12.369
Ivan: 就是可解释性。当然这个是是

453
00:52:12.820 --> 00:52:23.139
Ivan: 一方面，就是风险的考量，然后那现在很多公司在使用，在尝试使用它，就从其他的用其他的方法去

454
00:52:23.280 --> 00:52:25.020
Ivan: 尽量减少风险。

455
00:52:26.260 --> 00:52:32.440
Fan Yunjie: 怎么样的，比方说它既定的去使用ai的时候，它它怎么去like。

456
00:52:32.440 --> 00:52:44.520
Ivan: 比如说就是ai指有一部分ai，然后剩下还是用传统的方法，然后，然后就

457
00:52:44.830 --> 00:52:49.800
Ivan: 给给给一个小的给ai做出来的信号，一个比较小一点的权重。

458
00:52:52.150 --> 00:52:57.830
Ivan: 比如说对，然后当他稳定了，然后再逐渐增加权重，

459
00:52:58.550 --> 00:53:06.159
Ivan: 然后根据历史上实盘交易的结果来分析

460
00:53:06.280 --> 00:53:14.149
Ivan: 看，它就是你还是会收集数据的嘛？你会收集你的这个交易数据的嘛，你根据它交易数据

461
00:53:14.300 --> 00:53:24.120
Ivan: 来看他，你的，你的赔钱和亏钱是分布是什么样的，然后去理解它，

462
00:53:25.840 --> 00:53:28.129
Ivan: 然后再决定用不用他。

463
00:53:29.670 --> 00:53:36.730
Fan Yunjie: 所以其实在你的行业里面，大家完全都不会有一种。某一个就

464
00:53:36.970 --> 00:53:45.890
Fan Yunjie: senior的职能，或者说某一个中间力量的职能可能会被ait带，就只是存在一些比较functional的一些职能会被ait带的吧。

465
00:53:45.890 --> 00:53:51.860
Ivan: 对啊，对，就是职位应该是不会。目前看来，

466
00:53:52.350 --> 00:53:59.209
Ivan: 短期之内应该是不会被替代，可能至少未来五年，我觉得应该还不会被替代，

467
00:54:00.030 --> 00:54:08.490
Ivan: 然后的职位就是非常critical的这种development

468
00:54:09.010 --> 00:54:13.279
Ivan: 做非常critical development的这些人，他们不会被替代。

469
00:54:14.650 --> 00:54:21.299
Ivan: 然后做一些比较边角料一些的开发，可能这些就会被ai替代。对。

470
00:54:23.080 --> 00:54:26.310
Fan Yunjie: 你觉得应该本身五到十年之内会

471
00:54:26.460 --> 00:54:37.950
Fan Yunjie: 对你现在所了解的行，这些机构也好，或者说这些平台也好，带来一些颠覆性的改变吗？还是说大家还是在自适应的一个状态。

472
00:54:41.050 --> 00:54:47.709
Ivan: 就是咳，无论ai怎么发展，这个目前看来

473
00:54:48.190 --> 00:54:56.510
Ivan: 他发展是确实很快啊，超过了很，就是很多人的认知，或者

474
00:54:56.660 --> 00:55:05.620
Ivan: 就超过了很多人的预期，甚至超过了很多人的认知。但是咳，我觉得

475
00:55:06.390 --> 00:55:17.209
Ivan: 真正掌控这个ai的这些人，他会，它会建，它会让ai被掌控，

476
00:55:20.050 --> 00:55:23.520
Ivan: 然后在整个行业里面

477
00:55:23.900 --> 00:55:34.789
Ivan: 这些大的合伙人，各个知名公司的大的合伙人，他最主要的一点，还是要他要掌控他。下面的人能够不断的帮他赚钱，

478
00:55:35.520 --> 00:55:40.969
Ivan: 无论是用ai也好，还是用人工也好，它

479
00:55:41.850 --> 00:55:47.769
Ivan: 就是肯定是希望这个自己可以有全全局的掌控。

480
00:55:49.380 --> 00:55:50.920
Fan Yunjie: 所以它不会。

481
00:55:50.920 --> 00:55:55.249
Ivan: 他不会选择他掌控不了的ai工具，

482
00:55:56.570 --> 00:56:02.710
Ivan: 因为就是他有一个风险在里面，如果他完全掌控不了的话，那

483
00:56:03.160 --> 00:56:14.610
Ivan: 他们就没法知道他他投入这些钱进去是不是一定会稳定的赚钱，他可能今天今年赚个赚很多钱，明年就都都赔没了，那也有可能，

484
00:56:14.720 --> 00:56:21.459
Ivan: 所以他他，但是实际上他们需要的是每年都稳定的赚钱，而

485
00:56:21.570 --> 00:56:29.620
Ivan: 哪怕今年少赚一点，但是就是很稳，他很确定会赚这些钱，对吧？

486
00:56:29.940 --> 00:56:36.310
Ivan: 那，所以他肯定只会选择他能完全掌控的。

487
00:56:38.520 --> 00:56:44.080
Fan Yunjie: 所以我觉得对于整个行业来说，它只会使有钱的人。

488
00:56:44.120 --> 00:56:47.889
Ivan: 更有钱，因为他们是真正实际上在掌控

489
00:56:48.470 --> 00:56:52.699
Ivan: 整个游戏整个市场整个行业的，人

490
00:56:53.570 --> 00:56:58.919
Ivan: 然后进入这个行业的人会进入这个行业会越来越难，因为

491
00:56:59.180 --> 00:57:08.789
Ivan: 有很多职位不再需要了，你没有机会从一个比较不是特别重要的角色开始做，然后慢慢转到一个

492
00:57:09.340 --> 00:57:17.720
Ivan: 更重要的角色，这种机会会越来越少，因为这些不重要的工作会被ait的

493
00:57:17.990 --> 00:57:25.719
Ivan: 你不可以就你更难进入行，就有很多时候是你先进入这个行业，无论你做点什么，然后你再

494
00:57:25.880 --> 00:57:37.300
Ivan: 你再考虑去转转换角色从一个不是很重要的角色转换成更越来越重要的角色。然后这也当然也是一个不断学习的过程，但是以后这种机会就会越来越少，

495
00:57:37.490 --> 00:57:44.850
Ivan: 你就只能是本身就很厉害，然后你一次就进进来做一个这种

496
00:57:45.310 --> 00:57:46.710
Ivan: 关键的岗位。

497
00:57:51.110 --> 00:57:58.159
Ivan: 所以对这个job market我觉得会有很大的影响，但是对于各个公司来说，

498
00:57:59.340 --> 00:58:01.249
Ivan: 我不觉得他就

499
00:58:02.170 --> 00:58:10.340
Ivan: 就是有有一些公司可能会把其他公司给挤挤掉，因为它做得很成功，其他公司它是灵活游戏嘛。所以

500
00:58:10.600 --> 00:58:15.680
Ivan: 你其他公司你赚钱，其他人就不赚钱了，那有一些公司可能就不赚钱就不行了，

501
00:58:17.100 --> 00:58:23.870
Ivan: 然后它就会变得更加那么几家公司可能会主导这个行业。

502
00:58:26.100 --> 00:58:27.750
Fan Yunjie: 那你觉得他最

503
00:58:28.010 --> 00:58:39.500
Fan Yunjie: 最值得突破的是什么环节啊？是它目前还是预测呢？还是仍然在有效性上，可还是你刚刚提到的解释性上，还是说本身，

504
00:58:39.840 --> 00:58:48.089
Fan Yunjie: 如果说监管它，不去管你它的可解释性的话，只管你挣不赚钱，

505
00:58:48.200 --> 00:58:52.879
Fan Yunjie: 是不是变得意见很多呀，有没有这个可能。

506
00:58:54.820 --> 00:58:56.029
Ivan: 监管是谁。

507
00:58:56.510 --> 00:59:04.629
Fan Yunjie: 就是你，不论是你们的行业要求啊，还是说任何，因为你们只管赚不赚钱啊。在我理解为什么一定要让他可解释呢？

508
00:59:04.630 --> 00:59:05.260
Ivan: 对啊。

509
00:59:05.810 --> 00:59:07.229
Fan Yunjie: 如果说他就是赚钱。

510
00:59:07.230 --> 00:59:12.280
Ivan: 因为你只有理解了，你才能够知道它是不是会一直赚钱，对

511
00:59:13.500 --> 00:59:21.049
Ivan: 你是要对他有一个理解，然后最主要是你要知道为什么赚钱？对，

512
00:59:24.200 --> 00:59:25.389
Ivan: 因为就。

513
00:59:25.390 --> 00:59:25.830
Fan Yunjie: 总是不是。

514
00:59:25.830 --> 00:59:31.969
Ivan: 很难讲，就是你要知道你要知道你赚钱，但是为什么赚钱，这不是一个，

515
00:59:34.570 --> 00:59:51.520
Ivan: 有的时候是有一点你就是做这个行业不能依依赖自己的幸运，因为运气是依赖不住的，不确定你可能幸运，你可能不幸运，就是不确定，所以你要你要能一直持续赚钱，本质还是

516
00:59:51.640 --> 00:59:58.549
Ivan: 你要去，就是真正理解，理解市场，理解你的模型，理解你的，

517
00:59:58.980 --> 01:00:01.649
Ivan: 你为什么赚钱，你赚了谁的钱，

518
01:00:03.950 --> 01:00:12.320
Ivan: 你才能够你才能够一直赚钱，那你ai，ai现在他是不错不错，

519
01:00:12.510 --> 01:00:28.680
Ivan: 那那些做做起来的公司用ai模型做起来的公司，他也其实不是完全不理解他做了很多努力去理解这些模型，只不过他不如他，他可能不如他像理解这些传统的方法，像线性回归。

520
01:00:28.720 --> 01:00:34.319
Ivan: 还有这些huntc feature那么透彻，但是它也

521
01:00:34.700 --> 01:00:40.940
Ivan: 尽最大能力去理解这个模型，然后理解为什么赚钱，不然他们也不会

522
01:00:41.170 --> 01:00:43.709
Ivan: 就是很成功。

523
01:00:45.510 --> 01:00:47.050
Fan Yunjie: 好的。Ok，我，我。

524
01:00:47.050 --> 01:00:47.429
Ivan: 对呀。

525
01:00:47.430 --> 01:00:59.380
Fan Yunjie: 我会有个问题吧。我觉得如果说现在新人的话，你，你会给他什么建议在ai使用上面，或者说是你会如果说给他一个建议，在他自己本身去

526
01:01:00.220 --> 01:01:04.520
Fan Yunjie: 去去去培训自己的这个情况之下，让他具备什么技能。

527
01:01:10.550 --> 01:01:21.640
Ivan: 咳，就自己培养自己的话，就你要对这个市市面上比较主流的这些

528
01:01:22.050 --> 01:01:25.259
Ivan: 神经网络，什么要有比较

529
01:01:26.010 --> 01:01:30.959
Ivan: 比较细的人理解就理解的要比较透彻，比较细致，

530
01:01:31.980 --> 01:01:38.730
Ivan: 然后除了这些之外，更多还是要对数据有一些比较好的理解，

531
01:01:38.850 --> 01:01:41.299
Ivan: 想能做出一个从

532
01:01:41.620 --> 01:01:49.209
Ivan: input data，然后到最后是pipeline，这样的公司非常非常少。其实所以

533
01:01:49.350 --> 01:01:55.539
Ivan: 大多数还是比较ad hoc的这种方法就是自己做一个小模小一点的模型。

534
01:01:55.710 --> 01:01:58.280
Fan Yunjie: 然后去把它当成是一个。

535
01:01:58.280 --> 01:02:01.770
Ivan: 预测工具，那这种情况你

536
01:02:02.430 --> 01:02:08.469
Ivan: 对吧，就是你还是。而而且你实际上你还是需要去做一些手动做一些

537
01:02:08.860 --> 01:02:17.049
Ivan: 按coffee feature去给你的模型一些guidance，那就是这些能力，基本这些基本能力你还是需要具备的

538
01:02:18.440 --> 01:02:29.360
Ivan: 对，然后再就是就是就是不能有短板嘛，就是你要数学

539
01:02:30.570 --> 01:02:36.970
Ivan: 就是相关的数学分支要非常了解，然后

540
01:02:37.110 --> 01:02:43.339
Ivan: 然后你的工程能力，你的写代码能力要足够快因为其实你用像changept这种

541
01:02:43.920 --> 01:02:51.790
Ivan: 这种工具，目前看来它已经越变得越来越好，就是写的代码越质量越来越高。但是，

542
01:02:52.730 --> 01:03:04.440
Ivan: 就是很多时候还还是没有一个非常水平非常高的developer写的

543
01:03:04.730 --> 01:03:12.560
Ivan: 写得快和好，就是因为你像changept的话，你可能给他一些非常细的

544
01:03:13.040 --> 01:03:15.869
Ivan: 给他写一些非常细致的任务，

545
01:03:16.070 --> 01:03:24.129
Ivan: 然后让他去根据你的要求去做一套代码。当然，你写完之后，你需要看一下这个代码，对吧？看完之后你发现，哎，

546
01:03:24.550 --> 01:03:28.710
Ivan: 这个代码好像和你想要的还是有一些区别。

547
01:03:29.840 --> 01:03:33.160
Ivan: 不完全一样，所以你就需要再

548
01:03:33.440 --> 01:03:45.079
Ivan: 告诉他哪里不一样，然后让他再去修，然后这样迭代。但如果是一个水平本身就很高，他不需要去做这个迭代，他就可能一次就写完了。

549
01:03:46.080 --> 01:03:53.800
Ivan: 对，就是还是这样的人还是挺多的，所以就目前，然后

550
01:03:54.400 --> 01:04:01.940
Ivan: 就是你这个工程能力并并不并不应该因为ai的存在而受到影响，就是

551
01:04:02.520 --> 01:04:06.310
Ivan: 自己的水技能还是要打磨得更细致一些。

552
01:04:07.640 --> 01:04:09.200
Ivan: 要更全面一些。

553
01:04:11.520 --> 01:04:15.739
Fan Yunjie: 那你在工作以外你还会哇，

554
01:04:16.320 --> 01:04:19.570
Fan Yunjie: 你在工作以外，你还会用ai吗？

555
01:04:20.100 --> 01:04:23.669
Ivan: 那肯定会啊，那我不知道的东西太多了。

556
01:04:24.050 --> 01:04:28.050
Ivan: Ai虽然并不讲的并不都是对的，但是

557
01:04:28.360 --> 01:04:35.160
Ivan: 就是准确率还是比较高的，那我有很多不懂的事情，我会问他，就比如说

558
01:04:35.260 --> 01:04:37.399
Ivan: 我对法律就不是很了解。

559
01:04:38.410 --> 01:04:40.819
Fan Yunjie: 然后法律就没有问题啊。

560
01:04:40.820 --> 01:04:48.010
Ivan: 那我有可能就会问一下他，对吧？或者我对这个如何。

561
01:04:48.530 --> 01:04:49.950
Fan Yunjie: 如何，他都可以。

562
01:04:49.950 --> 01:04:52.659
Ivan: 健康就不是很了解。

563
01:04:52.660 --> 01:04:54.030
Fan Yunjie: Pay the drove.

564
01:04:55.010 --> 01:04:57.669
Ivan: 对啊，就很多这些不了解的。

565
01:04:58.120 --> 01:05:13.890
Fan Yunjie: 我跟你说我，我发现了一个很有趣的一个事情，就我在我访问这这一我一开始会访问几个phd，但后来我就不喜欢跟他们讲话，我就是把这个事，把这个工作丢给了别人。然后呢，

566
01:05:14.430 --> 01:05:22.749
Fan Yunjie: 我发现他们本身会有一个很大的爱好，就是问那个ai怎么去跟别人沟通，我觉得这个很有趣，对

567
01:05:23.720 --> 01:05:25.589
Fan Yunjie: 你会有这个爱好吗？

568
01:05:26.020 --> 01:05:27.759
Ivan: 我没有这个爱好。

569
01:05:28.610 --> 01:05:30.989
Fan Yunjie: 但是我觉得这个倒是一个

570
01:05:31.670 --> 01:05:42.409
Fan Yunjie: 很有趣的点，他们会问，ai我怎么样，去回下一句话，这样子把那个聊天截图发给那个ai，请问我下一句要怎么回这样？

571
01:05:45.940 --> 01:05:46.640
Fan Yunjie: 然后还有。

572
01:05:46.640 --> 01:05:52.370
Ivan: 这个我不太这个，我觉得他是问中文还是问英文。

573
01:05:53.500 --> 01:06:03.140
Fan Yunjie: 问，中文也好，英文也好不在于说他的语言不行，他思路没有打通。懂我的意思吗？他不知道怎么回复了，

574
01:06:05.560 --> 01:06:12.450
Ivan: 我不太会，因为我觉得对，就是

575
01:06:14.790 --> 01:06:18.329
Ivan: 就是ai生成的，这种回复还是挺ai的。

576
01:06:21.010 --> 01:06:21.500
Fan Yunjie: 哎，我们。

577
01:06:21.500 --> 01:06:24.430
Ivan: 他的思思维方式比较ai。

578
01:06:25.110 --> 01:06:33.259
Fan Yunjie: 没有我看他们的沟通记录，那个ai有一个就是帮他去聊那个dating app的对象

579
01:06:33.510 --> 01:06:36.689
Fan Yunjie: 就聊得真的很好，聊得真的很。

580
01:06:39.600 --> 01:06:42.780
Fan Yunjie: 那这种我觉得对这种。

581
01:06:42.780 --> 01:06:48.170
Ivan: 那也挺好的呀，就我觉得对

582
01:06:48.330 --> 01:06:51.399
Fan Yunjie: 你在尝试使用吗？如果说你的话。

583
01:06:54.730 --> 01:07:02.329
Ivan: 应该不会吧？因为我觉得如果我要跟人聊天，就如果是business的原因有可能会，

584
01:07:02.880 --> 01:07:08.240
Ivan: 但我也不需要和人打交道，所以没有这样需求。

585
01:07:08.620 --> 01:07:13.489
Fan Yunjie: 为什么不需要跟人打交道，你难道不需要跟别人过你的策略之类的吗？

586
01:07:15.240 --> 01:07:22.219
Ivan: 过策略，这种我们还需要跟需要，需要用ai告诉我吗？我，首先我策略就不能告诉ai。

587
01:07:24.160 --> 01:07:25.590
Ivan: 所以。

588
01:07:25.590 --> 01:07:32.109
Fan Yunjie: 你们会涉及到一些跟一些投资人之类的一些对话吗？这些。

589
01:07:32.110 --> 01:07:33.830
Ivan: 不设计，没有投资人对。

590
01:07:35.860 --> 01:07:39.379
Fan Yunjie: 没有任何的需要去的一些东西。

591
01:07:41.140 --> 01:07:42.670
Ivan: 有一些

592
01:07:43.000 --> 01:07:52.860
Ivan: 没有投资人这个，所以不需要。但是有一些business development的工作，就比如说要跟交易所打交道，那这种也不需要我去打交道，这种需

593
01:07:53.060 --> 01:07:59.609
Ivan: 是有专门的人去打交道，那他们用没用change vt ai工具，我就不知道。

594
01:08:00.290 --> 01:08:08.029
Fan Yunjie: 那你们的这个岗位如果再往上去到管理类的岗位的话，他们也没有说对外的需求吗？他们也没有。

595
01:08:08.030 --> 01:08:10.060
Ivan: 我们没有管理类的岗位。

596
01:08:10.490 --> 01:08:13.740
Fan Yunjie: 你们全部都是就是闷头干活的岗位。

597
01:08:14.970 --> 01:08:21.040
Ivan: 对就是所谓管理的岗位，就是

598
01:08:21.560 --> 01:08:23.639
Fan Yunjie: 个人类团的活。

599
01:08:23.640 --> 01:08:29.230
Ivan: 所谓所谓的管理的岗位，最我们做的就是最难的活，就是

600
01:08:30.040 --> 01:08:37.259
Ivan: 无论是这些开发人员还是trader，做的就是最难的活，因为最难的部分就是

601
01:08:37.510 --> 01:08:41.720
Ivan: 把钱赚进赚到公司的账里，这是最难的。

602
01:08:42.080 --> 01:08:50.900
Ivan: 然后所谓管我们的人就是基金经理，基金经理的话，他们本身也是trader，

603
01:08:51.640 --> 01:08:57.799
Ivan: 大多数的基因经理都是trader出身，如果不是一个trailer出身，基金经理，不会有人跟他跟他去做的，

604
01:08:58.240 --> 01:09:01.420
Ivan: 不会不会有有人愿意跟他去做。

605
01:09:02.490 --> 01:09:02.879
Fan Yunjie: 那他。

606
01:09:02.880 --> 01:09:03.890
Ivan: 它这个

607
01:09:04.170 --> 01:09:12.200
Ivan: 经营经理，经营经理，他就是跟个人风格有关，有一些经营经理，他会自己做很多策略，

608
01:09:12.819 --> 01:09:20.490
Ivan: 因为他他赚的钱他有一部分钱是team从team拿绩效，有一部分钱

609
01:09:20.750 --> 01:09:23.409
Ivan: 他也可以自己做策略，自己拿技巧，

610
01:09:24.140 --> 01:09:32.850
Ivan: 然后有有一些经营经历，他就属于更偏一点，就manage一个team，那他的工作也是，但是他的工作是

611
01:09:33.050 --> 01:09:41.620
Ivan: 首先他，他得了理解每个人做的东西，然后他主要是要去

612
01:09:41.850 --> 01:09:48.240
Ivan: 把握方向，然后并且分配资源，这他他可能

613
01:09:49.250 --> 01:09:56.549
Ivan: 主要在做这个，所以他其实是和这个team的人聊的比较多，然后就是

614
01:09:57.380 --> 01:10:04.270
Ivan: 他和这个外面的人去要资源，这个这个行业就是非常

615
01:10:04.550 --> 01:10:15.069
Ivan: 简单粗暴，就是你，你background怎么样？你background好你的trading record。好，你，你这个team之前赚了很多钱，

616
01:10:15.450 --> 01:10:21.779
Ivan: 那你的资源资源自然就就来了，不需要他去，要就能拿到。

617
01:10:23.600 --> 01:10:31.059
Ivan: 然后在他team赚钱之前，在最开始的时候可能需要一些，

618
01:10:31.220 --> 01:10:34.010
Ivan: 就是要去说服这些

619
01:10:34.510 --> 01:10:44.900
Ivan: 合伙人愿意分钱，愿意建这个team相信你，并且建给你建一个team，那这个本身也是

620
01:10:45.610 --> 01:10:54.879
Ivan: 靠你过去的。在其他公司的经验做出来的成绩来看，然后这是最主要的。当然，你具体

621
01:10:55.560 --> 01:11:06.320
Ivan: 也需要去convince这些人，但你convince这些人，他们并不会因为你说话说得好听就convince你要说有干货才行。这些合伙人也不是傻子。

622
01:11:06.790 --> 01:11:10.589
Fan Yunjie: 对吧？他，你这个我们就基本上。

623
01:11:10.640 --> 01:11:16.380
Ivan: 你面试，大家听一听就知道这个人懂不懂懂多少，所以

624
01:11:16.720 --> 01:11:22.859
Ivan: 就是没有那么多需要ai去帮我们做的，就是

625
01:11:23.550 --> 01:11:33.800
Ivan: 因为他他他们会问得非常细，然后问的这些问的非常细，都是经验，都是来自于经验，然后你ai只能给一个比较

626
01:11:34.030 --> 01:11:41.330
Ivan: generic的答案，不能因为他没有做一些东西，他没有自己去做，而且他也没有人

627
01:11:41.500 --> 01:11:43.910
Ivan: 会愿意去把这些数据

628
01:11:44.480 --> 01:11:52.759
Ivan: 分享到这个去，用来去训练ai，因为这些都是他们赚钱的吃饭的

629
01:11:53.010 --> 01:12:00.970
Ivan: 吃饭的工具，他们肯定不会把自己吃饭的工具叫就提交给这个成为公共福利，对吧？

630
01:12:01.730 --> 01:12:05.350
Ivan: 所以他ai他看不到这些数据，他就没法学这些东西。

631
01:12:05.950 --> 01:12:10.479
Fan Yunjie: 那我觉得你们这个工作还真的很适合博士来做，

632
01:12:10.620 --> 01:12:17.580
Fan Yunjie: 因为我觉得我身边的博士，他们最大的一个工作壁垒就在于他们可能对外counication这个方面。

633
01:12:18.040 --> 01:12:30.459
Fan Yunjie: 我不知道为什么他们那么抗拒这个事情。然后呢，有一些岗位做到一定程度之上又不会存在这个绝对的问题，所以他们的天花板也很受限，

634
01:12:30.910 --> 01:12:43.980
Fan Yunjie: 但是这个蛮好啊，你们这个都不需要说有多么强的，这个就把自己的板短板完全可以去规避掉。当然，也许或许对于你来说不是短板，我也不知道

635
01:12:44.480 --> 01:12:48.959
Fan Yunjie: ok，anyway。我觉得都差不多了，问的差不多了。

636
01:12:49.650 --> 01:12:50.380
Ivan: En.

637
01:12:50.380 --> 01:12:54.840
Fan Yunjie: We'll stop recording啊。

