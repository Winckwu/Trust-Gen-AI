# The Trust-Use Paradox in Generative AI:
# How Informed Distrust Drives Deeper Engagement

---

## Introduction

The rapid proliferation of generative artificial intelligence (GenAI) tools has fundamentally transformed professional work across industries ranging from software development and healthcare to education, financial services, and creative industries (Brynjolfsson et al. 2023; Dell'Acqua et al. 2023). As these technologies become increasingly embedded in daily work practices, understanding how users develop and calibrate their trust in GenAI emerges as a critical research priority. Yet as adoption has surged, a puzzling phenomenon has emerged that challenges conventional assumptions about the relationship between trust and technology use.

A landmark global study surveying over 48,000 individuals across 47 countries reveals the scope of this puzzle: while 66% of people regularly use AI tools, less than half (46%) report being willing to trust them—a figure that has actually *declined* since the pre-ChatGPT era despite massive increases in adoption (Gillespie et al. 2025). This pattern echoes findings from the software development domain, where AI coding tool usage surged from 49% to 85% between 2024 and 2025, while developer trust in AI accuracy simultaneously plummeted from 40% to just 29% (ByteIota 2025). These findings suggest that the relationship between trust and use in GenAI contexts may be fundamentally different from what prior technology adoption theories would predict.

Traditional models of trust in automation assume that trust and reliance move in tandem—higher trust leads to greater use, while negative experiences erode both trust and subsequent engagement (Lee and See 2004; Parasuraman and Riley 1997). The Technology Acceptance Model and its extensions similarly position trust as an antecedent to behavioral intention and actual use (Davis 1989; Gefen et al. 2003). However, these frameworks struggle to explain the emerging "trust paradox" in AI adoption, wherein users continue—and even intensify—their engagement with systems they explicitly distrust (Glikson and Woolley 2020; Choung et al. 2023).

This apparent contradiction raises important theoretical and practical questions: Why do users who report declining trust in GenAI simultaneously increase their usage? What psychological and strategic mechanisms enable users to navigate this tension? And what does this pattern reveal about the nature of human-AI collaboration in knowledge work? These questions carry significant implications for both IS theory and practice, as they challenge foundational assumptions about trust-mediated technology adoption and suggest the need for new frameworks to understand human-GenAI interaction.

To address these questions, we conducted an in-depth qualitative investigation of 49 young professionals who regularly use GenAI tools for work-related purposes. Through semi-structured interviews spanning diverse industries and use contexts, we traced participants' trust trajectories over periods ranging from two to five years of GenAI use. Our analysis reveals three interconnected paradoxes that characterize expert GenAI use: (1) **Trust-Use Decoupling**, where declining trust coexists with—and may even drive—increased engagement; (2) the **Skill Anxiety-Dependence Loop**, where users who fear AI-induced skill degradation become increasingly reliant on these tools; and (3) **Sycophancy-Skepticism Tension**, where users simultaneously desire AI validation while distrusting systems that provide it too readily.

We argue that these paradoxes should not be interpreted as symptoms of dysfunction or cognitive dissonance, but rather as manifestations of *epistemic recalibration*—a sophisticated process through which experienced users develop nuanced, context-sensitive relationships with GenAI. Drawing on the concept of appropriate reliance from human factors research (Lee and See 2004; Schemmer et al. 2023), we propose that what appears as "distrust" may actually represent *informed skepticism*—a mature stance that enables more effective human-AI collaboration. Users who exhibit declining trust scores are often the most sophisticated in their engagement strategies, having developed detailed mental models of when to rely on AI outputs and when to exercise independent judgment.

This study makes five primary contributions to the IS literature. First, we empirically document the **Trust-Use Paradox** in GenAI adoption, demonstrating through rich qualitative evidence that trust and use can—and frequently do—move in opposite directions among experienced users. Second, we introduce the concept of **Trust Ceiling**—a psychological upper bound on trust that users deliberately maintain regardless of positive experiences. Third, we identify the **Skill Anxiety-Dependence Loop**, revealing how awareness of AI-induced skill degradation paradoxically coexists with continued heavy use. Fourth, we document the **Sycophancy-Skepticism Tension**, showing that sophisticated users interpret excessive AI agreement as a negative trust signal and wish AI would challenge their ideas. Fifth, we develop the concept of **Informed Distrust** as an integrative framework explaining how expert users achieve appropriate reliance through calibrated skepticism.

To ground these contributions in existing scholarship and identify the research gaps they address, the remainder of this paper proceeds as follows. We first review relevant literature on trust in automation, the emerging trust paradox in AI, and cognitive concerns related to GenAI use. We then present our research methodology, including data collection and analysis procedures. The findings section presents our three paradoxes with supporting evidence from participant interviews. We conclude with a discussion of theoretical and practical implications, limitations, and directions for future research.

---

*Word Count: ~750 words (~2.5 pages in ICIS format)*

## Key References for Introduction

- Brynjolfsson, E., Li, D., & Raymond, L. R. (2023). Generative AI at work. NBER Working Paper.
- Dell'Acqua, F., et al. (2023). Navigating the jagged technological frontier. Harvard Business School Working Paper.
- Gillespie, N., et al. (2025). Trust, attitudes and use of artificial intelligence: A global study. KPMG/Melbourne Business School.
- Lee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human Factors, 46(1), 50-80.
- Schemmer, M., et al. (2023). Appropriate reliance on AI advice. Proceedings of IUI '23.
- Glikson, E., & Woolley, A. W. (2020). Human trust in artificial intelligence. Academy of Management Annals, 14(2), 627-660.
- Choung, H., David, P., & Ross, A. (2023). Trust in AI and its role in the acceptance of AI technologies. International Journal of Human-Computer Interaction, 39(9), 1727-1739.
