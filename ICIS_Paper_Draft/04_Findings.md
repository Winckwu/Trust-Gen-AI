# Findings

Our analysis revealed three interconnected paradoxes characterizing how experienced users navigate trust in GenAI. We first present an overview of trust trajectories observed across the sample, then examine each paradox in depth.

## 4.1 Trust Trajectories: An Overview

Participants reported diverse patterns of trust evolution over their period of GenAI use. Using self-reported percentage estimates as reference points, we identified five distinct trajectory patterns summarized in Table 2.

**Table 2: Trust Trajectory Patterns**

| Pattern | Description | N | % | Typical Range |
|---------|-------------|---|---|---------------|
| **Rising** | Trust increased with use | 19 | 38.8% | 30-50% → 70-90% |
| **Declining** | Trust decreased with use | 8 | 16.3% | 50-75% → 30-40% |
| **Steady** | Trust remained relatively constant | 7 | 14.3% | 50-60% throughout |
| **Bell Curve** | Trust rose then fell | 5 | 10.2% | 20% → 80% → 20% |
| **Task-Divergent** | Different trajectories for different tasks | 10 | 20.4% | Varies by task |

While rising trust was the most common pattern, consistent with traditional technology adoption trajectories, the substantial presence of declining, bell curve, and task-divergent patterns challenges simplistic models. Moreover, as we elaborate below, even participants with declining trust often exhibited high or increasing use—a finding that demands explanation.

## 4.2 Paradox 1: Trust-Use Decoupling

The most striking finding from our analysis was the frequent disconnect between trust levels and engagement patterns. Counter to theoretical expectations, participants reporting the steepest trust declines were often among the most intensive users. This pattern manifested in several ways.

### 4.2.1 High Use Despite Low Trust

Several participants explicitly articulated their awareness that they continued relying on tools they did not trust. R46, a Master's student in Learning Science who built a custom chatbot interfacing with multiple AI models, exemplified this pattern:

> "When I first started using it, I would say [my trust was] around 50.... I seem to trust the AI less [now]... maybe around 30. I think. [The hallucination problem] is still quite prevalent. I found that 3 or 4 of [the journal articles it cited] do not exist." (R46)

Despite this trust decline, R46 reported daily use across multiple platforms and had invested significant effort in building custom tools—hardly the behavior of someone disengaging from the technology. When probed about this apparent contradiction, participants offered sophisticated reasoning:

> "I see it as a collaborative partner rather than a tool... I always verify, especially for citations. I never trust the citations it gives me." (R46)

This quote reveals that "low trust" may not mean rejection, but rather a recalibrated relationship where verification becomes integral to the workflow. Trust is not a precondition for use but a calibrated expectation that shapes *how* one uses.

### 4.2.2 The Trust Ceiling Phenomenon

A particularly notable finding was the emergence of what we term "Trust Ceiling"—a psychological upper bound on trust that users deliberately maintain regardless of positive experiences. R45, a biomedical optics researcher and postdoctoral fellow, articulated this clearly:

> "I will never completely be able to trust it. Maybe my cap will be around 75%... Initially, that will be around 30, 35%... I would say [now] around 60%." (R45)

Despite trust increasing from approximately 35% to 60%—a substantial positive trajectory—R45 explicitly bounded future trust growth. This ceiling was not based on specific negative experiences but on an epistemological stance about AI capabilities:

> "It does not have the intuition that you have... At the end of the day, I can't rely completely even today on this." (R45)

The Trust Ceiling represents a deliberate choice to maintain skepticism as a protective mechanism. Several participants described similar self-imposed limits, suggesting this may be a common adaptive strategy among sophisticated users.

### 4.2.3 Inverse Trust-Use Relationships

In some cases, declining trust appeared to *increase* rather than decrease engagement. R38, an AI researcher with deep domain expertise, described a trust trajectory that followed a striking bell curve pattern:

> "It follows like bell curve or something. Initially... around 20%. Then I think peak in 2024... maybe 80% or 90% even in the peak. Now I think it's like 20%... Because I'm quite involved in the field of AI... I read literatures or interviews that talk about limitation. I have better mental model on what kind of task I can assign it." (R38)

Paradoxically, this crash in reported trust coincided with more sophisticated engagement patterns. The participant explained that lower trust reflected *better understanding* rather than rejection—a distinction with important theoretical implications. R38 explicitly connected declining trust to increased expertise: knowing AI limitations enabled more effective use.

**Table 3: Trust-Use Decoupling Examples**

| Participant | Trust Trajectory | Use Pattern | Resolution Strategy |
|-------------|------------------|-------------|---------------------|
| R46 | 50% → 30% | Daily, multi-platform | Built custom validation tools |
| R45 | 35% → 60% (cap 75%) | Daily | Domain-specific selective use |
| R38 | 20% → 80% → 20% | Regular use | Refined mental model of capabilities |
| R11 | 50% → 30% (for coding) | High frequency | Task-specific trust calibration |

## 4.3 Paradox 2: The Skill Anxiety-Dependence Loop

The second paradox involves users who expressed significant concern about AI-induced skill degradation yet found themselves unable or unwilling to reduce their dependence. This creates a recursive loop: awareness of deskilling risks intensifies anxiety, but the productivity benefits that created the dependence in the first place make disengagement impractical.

### 4.3.1 Explicit Skill Degradation Awareness

Multiple participants spontaneously raised concerns about their own skill atrophy, often with visible discomfort. R47, a 45-year-old sales manager with a PhD in chemical engineering, described her realization with remarkable candor:

> "I am so scare about it. So I start going back to doing myself some stuff... I realize I was feeling scared of writing in English, this kind of things. I would be losing that ability to translate... in my mind." (R47)

This participant's response was notable for its embodied quality—she was not abstractly concerned about deskilling as a societal issue, but personally anxious about her own cognitive capabilities. Similarly, R48, an engineering undergraduate, acknowledged skill decline with striking directness:

> "I think that's the case where my writing skill, I would say my writing skill is not really that good. Cuz I, we can just use [ChatGPT] for most sort of stuff... Maybe I need to came up with a question... Maybe I can come up with one or two. But if I just ask [ChatGPT] to give me 10 or 20, they can just give [them]." (R48)

The phrase "chat ability is thinking for me" (R49) captured this dynamic succinctly—participants recognized that cognitive work was being displaced, not merely assisted.

### 4.3.2 Strategies for Managing the Loop

Participants developed varied strategies to manage the tension between productivity benefits and skill preservation concerns. Some, like R47, deliberately restricted AI use for certain tasks:

> "I am being better at identifying what things are not damaging my skills... I don't care if I lose that ability [for some things]. And in this other way, I do care. So I don't want to lose [those skills]." (R47)

This selective restriction represents a conscious attempt to preserve core competencies while still capturing productivity gains for less critical tasks. R47's language—"damaging my skills"—frames AI use in almost adversarial terms, as something requiring active defense rather than passive adoption.

Others found the loop difficult to escape. R49, a chemical engineering student, described escalating dependence despite awareness:

> "More heavy reliance on [ChatGPT]... Almost certain extent, I have to agree [I cannot live without it]... In some of the cases that I'm supposed to think more critically, right, then it becomes that chat ability is thinking for me." (R49)

The acknowledgment "I have to agree" suggests a somewhat reluctant acceptance of dependency—a recognition that the loop, once entered, is difficult to exit.

Some participants, however, described active attempts to break the loop. R38, an AI researcher who had experienced a dramatic trust decline, described deliberate efforts to reduce dependence:

> "I wean myself off this habit of depending on it... I try not to write with ChatGPT. My writing is entirely done by me... I think it's quite cognitively dangerous." (R38)

R38's use of addiction-like language ("wean myself off") is striking, suggesting that the loop can feel compulsive even to users with deep technical understanding of AI systems.

## 4.4 Paradox 3: Sycophancy-Skepticism Tension

The third paradox involves users' simultaneous desire for AI validation and distrust of AI systems that provide it too readily. This creates a double bind: AI agreement is suspect, but disagreement may indicate AI error rather than user error.

### 4.4.1 Frustration with AI Agreement

Several participants expressed frustration with AI systems that agreed too readily with user positions. R47 described a specific incident that eroded her trust:

> "I correct him and I say, no [the chemistry principle works differently]... And then he said like, 'oh, yes, you're right.' And then it completely change the answer... That was like six, seven months ago." (R47)

The immediate capitulation—without reasoning or defense of the original position—signaled to this participant that the AI lacked genuine understanding and would simply validate whatever the user asserted. This undermined the AI's value as a verification or feedback mechanism.

### 4.4.2 Desire for AI Pushback

Remarkably, several participants explicitly wished AI systems would disagree with them more often. R34, a computational chemistry postdoctoral researcher, articulated this most directly:

> "If it can argue with me, then I would have a higher trust." (R34)

This counterintuitive statement—that argumentation would *increase* trust—reveals sophisticated reasoning about what AI agreement actually signals. R34 recognized that an AI that consistently agrees provides no independent verification value. Similarly, R48 expressed:

> "They know how to... reject your idea. It's not like the user, the things user provide is always the best." (R48)

These statements, expressing wishes that AI would "argue" or "reject" user ideas, run counter to assumptions embedded in AI training toward helpfulness and user satisfaction. For sophisticated users, an AI that challenges their thinking may be more valuable than one that validates it—precisely because challenge signals independent reasoning capability.

### 4.4.3 Navigating the Double Bind

Users developed nuanced strategies to work around sycophancy concerns. These included:

- **Deliberate devil's advocacy prompting**: Instructing AI to argue against the user's position
- **Multiple model comparison**: Using different AI systems to surface disagreements
- **Treating agreement as non-informative**: Discounting AI validation while still using AI for generation

R48's observation that they would open new chat sessions when stuck—"Sometimes I might even just open a new chat again... If I try to use a new [chat], sometimes it might give me a fresh new idea"—represents another workaround. By resetting context, users could escape the conversational dynamics that might lead AI to converge on previous positions.

## 4.5 Integration: Informed Distrust as Adaptive Strategy

Across all three paradoxes, a common thread emerges: what appears as "distrust" or "declining trust" often represents not rejection but sophistication. Users with the lowest trust scores frequently demonstrated the most nuanced understanding of AI capabilities and the most strategic patterns of engagement. We term this stance "informed distrust"—a calibrated skepticism that enables rather than prevents effective human-AI collaboration.

Key characteristics of informed distrust include:

1. **Task-specific trust calibration**: Rather than global trust/distrust, sophisticated users develop differentiated expectations across task types
2. **Verification as workflow**: Validation becomes not an occasional check but an integral part of how AI is used
3. **Mental model refinement**: Declining trust often reflects improved understanding rather than disillusionment
4. **Protective skepticism**: Deliberately maintained distrust serves as defense against both AI errors and skill degradation

R49's advice captured this stance eloquently:

> "Use [GenAI] as a reference, not as the answer script itself. Definitely still need to go through your own level of thinking." (R49)

This formulation—AI as "reference" rather than "answer"—encapsulates the informed distrust stance that characterized many of our most sophisticated participants.

---

*Word Count: ~1,900 words (~5 pages in ICIS format)*
