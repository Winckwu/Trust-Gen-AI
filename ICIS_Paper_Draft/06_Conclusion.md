# Conclusion

This study investigated trust calibration patterns among 49 young professionals using Generative AI tools for work-related purposes. Through in-depth qualitative analysis, we uncovered three interconnected paradoxes that characterize expert GenAI engagement: Trust-Use Decoupling, where declining trust coexists with intensive use; the Skill Anxiety-Dependence Loop, where users continue relying on tools they fear are degrading their capabilities; and Sycophancy-Skepticism Tension, where users simultaneously desire and distrust AI validation.

Our central argument is that these paradoxes should not be interpreted as symptoms of dysfunction, irrationality, or failed adoption. Rather, they represent *epistemic maturation*—a sophisticated process through which experienced users develop calibrated, context-sensitive relationships with AI systems. What appears as "declining trust" often reflects refined understanding rather than disillusionment; what appears as continued use "despite" distrust actually represents informed engagement characterized by verification and selective application.

We introduced two theoretical constructs to capture these dynamics. The **Trust Ceiling** describes the deliberate upper bound that sophisticated users impose on their trust in AI, regardless of accumulated positive experiences—a form of protective skepticism that guards against both AI errors and excessive cognitive dependence. **Informed Distrust** characterizes a calibrated stance that enables rather than prevents effective human-AI collaboration, distinguishing mature users from both naive enthusiasts and wholesale skeptics.

These findings carry practical implications for AI system design, user training, and organizational policy. Rather than optimizing for trust maximization, designers might support healthy skepticism through features that normalize verification and occasional constructive disagreement. Training programs might validate declining trust as sophistication rather than treating it as failure. Organizations might recognize that optimal human-AI teaming may involve users who maintain deliberate distrust while still capturing significant productivity benefits.

The emergence of this paradoxical stance—intensive engagement mediated by informed distrust—may represent a defining characteristic of mature human-AI relationships in the GenAI era. As one participant (R49) advised: "Use [GenAI] as a reference, not as the answer script itself. Definitely still need to go through your own level of thinking." This formulation captures the essence of appropriate reliance in generative AI contexts: not blind trust or wholesale rejection, but discerning engagement that leverages AI capabilities while preserving human judgment.

As GenAI tools become ever more deeply embedded in knowledge work, understanding how users navigate the tensions between productivity, trust, and skill preservation becomes increasingly critical. Our findings suggest that the path to effective human-AI collaboration may run not through trust enhancement but through trust calibration—helping users develop accurate mental models, appropriate skepticism, and context-sensitive engagement strategies. The most effective GenAI users, it appears, are not those who trust AI most, but those who have learned precisely when and how much to trust.

---

*Word Count: ~450 words (~1 page in ICIS format)*

---

# References

Bastani, H., Bastani, O., Sungu, A., Ge, H., Kabakcı, Ö., & Mariman, R. (2024). Generative AI can harm learning. *SSRN Working Paper*.

Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. *Qualitative Research in Psychology*, 3(2), 77-101.

Brynjolfsson, E., Li, D., & Raymond, L. R. (2023). Generative AI at work. *NBER Working Paper No. 31161*.

Choung, H., David, P., & Ross, A. (2023). Trust in AI and its role in the acceptance of AI technologies. *International Journal of Human–Computer Interaction*, 39(9), 1727-1739.

Corbin, J., & Strauss, A. (2008). *Basics of Qualitative Research: Techniques and Procedures for Developing Grounded Theory* (3rd ed.). Sage.

Davis, F. D. (1989). Perceived usefulness, perceived ease of use, and user acceptance of information technology. *MIS Quarterly*, 13(3), 319-340.

Dell'Acqua, F., McFowland, E., Mollick, E. R., Lifshitz-Assaf, H., Kellogg, K., Rajendran, S., ... & Lakhani, K. R. (2023). Navigating the jagged technological frontier: Field experimental evidence of the effects of AI on knowledge worker productivity and quality. *Harvard Business School Working Paper No. 24-013*.

Gefen, D., Karahanna, E., & Straub, D. W. (2003). Trust and TAM in online shopping: An integrated model. *MIS Quarterly*, 27(1), 51-90.

Gillespie, N., Lockey, S., Curtis, C., Pool, J., & Akber, A. (2025). Trust, attitudes and use of artificial intelligence: A global study 2025. *KPMG/University of Queensland*.

Glaser, B. G., & Strauss, A. L. (1967). *The Discovery of Grounded Theory: Strategies for Qualitative Research*. Aldine.

Glikson, E., & Woolley, A. W. (2020). Human trust in artificial intelligence: Review of empirical research. *Academy of Management Annals*, 14(2), 627-660.

Guest, G., Bunce, A., & Johnson, L. (2006). How many interviews are enough? An experiment with data saturation and variability. *Field Methods*, 18(1), 59-82.

Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., ... & Fung, P. (2023). Survey of hallucination in natural language generation. *ACM Computing Surveys*, 55(12), 1-38.

Kim, J., Lee, S., & Park, Y. (2025). A validation of the Human-Generative Artificial Intelligence Trust Scale. *International Journal of Human–Computer Interaction*.

Klein, H. K., & Myers, M. D. (1999). A set of principles for conducting and evaluating interpretive field studies in information systems. *MIS Quarterly*, 23(1), 67-94.

Lee, J. D., & Moray, N. (1992). Trust, control strategies and allocation of function in human-machine systems. *Ergonomics*, 35(10), 1243-1270.

Lee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance. *Human Factors*, 46(1), 50-80.

Liao, Q. V., & Vaughan, J. W. (2024). AI transparency in the age of LLMs: A human-centered research roadmap. *Harvard Data Science Review*.

Lincoln, Y. S., & Guba, E. G. (1985). *Naturalistic Inquiry*. Sage.

Madhavan, P., & Wiegmann, D. A. (2007). Similarities and differences between human–human and human–automation trust: An integrative review. *Theoretical Issues in Ergonomics Science*, 8(4), 277-301.

Muir, B. M. (1987). Trust between humans and machines, and the design of decision aids. *International Journal of Man-Machine Studies*, 27(5-6), 527-539.

Myers, M. D., & Newman, M. (2007). The qualitative interview in IS research: Examining the craft. *Information and Organization*, 17(1), 2-26.

OpenAI. (2025). Sycophancy in GPT-4o: What happened and what we're doing about it. *OpenAI Blog*.

Parasuraman, R., & Riley, V. (1997). Humans and automation: Use, misuse, disuse, abuse. *Human Factors*, 39(2), 230-253.

Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S., ... & Kaplan, J. (2023). Discovering language model behaviors with model-written evaluations. *Findings of ACL 2023*.

Risko, E. F., & Gilbert, S. J. (2016). Cognitive offloading. *Trends in Cognitive Sciences*, 20(9), 676-688.

Robbins, S. (2023). Exploring the artificial intelligence "trust paradox": Evidence from a survey experiment in the United States. *PLOS ONE*, 18(7), e0288109.

Rubin, H. J., & Rubin, I. S. (2012). *Qualitative Interviewing: The Art of Hearing Data* (3rd ed.). Sage.

Schemmer, M., Hemmer, P., Kühl, N., Benz, C., & Satzger, G. (2022). Should I follow AI-based advice? Measuring appropriate reliance in human-AI decision-making. *arXiv preprint arXiv:2204.06916*.

Schemmer, M., Kuehl, N., Benz, C., Bartos, A., & Satzger, G. (2023). Appropriate reliance on AI advice: Conceptualization and the effect of explanations. *Proceedings of the 28th International Conference on Intelligent User Interfaces* (pp. 410-422).

Sharma, M., Tong, M., Korbak, T., Duvenaud, D., Askell, A., Bowman, S. R., ... & Perez, E. (2023). Towards understanding sycophancy in language models. *arXiv preprint arXiv:2310.13548*.

Talone, A. (2019). *The effect of reliability information and risk on appropriate reliance in an autonomous robot teammate*. Doctoral dissertation, University of Central Florida.

Thiele, L. P. (2025). Deskilling: The atrophy of cognitive and social aptitudes. In *Human Agency, Artificial Intelligence, and the Attention Economy* (pp. 75-98). Palgrave Macmillan.

Trope, Y., & Liberman, N. (2010). Construal-level theory of psychological distance. *Psychological Review*, 117(2), 440-463.

Venkatesh, V., Morris, M. G., Davis, G. B., & Davis, F. D. (2003). User acceptance of information technology: Toward a unified view. *MIS Quarterly*, 27(3), 425-478.

Walsham, G. (1995). Interpretive case studies in IS research: Nature and method. *European Journal of Information Systems*, 4(2), 74-81.

Wang, L., Jamieson, G. A., & Hollands, J. G. (2008). Selecting methods for the analysis of reliance on automation. *Proceedings of the Human Factors and Ergonomics Society Annual Meeting*, 52(4), 287-291.

Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ... & Fedus, W. (2024). Emergent abilities of large language models. *Transactions on Machine Learning Research*.

---

*Total Estimated Word Count: ~7,100 words (~16 pages in ICIS format)*
