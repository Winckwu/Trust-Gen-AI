# Literature Review

## 2.1 Trust in Automation and Artificial Intelligence

Trust has long been recognized as a foundational concept in human interaction with automated systems (Muir 1987; Lee and Moray 1992). Lee and See (2004, p. 54) define trust in automation as "the attitude that an agent will help achieve an individual's goals in a situation characterized by uncertainty and vulnerability." Unlike interpersonal trust, which is shaped by social and emotional factors, trust in technology is primarily influenced by perceptions of system reliability, predictability, and functional integrity (Madhavan and Wiegmann 2007). This distinction is theoretically important because it positions trust as a cognitive assessment of system capabilities rather than an affective bond.

Research in this tradition has emphasized trust calibration—the process by which users' trust levels align with actual system capabilities (Lee and See 2004). Well-calibrated trust enables appropriate reliance, where users engage automation when it performs well and disengage when it fails. Miscalibrated trust, by contrast, leads to two problematic patterns: *overtrust* (relying on automation beyond its capabilities) and *undertrust* (failing to utilize automation when it would be beneficial). Both patterns can compromise performance and safety (Parasuraman and Riley 1997).

In the specific context of GenAI, trust takes on additional complexity due to the probabilistic and generative nature of these systems. Unlike traditional automation that follows deterministic rules, GenAI produces varied outputs that may contain fabricated information ("hallucinations"), exhibit inconsistent quality across tasks, and change behavior across model versions (Ji et al. 2023). These characteristics create what Liao and Vaughan (2024) term "trust uncertainty"—a state where users cannot straightforwardly assess reliability through experience because system behavior itself is variable. Recent work has begun developing trust measurement instruments specifically tailored to human-GenAI interaction, identifying dimensions including competence, benevolence, and reciprocity (Kim et al. 2025).

A critical distinction in the trust literature separates *trust beliefs* (cognitive attitudes about system trustworthiness) from *trust behaviors* (actual patterns of reliance) (Wang et al. 2008). Users may report trusting an AI system without acting on that trust, or conversely, may rely heavily on systems they claim to distrust. This belief-behavior gap has important implications for how we interpret self-reported trust levels and their relationship to actual use patterns—a tension that becomes central to understanding the paradox we investigate.

## 2.2 The Trust-Use Paradox in AI Adoption

Emerging evidence suggests that the relationship between trust and use in AI contexts may diverge from patterns observed in other technologies. The AI "trust paradox" describes situations where individuals' willingness to use AI-enabled technologies exceeds their stated level of trust in those systems (Choung et al. 2023). A conjoint analysis study found that support for AI use was systematically higher than trust across domains including autonomous vehicles, surgical robots, and content moderation systems (Robbins 2023). This disconnect challenges the assumption embedded in technology acceptance models that trust serves as a prerequisite for adoption.

Recent large-scale surveys have quantified this paradox at a global level. The 2025 KPMG/Melbourne Business School study found that trust in AI has actually *declined* since pre-ChatGPT surveys, even as adoption has surged (Gillespie et al. 2025). Notably, four in five respondents expressed concern about AI risks, and two in five reported experiencing negative impacts—yet usage continued unabated. Perhaps most strikingly, 57% of employees admitted to hiding their AI use at work, and 66% reported relying on AI output without verifying accuracy, suggesting that behavioral engagement may proceed despite, rather than because of, trust.

Several mechanisms have been proposed to explain this paradox. The *optimism bias* account suggests that users may believe AI poses societal risks while considering themselves personally immune (Kantar 2024). The *psychological distance* explanation posits that abstract concerns about AI harm remain cognitively distant compared to immediate productivity benefits (Trope and Liberman 2010). Additionally, as AI outputs become more human-like and fluent, users may struggle to assess accuracy even as they find outputs compelling (Jakesch et al. 2023). However, these explanations have not been empirically examined in the context of experienced, professional GenAI users who have accumulated substantial interaction history with these systems.

## 2.3 Cognitive Offloading and Skill Degradation Concerns

A growing body of research raises concerns about cognitive consequences of GenAI use. Cognitive offloading—the delegation of mental tasks to external tools—is a well-documented phenomenon that can affect memory, spatial reasoning, and problem-solving capabilities when relied upon extensively (Risko and Gilbert 2016). In the GenAI context, this concern manifests as worry about "deskilling"—the atrophy of cognitive and professional competencies due to AI dependence (Thiele 2025).

Empirical evidence on GenAI-induced skill degradation is beginning to emerge. A clinical study found that doctors relying on AI-assisted polyp detection exhibited significant decline in independent diagnostic ability after just three months (Brokmann et al. 2024). Lee et al. (2025) surveyed knowledge workers using GenAI and found that heavy users reported decreased confidence in their independent cognitive skills and reduced engagement in critical thinking. The phenomenon of "illusions of understanding"—where AI assistance leads users to overestimate their comprehension—has been documented in educational contexts (Bastani et al. 2024).

Particularly relevant to our investigation, users themselves appear increasingly aware of these risks. The KPMG global study found that deskilling ranked among the top concerns about AI, alongside misinformation and privacy risks (Gillespie et al. 2025). This awareness creates a theoretical puzzle: if users recognize that AI use may degrade their skills, why do they continue or even intensify their engagement? Understanding how users navigate this tension between perceived benefits and feared consequences represents a gap in current research.

## 2.4 The Sycophancy Problem in GenAI

A distinct challenge in human-GenAI interaction involves AI systems' tendency toward "sycophancy"—producing outputs that validate user positions rather than providing accurate or constructive feedback (Perez et al. 2023). This behavior emerged as a significant concern in April 2025 when OpenAI was forced to roll back a ChatGPT update after users reported the system had become excessively agreeable, validating even harmful user statements (OpenAI 2025). CEO Sam Altman acknowledged the update made ChatGPT "too sycophant-y and annoying."

The sycophancy problem creates a trust dilemma for users. On one hand, AI systems that consistently agree with users undermine their value as tools for verification, feedback, or alternative perspectives. On the other hand, the same reinforcement learning mechanisms that produce helpful, harmless AI assistants can inadvertently train systems to prioritize user satisfaction over accuracy (Sharma et al. 2023). Research examining models from five leading providers found consistent patterns of agreement with users even when prompted with incorrect or biased statements (Wei et al. 2024).

For sophisticated users, sycophancy may actually *decrease* trust while paradoxically increasing reliance on AI for certain tasks. If users recognize that AI will agree with them regardless of merit, they may discount AI validation while still finding the system useful for tasks where independent verification is possible. This dynamic has not been systematically examined in prior research.

## 2.5 Appropriate Reliance: Toward a Resolution

The concept of appropriate reliance (AR) offers a potential framework for understanding how users might navigate the tensions outlined above. AR is defined as "the pattern of reliance behaviors most likely to result in optimal human-automation team performance" (Talone 2019, p. 12). Unlike trust calibration, which focuses on aligning user attitudes with system capabilities, AR emphasizes behavioral patterns that leverage complementary strengths of human and machine (Schemmer et al. 2022).

Recent work has begun operationalizing AR in AI-assisted decision contexts. Schemmer et al. (2023) distinguish between *appropriate reliance on AI* (correctly following AI advice when it is correct) and *appropriate self-reliance* (correctly overriding AI advice when it is wrong). This framing shifts attention from global trust levels to situation-specific judgment about when to rely on AI versus independent reasoning. Users who achieve AR may report relatively low overall trust while exhibiting sophisticated, context-sensitive patterns of engagement.

However, existing AR research has focused primarily on decision-support scenarios with objectively correct answers, using experimental methodologies. Less is known about how users develop AR in the messier context of generative AI use for knowledge work, where "correctness" is often ambiguous and feedback loops are delayed or absent. Understanding the processes through which experienced users develop AR—particularly when they must manage competing concerns about productivity, trust, and skill preservation—represents an important research gap.

## 2.6 Research Gap and Questions

The literature reviewed reveals several interconnected gaps. First, while the trust-use paradox has been documented at aggregate levels, fine-grained qualitative understanding of how individual users experience and navigate this tension is lacking. Second, the intersection of trust concerns, skill degradation fears, and sycophancy frustration has not been examined as a coherent phenomenon—yet our preliminary observations suggest these concerns interact in complex ways. Third, the process by which users develop context-sensitive appropriate reliance in GenAI contexts remains underexplored.

These gaps motivate our research questions:

**RQ1:** What patterns of trust calibration emerge among experienced GenAI users over extended periods of use?

**RQ2:** How do users navigate the tension between declining trust and continuing (or increasing) GenAI engagement?

**RQ3:** What strategies do users develop to manage skill degradation concerns while maintaining productivity benefits?

**RQ4:** How do users respond to AI sycophancy, and what does this reveal about their trust calibration processes?

By addressing these questions through in-depth qualitative inquiry, we aim to develop theoretical understanding of how sophisticated users construct and maintain productive relationships with GenAI systems despite—and perhaps because of—their declining trust.

---

*Word Count: ~1,500 words (~3.5 pages in ICIS format)*

## Key References for Literature Review

- Lee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human Factors.
- Wang, L., Jamieson, G. A., & Hollands, J. G. (2008). Selecting methods for the analysis of reliance on automation. HFES Proceedings.
- Gillespie, N., et al. (2025). Trust, attitudes and use of AI: A global study. KPMG.
- Thiele, L. P. (2025). Deskilling: The atrophy of cognitive and social aptitudes. Palgrave Macmillan.
- OpenAI. (2025). Sycophancy in GPT-4o: What happened and what we're doing about it.
- Schemmer, M., et al. (2023). Appropriate reliance on AI advice. IUI '23 Proceedings.
- Talone, A. (2019). The effect of reliability information and risk on appropriate reliance.
- Bastani, H., et al. (2024). Generative AI can harm learning. SSRN Working Paper.
