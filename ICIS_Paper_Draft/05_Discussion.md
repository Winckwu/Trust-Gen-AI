# Discussion

Our findings reveal a complex, paradox-laden relationship between trust and use in GenAI contexts that challenges prevailing theoretical frameworks. In this section, we discuss the theoretical contributions of our study, practical implications for design and training, and the limitations that bound our conclusions.

## 5.1 Theoretical Contributions

### 5.1.1 Reconceptualizing Trust-Use Relationships

Our first and most fundamental contribution is empirical documentation of the Trust-Use Paradox in GenAI adoption. Conventional technology adoption models—from TAM (Davis 1989) to UTAUT (Venkatesh et al. 2003)—position trust as an antecedent to use, implying a positive correlation: higher trust leads to greater adoption and engagement. Our findings challenge this assumption directly. Among our sample of experienced GenAI users, we observed substantial numbers for whom trust and use moved in *opposite* directions, with declining trust accompanying maintained or increased engagement.

This finding aligns with and extends emerging research on the AI trust paradox (Choung et al. 2023; Robbins 2023) by providing rich qualitative evidence of how this paradox manifests at the individual level. Prior work has documented the paradox through survey data showing aggregate disconnects between trust attitudes and use behaviors. Our contribution is to reveal the *mechanisms* through which users navigate this disconnect—the strategies, rationalizations, and adaptations that enable continued engagement despite declining trust.

We propose that trust in GenAI contexts may function less as a gateway variable (use/don't use) and more as a *calibration* variable shaping *how* one uses. Users with low trust do not necessarily disengage; instead, they develop more elaborate verification routines, restrict use to specific task types, and maintain heightened vigilance. In this sense, distrust is not the opposite of engagement but a particular *mode* of engagement characterized by skepticism and validation-seeking.

### 5.1.2 Trust Ceiling as Theoretical Construct

Our second contribution is the introduction of Trust Ceiling as a distinct phenomenon warranting theoretical attention. The Trust Ceiling represents a deliberate, self-imposed upper bound on trust that users maintain regardless of accumulated positive experiences. R45's articulation—"I will never completely be able to trust it. Maybe my cap will be around 75%"—exemplifies this phenomenon.

The Trust Ceiling differs from undertrust (Parasuraman and Riley 1997) in important ways. Undertrust implies miscalibration—trusting a system less than its capabilities warrant, typically due to insufficient experience or understanding. The Trust Ceiling, by contrast, represents a *deliberate* choice made by sophisticated users with extensive experience. It is not a failure of calibration but a strategic stance—a form of protective skepticism that guards against both AI failures and excessive dependence.

We suggest the Trust Ceiling may serve multiple functions:
- **Error protection**: Maintaining skepticism ensures users continue validating outputs, catching errors that might slip through with higher trust
- **Skill preservation**: By never fully delegating cognitive tasks, users maintain their own capabilities
- **Epistemic humility**: Acknowledging fundamental uncertainty about AI systems' reliability and future behavior

The Trust Ceiling concept connects to broader discussions of appropriate reliance (Lee and See 2004; Schemmer et al. 2023) but adds a temporal dimension. Where appropriate reliance typically refers to situation-specific decisions about when to rely on AI, the Trust Ceiling represents a meta-level constraint on the *range* of trust one is willing to extend across situations.

### 5.1.3 Informed Distrust and Appropriate Reliance

Our third contribution is reframing declining trust as a potential indicator of user maturation rather than system failure. We introduce the concept of "informed distrust"—a calibrated skepticism that emerges from deep engagement with AI systems and enables rather than prevents effective human-AI collaboration.

This reconceptualization has important implications for how we interpret trust metrics. If declining trust scores can indicate growing sophistication, then simple before-after trust comparisons may obscure meaningful dynamics. A user whose trust drops from 80% to 40% after extensive use may have achieved *better* calibration than one whose trust remains at 80%—particularly if the decline reflects accurate recognition of AI limitations rather than unwarranted pessimism.

Informed distrust connects to the appropriate reliance literature (Talone 2019; Schemmer et al. 2022) while adding psychological depth. Where AR research focuses on behavioral patterns (when users do or don't follow AI advice), informed distrust captures the cognitive stance that underlies these behaviors. Users operating from informed distrust approach AI outputs with a particular orientation—skeptical but engaged, validating but utilizing, distrustful of specific claims but appreciative of overall utility.

## 5.2 Practical Implications

### 5.2.1 Implications for AI System Design

Our findings suggest several design directions that depart from conventional wisdom about trust enhancement:

**Support for healthy skepticism**: Rather than designing to maximize trust, systems might benefit from features that support calibrated skepticism. This could include built-in prompts to verify critical information, explicit confidence indicators, or interfaces that normalize validation as part of the workflow.

**Reduce sycophancy**: Users' frustration with excessive AI agreement (Paradox 3) suggests that designing for user satisfaction may backfire with sophisticated users. Systems that occasionally push back, request clarification, or express uncertainty may paradoxically generate more trust than those that consistently validate user positions.

**Enable task-specific calibration**: Given the prevalence of task-divergent trust patterns, interfaces might benefit from helping users articulate and track their task-specific trust levels, surfacing when they are using AI for tasks where their trust is low.

### 5.2.2 Implications for Training and Onboarding

Our findings suggest that organizational GenAI training should acknowledge rather than suppress the trust-use paradox:

**Validate declining trust as sophistication**: Rather than positioning declining trust as failure, training might frame it as a sign of developing expertise—an indication that users have moved from naive enthusiasm to informed skepticism.

**Teach verification as workflow**: Training could position validation not as an occasional check but as an integral part of AI-augmented work, normalizing the practices that our most sophisticated users had developed independently.

**Address skill concerns directly**: Given the salience of skill degradation anxieties, training might explicitly address these concerns and offer strategies for maintaining core competencies while capturing productivity benefits.

### 5.2.3 Implications for Organizational Policy

Organizations deploying GenAI might reconsider policies that implicitly assume trust-use correlation:

**Rethink trust metrics**: Tracking employee trust in AI as a success metric may be misleading if declining trust indicates growing sophistication rather than dissatisfaction.

**Support selective restriction**: Rather than encouraging maximal AI use, policies might support employees' deliberate choices to restrict AI use for certain tasks—recognizing this as a legitimate form of skill preservation.

## 5.3 Limitations and Future Research

Several limitations bound our conclusions and suggest directions for future research.

**Sample characteristics**: Our sample, while diverse in many respects, skewed toward highly educated young professionals with above-average technical sophistication. The patterns we observed—particularly the capacity for informed distrust—may not generalize to less experienced or less technically oriented populations. Future research should examine trust-use dynamics across broader demographic segments.

**Self-reported trust**: Our reliance on self-reported trust levels introduces potential biases. Participants' articulated trust levels may not fully correspond to their operational trust behaviors. Future research might combine self-reports with behavioral measures or experimental paradigms.

**Cross-sectional reconstruction**: While we asked participants to reconstruct trust trajectories over time, we did not track them longitudinally. Memory biases may affect how participants characterize their initial trust levels or the events that influenced change. Longitudinal designs tracking trust evolution in real-time would strengthen causal inference.

**Cultural context**: Although our sample included participants from multiple countries, we did not systematically analyze cultural variations in trust dynamics. Given documented cross-cultural differences in technology trust (Li et al. 2021), future research should examine whether the paradoxes we identified manifest differently across cultural contexts.

**Generalizability across AI systems**: Our findings emerged primarily from ChatGPT users, reflecting its market dominance. Whether similar patterns emerge with other GenAI systems—which may differ in reliability, interface design, or characteristic failure modes—remains an open question.

---

*Word Count: ~1,300 words (~2.5 pages in ICIS format)*
